2019,The FAIR TRADE Framework for Assessing Decentralised Data Solutions.,"Decentralised data solutions bring their own sets of capabilities, requirements and issues not necessarily present in centralised solutions. In order to compare the properties of different approaches or tools for management of decentralised data, it is important to have a common evaluation framework. We present a set of dimensions relevant to data management in decentralised contexts and use them to define principles extending the FAIR framework, initially developed for open research data. By characterising a range of different data solutions or approaches by how TRusted, Autonomous, Distributed and dEcentralised, in addition to how Findable, Accessible, Interoperable and Reusable, they are, we show that our FAIR TRADE framework is useful for describing and evaluating the management of decentralised data solutions, and aim to contribute to the development of best practice in a developing field.
"
2017,Linked Data Notifications: A Resource-Centric Communication Protocol.,"Abstract
In this article we describe the Linked Data Notifications (LDN) protocol, which is a W3C Candidate Recommendation. Notifications are sent over the Web for a variety of purposes, for example, by social applications. The information contained within a notification is structured arbitrarily, and typically only usable by the application which generated it in the first place. In the spirit of Linked Data, we propose that notifications should be reusable by multiple authorised applications. Through separating the concepts of senders, receivers and consumers of notifications, and leveraging Linked Data principles of shared vocabularies and URIs, LDN provides a building block for decentralised Web applications. This permits end users more freedom to switch between the online tools they use, as well as generating greater value when notifications from different sources can be used in combination. We situate LDN alongside related initiatives, and discuss additional considerations such as security and abuse prevention measures. We evaluate the protocol‚Äôs effectiveness by analysing multiple, independent implementations, which pass a suite of formal tests and can be demonstrated interoperating with each other. To experience the described features please open this document in your Web browser under its canonical URI: http://csarven.ca/linked-data-notifications."
2017,"Decentralised Authoring, Annotations and Notifications for a Read-Write Web with dokieli.","Abstract
While the Web was designed as a decentralised environment, individual authors still lack the ability to conveniently author and publish documents, and to engage in social interactions with documents of others in a truly decentralised fashion. We present dokieli, a fully decentralised, browser-based authoring and annotation platform with built-in support for social interactions, through which people retain ownership of and sovereignty over their data. The resulting ‚Äúliving‚Äù documents are interoperable and independent of dokieli since they follow standards and best practices, such as HTML+RDFa for a fine-grained semantic structure, Linked Data Platform for personal data storage, and Linked Data Notifications for updates. This article describes dokieli‚Äôs architecture and implementation, demonstrating advanced document authoring and interaction without a single point of control. Such an environment provides the right technological conditions for independent publication of scientific articles, news, and other works that benefit from diverse voices and open interactions. To experience the described features please open this document in your Web browser under its canonical URI: http://csarven.ca/dokieli-rww."
2016,A Demonstration of the Solid Platform for Social Web Applications.,"Solid is a decentralized platform for social Web applications. In the Solid platform, users' data is managed independently of the applications that create and consume this data. Each user stores their data in a Web-accessible personal online datastore (or pod). Each user can have one or more pods from different pod providers, and can easily switch between providers. Applications access data in users' pods using well defined protocols, and a decentralized authentication and access control mechanism guarantees the privacy of the data. In this decentralized architecture, applications can operate on users' data wherever it is stored. Users control access to their data, and have the option to switch between applications at any time. We will demonstrate the utility of Solid and how it is experienced from the point of view of end users and application developers. For this, we will use a set of Solid servers and multiple Web applications that use these servers. We believe that experience with a concrete platform such as Solid is highly valuable in truly appreciating the power of a decentralized social Web.
"
2014,CIMBA - Client-Integrated MicroBlogging Architecture.,n/a
2012,Linked Open Government Data: Lessons from Data.gov.uk.,"Abstract:
A project to extract value from open government data contributes to the population of the linked data Web with high-value data of good provenance."
2011,Designing the web for an open society.,"How can we best design Web technology to support the features we would like of our society such as: openness, justice, transparency, accountability, participation, innovation, science and democracy?
"
2010,From the Semantic Web to social machines: A research challenge for AI on the World Wide Web.,"Abstract
The advent of social computing on the Web has led to a new generation of Web applications that are powerful and world-changing. However, we argue that we are just at the beginning of this age of ‚Äúsocial machines‚Äù and that their continued evolution and growth requires the cooperation of Web and AI researchers. In this paper, we show how the growing Semantic Web provides necessary support for these technologies, outline the challenges we see in bringing the technology to the next level, and propose some starting places for the research."
2010,World-wide web: the information universe.,"Purpose
The World?Wide Web (W3) initiative is a practical project designed to bring a global information universe into existence using available technology. This paper seeks to describe the aims, data model, and protocols needed to implement the ìwebî and to compare them with various contemporary systems.

Design/methodology/approach
Since Vannevar Bush's article, men have dreamed of extending their intellect by making their collective knowledge available to each individual by using machines. Computers provide us two practical techniques for human?knowledge interface. One is hypertext, in which links between pieces of text (or other media) mimic human association of ideas. The other is text retrieval, which allows associations to be deduced from the content of text. The W3 ideal world allows both operations and provides access from any browsing platform.

Findings
Various server gateways to other information systems have been produced, and the total amount of information available on the web is becoming very significant, especially since it includes all anonymous FTP archives, WAIS servers, and Gopher servers as well as specific W3 servers.

Originality/value
The paper notices that a W3 server could provide the functions of each of these servers, and so it looks forward to a single protocol that can be used by the whole community."
2010,"Put in Your Postcode, Out Comes the Data: A Case Study.","Abstract
A single datum or a set of a categorical data has little value on its own. Combinations of disparate sets of data increase the value of those data sets and helps to discover interesting patterns or relationships, facilitating the construction of new applications and services. In this paper, we describe an implementation of using open geographical data as a core set of ‚Äújoin point‚Äù(s) to mesh different public datasets. We describe the challenges faced during the implementation, which include, sourcing the datasets, publishing them as linked data, and normalising these linked data in terms of finding the appropriate ‚Äújoin points‚Äù from the individual datasets, as well as developing the client application used for data consumption. We describe the design decisions and our solutions to these challenges. We conclude by drawing some general principles from this work."
2009,Linked Data - The Story So Far.,"The term ìLinked Dataî refers to a set of best practices for publishing and connecting structured data on the Web. These best practices have been adopted by an increasing number of data providers over the last three years, leading to the creation of a global data space containing billions of assertionsó the Web of Data. In this article, the authors present the concept and technical principles of Linked Data, and situate these within the broader context of related technological developments. They describe progress to date in publishing Linked Data on the Web, review applications that have been developed to exploit the Web of Data, and map out a research agenda for the Linked Data community as it moves forward."
2009,Interacting with eHealth: towards grand challenges for HCI.,"While health records are increasingly stored electronically, we have little access to this data about ourselves. We're not used to thinking of these official records either as ours or as something we'd understand if we had access to them in any case. We increasingly turn to the Web, however, to query any ache, pain or health goal we may have before consulting with health care professionals. Likewise, for proactive health care, such as nutrition or fitness, or post diagnosis support, to find fellow-sufferers, we turn to online resources. There is, it seems, a potential disconnect between points at which professional and proactive health care intersect. Such gaps in information sharing may have direct impact on practices we decide to take up, the care we seek, and the support professionals offer. In this panel, we consider several places within proactive, preventative health care in particular HCI has a role towards enhancing health knowledge discovery and health support interaction. Our goal is to demonstrate how now is the time for eHealth to come to the forefront of the HCI research agenda.
"
2009,Policy-Aware Content Reuse on the Web.,"Abstract
The Web allows users to share their work very effectively leading to the rapid re-use and remixing of content on the Web including text, images, and videos. Scientific research data, social networks, blogs, photo sharing sites and other such applications known collectively as the Social Web have lots of increasingly complex information. Such information from several Web pages can be very easily aggregated, mashed up and presented in other Web pages. Content generation of this nature inevitably leads to many copyright and license violations, motivating research into effective methods to detect and prevent such violations.
This is supported by an experiment on Creative Commons (CC) attribution license violations from samples of Web pages that had at least one embedded Flickr image, which revealed that the attribution license violation rate of Flickr images on the Web is around 70-90%. Our primary objective is to enable users to do the right thing and comply with CC licenses associated with Web media, instead of preventing them from doing the wrong thing or detecting violations of these licenses. As a solution, we have implemented two applications: (1) Attribution License Violations Validator, which can be used to validate users‚Äô derived work against attribution licenses of reused media and, (2) Semantic Clipboard, which provides license awareness of Web media and enables users to copy them along with the appropriate license metadata."
2008,The Fractal Nature of the Semantic Web.,"Abstract
In the past, many knowledge representation systems failed because they were too monolithic and didn‚Äôt scale well, whereas other systems failed to have an impact because they were small and isolated. Along with this trade-off in size, there is also a constant tension between the cost involved in building a larger community that can interoperate through common terms and the cost of the lack of interoperability. The semantic web offers a good compromise between these approaches as it achieves wide-scale communication and interoperability using finite effort and cost. The semantic web is a set of standards for knowledge representation and exchange that is aimed at providing interoperability across applications and organizations. We believe that the gathering success of this technology is not derived from the particular choice of syntax or of logic. Its main contribution is in recognizing and supporting the fractal patterns of scalable web systems. These systems will be composed of many overlapping communities of all sizes, ranging from one individual to the entire population that have internal (but not global) consistency. The information in these systems, including documents and messages, will contain some terms that are understood and accepted globally, some that are understood within certain communities, and some that are understood locally within the system. The amount of interoperability between interacting agents (software or human) will depend on how many communities they have in common and how many ontologies (groups of consistent and related terms) they share. In this article we discuss why fractal patterns are an appropriate model for web systems and how semantic web technologies can be used to design scalable and interoperable systems."
2008,Information accountability.,"With access control and encryption no longer capable of protecting privacy, laws and systems are needed that hold people accountable for the misuse of personal information, whether public or secret.
"
2008,Web science: an interdisciplinary approach to understanding the web.,"The Web must be studied as an entity in its own right to ensure it keeps flourishing and prevent unanticipated social effects.
"
2008,N3Logic: A logical framework for the World Wide Web.,"The Semantic Web drives toward the use of the Web for interacting with logically interconnected data. Through knowledge models such as Resource Description Framework (RDF), the Semantic Web provides a unifying representation of richly structured data. Adding logic to the Web implies the use of rules to make inferences, choose courses of action, and answer questions. This logic must be powerful enough to describe complex properties of objects but not so powerful that agents can be tricked by being asked to consider a paradox. The Web has several characteristics that can lead to problems when existing logics are used, in particular, the inconsistencies that inevitably arise due to the openness of the Web, where anyone can assert anything. N3Logic is a logic that allows rules to be expressed in a Web environment. It extends RDF with syntax for nested graphs and quantified variables and with predicates for implication and accessing resources on the Web, and functions including cryptographic, string, math. The main goal of N3Logic is to be a minimal extension to the RDF data model such that the same language can be used for logic and data. In this paper, we describe N3Logic and illustrate through examples why it is an appropriate logic for the Web.
"
2008,Tabulator Redux: Browsing and Writing Linked Data.,"A first category of Semantic Web browsers was designed to present a given dataset (an RDF graph) for perusal in various forms. These include mSpace, Exhibit, and to a certain extent Haystack. A second category tackled mechanisms and display issues around presenting linked data gathered on the fly. These include Tabulator, Oink, Disco, Open Link Software's Data Browser, and Object Browser. The challenge of once that data is gathered, how might it be edited, extended and annotated has so far been left largely unaddressed. This is not surprising: there are a number of steep challenges for determining how to support editing information in the open web of linked data. These include the representation of both the web of documents and the web of things, and the relationships between them; ensuring the user is aware of and has control over the social context such as licensing and privacy of data being entered, and, on a web in which anyone can say anything about anything, helping the user intuitively select the things which they actually wish to see in a given situation. There is also the view update problem: the difficulty of reflecting user edits back through functions used to map web data to a screen presentation. In the latest version of the Tabulator project, described in this paper we have focused on providing the write side of the readable/writable web. Our approach has been to allow modification and addition of information naturally within the browsing interface, and to relay changes to the server triple by triple for least possible brittleness (there is no explicit 'save' operation). Challenges that remain include the propagation of changes by collaborators back to the interface to create a shared editing system. To support writing across (semantic) Web resources, our work has contributed several technologies, including a HTTP/SPARQL/Update-based protocol between an editor (or other system) and incrementally editable resources stored in an open source, world-writable 'data wiki'. This begins enabling the writable Semantic Web."
2008,Linked data on the web (LDOW2008).,"The Web is increasingly understood as a global information space consisting not just of linked documents, but also of Linked Data. More than just a vision, the resulting Web of Data has been brought into being by the maturing of the Semantic Web technology stack, and by the publication of an increasing number of datasets according to the principles of Linked Data.

The Linked Data on the Web (LDOW2008) workshop brings together researchers and practitioners working on all aspects of Linked Data. The workshop provides a forum to present the state of the art in the field and to discuss ongoing and future research challenges. In this workshop summary we will outline the technical context in which Linked Data is situated, describe developments in the past year through initiatives such as the Linking Open Data community project, and look ahead to the workshop itself."
2008,Linked Data on the Web.,"The Web is increasingly understood as a global information space consisting not just of linked documents, but also of Linked Data. More than just a vision, the resulting Web of Data has been brought into being by the maturing of the Semantic Web technology stack, and by the publication of an increasing number of datasets according to the principles of Linked Data.

The Linked Data on the Web (LDOW2008) workshop brings together researchers and practitioners working on all aspects of Linked Data. The workshop provides a forum to present the state of the art in the field and to discuss ongoing and future research challenges. In this workshop summary we will outline the technical context in which Linked Data is situated, describe developments in the past year through initiatives such as the Linking Open Data community project, and look ahead to the workshop itself."
2007,Data-Purpose Algebra: Modeling Data Usage Policies.,"Abstract:
Data is often encumbered by restrictions on the ways in which it may be used. These restrictions on usage may be determined by statute, by contract, by custom, or by common decency, and they are used to control collection of data, diffusion of data, and the inferences that can be made over the data. In this paper, we present a data-purpose algebra that can be used to model these kinds of restrictions in various different domains. We demonstrate the utility of our approach by modeling part of the Privacy Act (5 USC xi552a) 1 , which states that data collected about US citizens can be used only for the purposes for which it was collected. We show (i) how this part of the Privacy act can be represented as a set of restrictions on data usage, (ii) how the authorized purposes of data flowing through different government agencies can be calculated, and (iii) how these purposes can be used to determine whether the Privacy Act is being enforced appropriately."
2006,The Semantic Web Revisited.,"Abstract:
The article included many scenarios in which intelligent agents and bots undertook tasks on behalf of their human or corporate owners. Of course, shopbots and auction bots abound on the Web, but these are essentially handcrafted for particular tasks: they have little ability to interact with heterogeneous data and information types. Because we haven't yet delivered large-scale, agent-based mediation, some commentators argue that the semantic Web has failed to deliver. We argue that agents can only flourish when standards are well established and that the Web standards for expressing shared meaning have progressed steadily over the past five years"
2006,A Framework for Web Science.,n/a
2006,Using Semantic Web Technologies for Policy Management on the Web.,"With policy management becoming popular as a means of providing flexible Web security, the number of policy languages being proposed for the Web is constantly increasing. We recognize the importance of policies for securing the Web and believe that the future will only bring more policy languages. We do not, however, believe that users should be forced to conform the description of their policy relationships to a single standard policy language. Instead there should be a way of encompassing different policy languages and supporting heterogeneous policy systems. As a step in this direction, we propose Rein, a policy framework grounded in Semantic Web technologies, which leverages the distributed nature and linkability of the Web to provide Web-based policy management. Rein provides ontologies for describing policy domains in a decentralized manner and provides an engine for reasoning over these descriptions, both of which can be used to develop domain and policy language specific security systems. We describe the Rein policy framework and discuss how a Rein policy management systems can be developed for access control in an online photo sharing application.
"
2006,Transparent Accountable Data Mining: New Strategies for Privacy Protection.,"Attempts to address issues of personal privacy in a world of computerized databases and information networks ‚Äî from security technology to data protection regulation to Fourth Amendment law jurisprudence ‚Äî typically proceed from the perspective of controlling or preventing access to information. We argue that this perspective has become inadequate and obsolete, overtaken by the ease of sharing and copying data and of aggregating and searching across multiple data bases, to reveal private information from public sources. To replace this obsolete framework, we propose that issues of privacy protection currently viewed in terms of data access be re-conceptualized in terms of data use. From a technology perspective, this requires supplementing legal and technical mechanisms for access control with new mechanisms for transparency and accountability of data use. In this paper, we present a technology infrastructure ‚Äî the Policy Aware Web ‚Äî that supports transparent and accountable data use on the World Wide Web, and elements of a new legal and regulatory regime that supports privacy through provable accountability to usage rules rather than merely data access restrictions."
2006,Self-Describing Delegation Networks for the Web.,"Abstract:
As the necessity of flexible Web security becomes more apparent and as the notion of using policies for access control gains popularity, the number of policy languages being proposed for controlling access to Web resources increases. Instead of defining a single standard policy language, we believe that there should be a way of embracing different policy languages and of allowing interoperability between systems that use different policy languages. We propose Rein - a policy and delegation framework that is grounded in semantic Web technologies - to help the Web preserve maximum expressiveness for local policy communities by enabling global interoperability of policy reasoning. Rein provides ontologies for describing policy and delegation networks, and provides mechanisms for reasoning over them, both of which can be used to develop domain and policy language specific access control frameworks for Web resources. The focus of this paper is the delegation mechanisms of the Rein policy framework that support both delegation of authorization and trust. In this paper we give a brief overview of the Rein framework, describe its delegation mechanisms, and illustrate their usefulness through some examples"
2006,The next wave of the web.,"The World Wide Web has been revolutionary in terms of impact, scale and outreach. At every level society has been changed in some way by the Web. This Panel will consider likely developments in this extraordinary human construct as we attempt to realise the Next Wave of the Web - a Semantic Web.Nigel Shadbolt will Chair a discussion that will focus on the prospects for the Semantic Web, its likely form and the challenges it faces. Can we achieve the necessary agreements on shared meaning for the Semantic Web? Can we achieve a critical mass of semantically annotated data and content? How are we to trust such content? Do the scientific and commercial drivers really demand a Semantic Web? How will the move to a mobile and ubiquitous Web affect the Semantic Web? How does Web 2.0 relate to the Semantic Web?
"
2005,Experience with N3 rules.,"This short paper summarizes experience at MIT/CSAIL in developing and using Notation3 (N3) as a language for RDF and as a rules language for the Semantic Web. N3 was developed as simple syntax for RDF. Then, to make a rule language, graph literals and variables were added to N3. RDF properties were then introduced to allow rules to be expressed, web access and built-in functions."
2005,WWW at 15 years: looking forward.,"The key property of the WWWW is its universality: One must be able to access it whatever the hardware device, software platform, and network one is using, and despite the disabilities one might have, and whether oner is in a ""developed"" or ""developing"" country; it must support information of any language, culture, quality, medium, and field without discrimination so that a hypertext link can go anywhere; it must support information intended for people, and that intended for machine processing. The Web architecture incorporated various choices which support these axes of universality.Currently the architecture and the principles are being exploited in the recent Mobile Web initiative in W3C to promote content which can be accessed optimally from conventional computers and mobile devices. New exciting areas arise every few months as possible Semantic Web flagship applications. As new areas burst forth, the fundamental principles remain important and are extended and adjusted. At the same time, the principles of openness and consensus among international stakeholders which the WWW consortium employs for new technology are adjusted, but ever-important.
"
1997,World-Wide Computer.,"This chapter discusses the history and growth of World Wide Web (W3). The World-Wide Web was developed to be a pool of human knowledge, which would allow collaborators in remote sites to share their ideas and all aspects of a common project. Physicists and engineers at CERN, the European Particle Physics Laboratory in Geneva, Switzerland, collaborate with many other institutes to build the software and hardware for high-energy physics research. The idea of the Web was prompted by positive experience of a small ìhome-brewî personal hypertext system used for keeping track of personal information on a distributed project. The Web was designed so that if it was used independently for two projects, and later relationships were found between the projects, then no major or centralized changes would have to be made, but the information could smoothly reshape to represent the new state of knowledge. This property of scaling has allowed the Web to expand rapidly from its origins at CERN across the Internet irrespective of boundaries of nations or disciplines."
1997,"The World Wide Web - Past, Present and Future.",n/a
1996,"WWW: Past, Present, and Future.","Abstract:
The World Wide Web is simply defined as the universe of global network-accessible information. It is an abstract space within which people can interact, and it is chiefly populated by interlinked pages of text, images, and animations, with occasional sounds, videos, and three-dimensional worlds. The Web marks the end of an era of frustrating and debilitating incompatibility between computer systems. It has created an explosion of accessibility, with many potential social and economical impacts. The Web was designed to be a space within which people could work on a project. This was a powerful concept, in that: people who build a hypertext document of their shared understanding can refer to it at all times; people who join a project team can have access to a history of the team's activities, decisions, and so on; the work of people who leave a team can be captured for future reference; and a team's operations, if placed on the Web, can be machine-analyzed in a way that could not be done otherwise. The Web was originally supposed to be a personal information system and a tool for groups of all sizes, from a team of two to the entire world. People have rapidly developed new features for the Web, because of its tremendous commercial potential. This has made the maintenance of globalWeb interoperability a continuous task. This has also created a number of areas into which research must continue."
1996,Editorial.,n/a
1994,The World-Wide Web.,n/a
1992,The World-Wide Web.,"Abstract
This paper describes the World-Wide Web (W3) global information system initiative, its protocols and data formats, and how it is used in practice. It discusses the plethora of different but similar information systems which exist, and how the web unifies them, creating a single information space.
We describe the difficulties of information sharing between colleagues, and the basic W3 model of hypertext and searchable indexes. We list the protocols used by W3 and describe a new simple search and retrieve protocol (HTTP), and the SGML style document encoding used. We summarize the current status of the X11, NeXTStep, dumb terminal and other clients, and of the available server and gateway software."
1992,World-Wide Web: The Information Universe.,"The World?Wide Web (W3) initiative is a practical project designed to bring a global information universe into existence using available technology. This article describes the aims, data model, and protocols needed to implement the ìwebî and compares them with various contemporary systems.
"
1988,CERN experience.,n/a
