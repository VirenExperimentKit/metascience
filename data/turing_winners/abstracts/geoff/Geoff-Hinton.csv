2019,Analyzing and Improving Representations with the Soft Nearest Neighbor Loss.,We explore and expand the Soft Nearest Neighbor Loss to measure the entanglement of class manifolds in representation space: i.e.  how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool it provides insights into the evolution of class similarity structures during learning. Surprisingly    we find that maximizing the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer  possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to better-calibrated estimates of uncertainty on outlier data but also marginally improved generalization. Data that is not from the training distribution can be recognized by observing that in the hidden layers  it has fewer than the normal number of neighbors from the predicted class.          
2019,Similarity of Neural Network Representations Revisited.,Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity  but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA  CKA can reliably identify correspondences between representations in networks trained from different initializations.             
2019,Analyzing and Improving Representations with the Soft Nearest Neighbor Loss.,We explore and expand the Soft Nearest Neighbor Loss to measure the entanglement of class manifolds in representation space: i.e.  how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool it provides insights into the evolution of class similarity structures during learning. Surprisingly    we find that maximizing the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer  possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to better-calibrated estimates of uncertainty on outlier data but also marginally improved generalization. Data that is not from the training distribution can be recognized by observing that in the hidden layers  it has fewer than the normal number of neighbors from the predicted class.          
2019,Similarity of Neural Network Representations Revisited.,Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity  but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA  CKA can reliably identify correspondences between representations in networks trained from different initializations.             
2019,Cerberus: A Multi-headed Derenderer.,To generalize to novel visual scenes with new viewpoints and new object poses  a visual system needs representations of the shapes of the parts of an object that are invariant to changes in viewpoint or pose. 3D graphics representations disentangle visual factors such as viewpoints and lighting from object structure in a natural way. It is possible to learn to invert the process that converts 3D graphics representations into 2D images  provided the 3D graphics representations are available as labels. When only the unlabeled images are available  however  learning to derender is much harder. We consider a simple model which is just a set of free floating parts. Each part has its own relation to the camera and its own triangular mesh which can be deformed to model the shape of the part. At test time  a neural network looks at a single image and extracts the shapes of the parts and their relations to the camera. Each part can be viewed as one head of a multi-headed derenderer. During training  the extracted parts are used as input to a differentiable 3D renderer and the reconstruction error is backpropagated to train the neural net. We make the learning task easier by encouraging the deformations of the part meshes to be invariant to changes in viewpoint and invariant to the changes in the relative positions of the parts that occur when the pose of an articulated body changes. Cerberus  our multi-headed derenderer  outperforms previous methods for extracting 3D parts from single images without part annotations  and it does quite well at extracting natural parts of human figures.      
2019,Learning Sparse Networks Using Targeted Dropout.,Neural networks are easier to optimise when they have many more weights than are required for modelling the mapping from inputs to outputs. This suggests a two-stage learning procedure that first learns a large net and then prunes away connections or hidden units. But standard training does not necessarily encourage nets to be amenable to pruning. We introduce targeted dropout  a method for training a neural network so that it is robust to subsequent pruning. Before computing the gradients for each weight update  targeted dropout stochastically selects a set of units or weights to be dropped using a simple self-reinforcing sparsity criterion and then computes the gradients for the remaining weights. The resulting network is robust to post hoc pruning of weights or units that frequently occur in the dropped sets. The method improves upon more complicated sparsifying regularisers while being simple to implement and easy to tune.             
2019,When Does Label Smoothing Help?,The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models  including image classification  language translation and speech recognition. Despite its widespread use  label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization  label smoothing improves model calibration which can significantly improve beam-search. However  we also observe that if a teacher network is trained with label smoothing  knowledge distillation into a student network is much less effective. To explain these observations  we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes  which is necessary for distillation  but does not hurt generalization or calibration of the model's predictions.      
2019,Stacked Capsule Autoencoders.,Objects are composed of a set of geometrically organized parts. We introduce an unsupervised capsule autoencoder (SCAE)  which explicitly uses geometric relationships between parts to reason about objects. Since these relationships do not depend on the viewpoint  our model is robust to viewpoint changes. SCAE consists of two stages. In the first stage  the model predicts presences and poses of part templates directly from the image and tries to reconstruct the image by appropriately arranging the templates. In the second stage  SCAE predicts parameters of a few object capsules  which are then used to reconstruct part poses. Inference in this model is amortized and performed by off-the-shelf neural encoders  unlike in previous capsule networks. We find that object capsule presences are highly informative of the object class  which leads to state-of-the-art results for unsupervised classification on SVHN (55%) and MNIST (98.7%).        
2019,Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions.,Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. Most of the proposed methods for mitigating adversarial examples have subsequently been defeated by stronger attacks. Motivated by these issues  we take a different approach and propose to instead detect adversarial examples based on class-conditional reconstructions of the input. Our method uses the reconstruction network proposed as part of Capsule Networks (CapsNets)  but is general enough to be applied to standard convolutional networks. We find that adversarial or otherwise corrupted images result in much larger reconstruction errors than normal inputs  prompting a simple detection method by thresholding the reconstruction error. Based on these findings  we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. While this attack produces undetected adversarial examples  we find that for CapsNets the resulting perturbations can cause the images to appear visually more like the target class. This suggests that CapsNets utilize features that are more aligned with human perception and address the central issue raised by adversarial examples.          
2019,"Lookahead Optimizer: k steps forward, 1 step back.",The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes  such as AdaGrad and Adam  and (2) accelerated schemes  such as heavy-ball and Nesterov momentum. In this paper  we propose a new optimization algorithm  Lookahead  that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively  the algorithm chooses a search direction by looking ahead at the sequence of fast weights generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam  even with their default hyperparameter settings on ImageNet  CIFAR-10/100  neural machine translation  and Penn Treebank.    
2019,CvxNets: Learnable Convex Decomposition.,Any solid object can be decomposed into a collection of convex polytopes (in short  convexes). When a small number of convexes are used  such a decomposition can be thought of as a piece-wise approximation of the geometry. This decomposition is fundamental to real-time physics simulation in computer graphics  where it creates a unifying representation of dynamic geometry for collision detection. A convex object also has the property of being simultaneously an explicit and implicit representation: one can interpret it explicitly as a mesh derived by computing the vertices of a convex hull  or implicitly as the collection of half-space constraints or support functions. Their implicit representation makes them particularly well suited for neural network training  as they abstract away from the topology of the geometry they need to represent. We introduce a network architecture to represent a low dimensional family of convexes. This family is automatically derived via an autoencoding process. We investigate the applications of the network including automatic convex decomposition  image to 3D reconstruction  and part-based shape retrieval.        
2018,Who Said What: Modeling Individual Labelers Improves Classification.,Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree  the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches  however  do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information  we propose modeling the experts individually and then learning averaging weights for combining them  possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona (2010)  and by Mnih and Hinton (2012). Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training.         
2018,Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search.,We introduce Picturebook  a large-scale lookup operation to ground language via ‘snapshots’ of our physical world accessed through image search. For each word in a vocabulary  we extract the top-k images from Google image search and feed the images through a convolutional network to extract a word embedding. We introduce a multimodal gating function to fuse our Picturebook embeddings with other word representations. We also introduce Inverse Picturebook  a mechanism to map a Picturebook embedding back into words. We experiment and report results across a wide range of tasks: word similarity  natural language inference  semantic relatedness  sentiment/topic classification  image-sentence ranking and machine translation. We also show that gate activations corresponding to Picturebook embeddings are highly correlated to human judgments of concreteness ratings.        
2018,Large scale distributed neural network training through online distillation.,Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However  due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation)  these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially  we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset  ImageNet  and the largest to-date dataset used for neural language modeling  containing  tokens and based on the Common Crawl repository of web data.         
2018,Matrix capsules with EM routing.,A capsule is a group of neurons whose outputs represent different properties of the same entity. Each layer in a capsule network contains many capsules. We describe a version of capsules in which each capsule has a logistic unit to represent the presence of an entity and a 4x4 matrix which could learn to represent the relationship between that entity and the viewer (the pose). A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by trainable viewpoint-invariant transformation matrices that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefficient. These coefficients are iteratively updated for each image using the Expectation-Maximization algorithm such that the output of each capsule is routed to a capsule in the layer above that receives a cluster of similar votes. The transformation matrices are trained discriminatively by backpropagating through the unrolled iterations of EM between each pair of adjacent capsule layers. On the smallNORB benchmark  capsules reduce the number of test errors by 45\% compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attacks than our baseline convolutional neural network.              
2018,Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures.,The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI  however  has inspired proposals for understanding how the brain might learn across multiple layers  and hence how it might approximate BP. As of yet  none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical  or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST  CIFAR-10  and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms  and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST  but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP  especially for networks composed of locally connected units  opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.    
2018,Large scale distributed neural network training through online distillation.,Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However  due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation)  these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially  we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset  ImageNet  and the largest to-date dataset used for neural language modeling  containing 6×1011 tokens and based on the Common Crawl repository of web data.         
2018,Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures.,The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI  however  has inspired proposals for understanding how the brain might learn across multiple layers  and hence how it might approximate BP. As of yet  none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical  or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST  CIFAR-10  and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms  and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST  but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP  especially for networks composed of locally connected units  opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.    
2018,DARCCC: Detecting Adversaries by Reconstruction from Class Conditional Capsules.,"We present a simple technique that allows capsule models to detect adversarial images. In addition to being trained to classify images  the capsule model is trained to reconstruct the images from the pose parameters and identity of the correct top-level capsule. Adversarial images do not look like a typical member of the predicted class and they have much larger reconstruction errors when the reconstruction is produced from the top-level capsule for that class. We show that setting a threshold on the l2 distance between the input image and its reconstruction from the winning capsule is very effective at detecting adversarial images for three different datasets. The same technique works quite well for CNNs that have been trained to reconstruct the image from all or part of the last hidden layer before the softmax. We then explore a stronger  white-box attack that takes the reconstruction error into account. This attack is able to fool our detection technique but in order to make the model change its prediction to another class  the attack must typically make the ""adversarial"" image resemble images of the other class.            "
2017,ImageNet classification with deep convolutional neural networks.,"We trained a large  deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data  we achieved top-1 and top-5 error rates of 37.5% and 17.0%  respectively  which is considerably better than the previous state-of-the-art. The neural network  which has 60 million parameters and 650 000 neurons  consists of five convolutional layers  some of which are followed by max-pooling layers  and three fully connected layers with a final 1000-way softmax. To make training faster  we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%  compared to 26.2% achieved by the second-best entry.    "
2017,Distilling a Neural Network Into a Soft Decision Tree.,Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional  the relationship between the input and the output is complicated  and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead  explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.             
2017,Regularizing Neural Networks by Penalizing Confident Output Distributions.,We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions  which has been shown to improve exploration in reinforcement learning  acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10)  language modeling (Penn Treebank)  machine translation (WMT'14 English-to-German)  and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.          
2017,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.,The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation  where parts of the network are active on a per-example basis  has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice  however  there are significant algorithmic and performance challenges.  In this work  we address these challenges and finally realize the promise of conditional computation  achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE)  consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation  where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks  these models achieve significantly better results than state-of-the-art at lower computational cost.      
2017,Dynamic Routing Between Capsules.,A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions  via transformation matrices  for the instantiation parameters of higher-level capsules. When multiple predictions agree  a higher level capsule becomes active. We show that a discrimininatively trained  multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.           
2017,Boltzmann Machines.,A Boltzmann machine is a network of symmetrically connected  neuron-like units that make stochastic decisions about whether to be on or off. Boltzmann machines have a simple learning algorithm that allows them to discover interesting features that represent complex regularities in the training data. The learning algorithm is very slow in networks with many layers of feature detectors  but it is fast in “restricted Boltzmann machines” that have a single layer of feature detectors. Many hidden layers can be learned efficiently by composing restricted Boltzmann machines  using the feature activations of one as the training data for the next. Boltzmann machines are used to solve two quite different computational problems. For a search problem   the weights on the connections are fixed and are used to represent a cost function. The stochastic dynamics of a Boltzmann machine then allow it to sample binary state vectors that have low values of the cost...
2017,Deep Belief Nets.,Deep belief nets are probabilistic generative models that are composed of multiple layers of stochastic latent variables (also called “feature detectors” or “hidden units”). The top two layers have undirected  symmetric connections between them and form an associative memory. The lower layers receive top-down  directed connections from the layer above. Deep belief nets have two important computational properties. First  there is an efficient procedure for learning the top-down  generative weights that specify how the variables in one layer determine the probabilities of variables in the layer below. This procedure learns one layer of latent variables at a time. Second  after learning multiple layers  the values of the latent variables in every layer can be inferred by a single  bottom-up pass that starts with an observed data vector in the bottom layer and uses the generative weights in the reverse direction.        
2017,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.,The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation  where parts of the network are active on a per-example basis  has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice  however  there are significant algorithmic and performance challenges. In this work  we address these challenges and finally realize the promise of conditional computation  achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE)  consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation  where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks  these models achieve significantly better results than state-of-the-art at lower computational cost.      
2017,Regularizing Neural Networks by Penalizing Confident Output Distributions.,We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions  which has been shown to improve exploration in reinforcement learning  acts as a strong regularizer in supervised learning. Furthermore  we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10)  language modeling (Penn Treebank)  machine translation (WMT'14 English-to-German)  and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters  suggesting the wide applicability of these regularizers.        
2017,Who Said What: Modeling Individual Labelers Improves Classification.,Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree  the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches  however  do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information  we propose modeling the experts individually and then learning averaging weights for combining them  possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona (2010)  and by Mnih and Hinton (2012). Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training.         
2017,Dynamic Routing Between Capsules.,A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions  via transformation matrices  for the instantiation parameters of higher-level capsules. When multiple predictions agree  a higher level capsule becomes active. We show that a discrimininatively trained  multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.           
2017,Distilling a Neural Network Into a Soft Decision Tree.,Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional  the relationship between the input and the output is complicated  and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead  explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.            
2016,"Attend, Infer, Repeat: Fast Scene Understanding with Generative Models.",We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially  the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting  locating and classifying the elements of a scene - without any supervision  e.g.  decomposing 3D images with various numbers of objects in a single forward pass of a neural network at unprecedented speed. We further show that the networks produce accurate inferences when compared to supervised counterparts  and that their structure leads to improved generalization.          
2016,Using Fast Weights to Attend to the Recent Past.,Until recently  research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs  outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These fast weights'' can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proven helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.             
2016,"Attend, Infer, Repeat: Fast Scene Understanding with Generative Models.",We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially  the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting  locating and classifying the elements of a scene - without any supervision  e.g.  decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts  and that their structure leads to improved generalization.          
2016,Layer Normalization.,Training state-of-the-art  deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However  the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper  we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization  we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization  layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically  we show that layer normalization can substantially reduce the training time compared with previously published techniques.         
2016,Using Fast Weights to Attend to the Recent Past.,"Until recently  research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs  outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These ""fast weights"" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.             "
2015,Deep learning.,Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition  visual object recognition  object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images  video  speech and audio  whereas recurrent nets have shone light on sequential data such as text and speech.          
2015,Grammar as a Foreign Language.,Syntactic constituency parsing is a fundamental problem in natural language processing which has been the subject of intensive researchand engineering for decades. As a result  the most accurate parsers are domain specific  complex  and inefficient. In this paper we showthat the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntacticconstituency parsing dataset  when trained on a large synthetic corpusthat was annotated using existing parsers. It also matches the performance of standard parsers when trained on a small human-annotated dataset  which shows that this model is highly data-efficient  in contrast to sequence-to-sequence models without theattention mechanism. Our parser is also fast  processing over a hundred sentences per second with an unoptimized CPU implementation.        
2015,Distilling the Knowledge in a Neural Network.,A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately  making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users  especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts  these specialist models can be trained rapidly and in parallel.            
2015,A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.,Learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients. To overcome this difficulty  researchers have developed sophisticated optimization techniques and network architectures. In this paper  we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to LSTM on our four benchmarks: two toy problems involving long-range temporal structures  a large language modeling problem and a benchmark speech recognition problem.            
2014,Where Do Features Come From?,It is possible to learn multiple layers of non‐linear features by backpropagating error derivatives through a feedforward neural network. This is a very effective learning procedure when there is a huge amount of labeled training data  but for many learning tasks very few labeled examples are available. In an effort to overcome the need for labeled data  several different generative models were developed that learned interesting features by modeling the higher order statistical structure of a set of input vectors. One of these generative models  the restricted Boltzmann machine (RBM)  has no connections between its hidden units and this makes perceptual inference and learning much simpler. More significantly  after a layer of hidden features has been learned  the activities of these features can be used as training data for another RBM. By applying this idea recursively  it is possible to learn a deep hierarchy of progressively more complicated features without requiring any labeled data. This deep hierarchy can then be treated as a feedforward neural network which can be discriminatively fine‐tuned using backpropagation. Using a stack of RBMs to initialize the weights of a feedforward neural network allows backpropagation to work effectively in much deeper networks and it leads to much better generalization. A stack of RBMs can also be used to initialize a deep Boltzmann machine that has many hidden layers. Combining this initialization method with a new method for fine‐tuning the weights finally leads to the first efficient way of training Boltzmann machines with many hidden layers and millions of weights.        
2014,Dropout: a simple way to prevent neural networks from overfitting.,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However  overfitting is a serious problem in such networks. Large networks are also slow to use  making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training  dropout samples from an exponential number of different ""thinned"" networks. At test time  it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision  speech recognition  document classification and computational biology  obtaining state-of-the-art results on many benchmark data sets.        "
2014,Application of Deep Belief Networks for Natural Language Understanding.,Applications of Deep Belief Nets (DBN) to various problems have been the subject of a number of recent studies ranging from image classification and speech recognition to audio classification. In this study we apply DBNs to a natural language understanding problem. The recent surge of activity in this area was largely spurred by the development of a greedy layer-wise pretraining method that uses an efficient learning algorithm called Contrastive Divergence (CD). CD allows DBNs to learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms: Support Vector Machines (SVM)  boosting and Maximum Entropy (MaxEnt). The plain DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models. However  using additional unlabeled data for DBN pre-training and combining DBN-based learned features with the original features provides significant gains over SVMs  which  in turn  performed better than both MaxEnt and Boosting.          
2014,Autoregressive product of multi-frame predictions can improve the accuracy of hybrid models.,We describe a simple but effective way of using multi-frame targets to improve the accuracy of Artificial Neural Network-Hidden Markov Model (ANN-HMM) hybrid systems. In this approach a Deep Neural Network (DNN) is trained to predict the forced-alignment state of multiple frames using a separate softmax for each of the frames. This is in contrast to the usual method of training a DNN to predict only the state of the central frame. By itself this is not sufficient to improve accuracy of the system significantly. However  if we average the predictions for each frame — from the different contexts it is associated with — we achieve state of the art results on TIMIT using a fully connected Deep Neural Network without convolutional architectures or dropout training. On a 14 hour subset of Wall Street Journal (WSJ) using a context dependent DNN-HMM system it leads to a relative improvement of 6.4% on the dev set ( test-dev93) and 9.3% on test set ( test-eval92).              
2014,Grammar as a Foreign Language.,Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result  the most accurate parsers are domain specific  complex  and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset  when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset  which shows that this model is highly data-efficient  in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast  processing over a hundred sentences per second with an unoptimized CPU implementation.        
2013,Modeling Natural Images Using Gated MRFs.,This paper describes a Markov Random Field for real-valued image modelling that has two sets of latent variables. One set is used to gate the interactions between all pairs of pixels while the second set determines the mean intensities of each pixel. This is a powerful model with a conditional distribution over the input that is Gaussian with both mean and covariance determined by the configuration of latent variables  which is unlike previous models that were restricted to use Gaussians with either a fixed mean or a diagonal covariance matrix. Thanks to the increased flexibility  this gated MRF can generate more realistic samples after training on an unconstrained distribution of high-resolution natural images. Futhermore  the latent variables of the model can be inferred efficiently and can be used as very effective descriptors in recognition tasks. Both generation and discrimination drastically improce as layers of binary latent variables are added to the model  yielding a hierarchical model called a Deep Belief Network.           
2013,On rectified linear units for speech processing.,Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity  which is typically a logistic function. In this work  we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting  we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting  we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data.           
2013,Speech recognition with deep recurrent neural networks.,Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful  delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing  with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks  which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation  we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark  which to our knowledge is the best recorded score.          
2013,New types of deep neural network learning for speech recognition and related applications: an overview.,In this paper  we provide an overview of the invited and contributed papers presented at the special session at ICASSP-2013  entitled “New Types of Deep Neural Network Learning for Speech Recognition and Related Applications ” as organized by the authors. We also describe the historical context in which acoustic models based on deep neural networks have been developed. The technical overview of the papers presented in our special session is organized into five ways of improving deep learning methods: (1) better optimization; (2) better types of neural activation function and better network architectures; (3) better ways to determine the myriad hyper-parameters of deep neural networks; (4) more appropriate ways to preprocess speech for deep neural networks; and (5) ways of leveraging multiple languages or dialects that are more easily achieved with deep neural networks than with Gaussian mixture models.            
2013,Improving deep neural networks for LVCSR using rectified linear units and dropout.,Recently  pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random “dropout” procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid over-fitting  it has also been successful on a small-scale phone recognition task using larger neural nets. However  training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectified linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units  sometimes also improving discriminative performance. In this work  we show on a 50-hour English Broadcast News task that modified deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2% relative improvement over a DNN trained with sigmoid units  and a 14.4% relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code.         
2013,Tensor Analyzers.,Factor Analysis is a statistical method that seeks to explain linear variations in data by using unobserved latent variables. Due to its additive nature  it is not suitable for modeling data that is generated by multiple groups of latent factors which interact multiplicatively. In this paper  we introduce Tensor Analyzers which are a multilinear generalization of Factor Analyzers. We describe an efficient way of sampling from the posterior distribution over factor values and we demonstrate that these samples can be used in the EM algorithm for learning interesting mixture models of natural image patches. Tensor Analyzers can also accurately recognize a face under significant pose and illumination variations when given only one previous image of that face. We also show that Tensor Analyzers can be trained in an unsupervised  semi-supervised  or fully supervised settings.           
2013,On the importance of initialization and momentum in deep learning.,Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper  we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter  it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore  carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.            
2013,Using an autoencoder with deformable templates to discover features for automated speech recognition.,In this paper we show how we can discover non-linear features of frames of spectrograms using a novel autoencoder. The autoencoder uses a neural network encoder that predicts how a set of prototypes called templates need to be transformed to reconstruct the data  and a decoder that is a function that performs this operation of transforming prototypes and reconstructing the input. We demonstrate this method on spectrograms from the TIMIT database. The features are used in a Deep Neural Network - Hidden Markov Model (DNN-HMM) hybrid system for automatic speech recognition. On the TIMIT monophone recognition task we were able to achieve gains of 0.5% over Mel log spectra  by augmenting traditional the spectra with the predicted transformation parameters. Further  using the recently discovered edropoutf training  we were able to achieve a phone error rate (PER) of 17.9% on the dev set and 19.5% on the test set  which  to our knowledge is the best reported number on this task using a hybrid system. Speaking Rate Normalization with Lattice-Based Context-Dependent Phoneme Duration Modeling for Personalized Speech Recognizers on Mobile Devices         
2013,Modeling Documents with Deep Boltzmann Machines.,We introduce a Deep Boltzmann Machine model suitable for modeling and extracting latent semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This parameter tying enables an efficient pretraining algorithm and a state initialization scheme that aids inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA  Replicated Softmax  and DocNADE models on document retrieval and document classification tasks.             
2013,Discovering Multiple Constraints that are Frequently Approximately Satisfied.,Some high-dimensional data.sets can be modelled by assuming that there are many different linear constraints  each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations.              
2013,Speech Recognition with Deep Recurrent Neural Networks.,Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful  delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing  with better results returned by deep feedforward networks. This paper investigates \emph{deep recurrent neural networks}  which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation  we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark  which to our knowledge is the best recorded score.          
2013,Modeling Documents with Deep Boltzmann Machines.,We introduce a Deep Boltzmann Machine model suitable for modeling and extracting latent semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This parameter tying enables an efficient pretraining algorithm and a state initialization scheme that aids inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA  Replicated Softmax  and DocNADE models on document retrieval and document classification tasks.             
2012,Visualizing non-metric similarities in multiple maps.,Techniques for multidimensional scaling visualize objects as points in a low-dimensional metric map. As a result  the visualizations are subject to the fundamental limitations of metric spaces. These limitations prevent multidimensional scaling from faithfully representing non-metric similarity data such as word associations or event co-occurrences. In particular  multidimensional scaling cannot faithfully represent intransitive pairwise similarities in a visualization  and it cannot faithfully visualize “central” objects. In this paper  we present an extension of a recently proposed multidimensional scaling technique called t-SNE. The extension aims to address the problems of traditional multidimensional scaling techniques when these techniques are used to visualize non-metric similarities. The new technique  called multiple maps t-SNE  alleviates these problems by constructing a collection of maps that reveal complementary structure in the similarity data. We apply multiple maps t-SNE to a large data set of word association data and to a data set of NIPS co-authorships  demonstrating its ability to successfully visualize non-metric similarities.        
2012,An Efficient Learning Procedure for Deep Boltzmann Machines.,We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent statistics are estimated using a variational approximation that tends to focus on a single mode  and data-independent statistics are estimated using persistent Markov chains. The use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer pretraining phase that initializes the weights sensibly. The pretraining also allows the variational inference to be initialized sensibly with a single bottom-up pass. We present results on the MNIST and NORB data sets showing that deep Boltzmann machines learn very good generative models of handwritten digits and 3D objects. We also show that the features discovered by deep Boltzmann machines are a very effective way to initialize the hidden layers of feedforward neural nets  which are then discriminatively fine-tuned.             
2012,Introduction to the Special Section on Deep Learning for Speech and Language Processing.,Current speech recognition systems  for example  typically use Gaussian mixture models (GMMs)  to estimate the observation (or emission) probabilities of hidden Markov models (HMMs)  and GMMs are generative models that have only one layer of latent variables. Instead of developing more powerful models  most of the research effort has gone into finding better ways of estimating the GMM parameters so that error rates are decreased or the margin between different classes is increased. The same observation holds for natural language processing (NLP) in which maximum entropy (MaxEnt) models and conditional random fields (CRFs) have been popular for the last decade. Both of these approaches use shallow models whose success largely depends on the use of carefully handcrafted features.          
2012,Acoustic Modeling Using Deep Belief Networks.,Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features  we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.              
2012,Robust Boltzmann Machines for recognition and denoising.,While Boltzmann Machines have been successful at unsupervised learning and density modeling of images and speech data  they can be very sensitive to noise in the data. In this paper  we introduce a novel model  the Robust Boltzmann Machine (RoBM)  which allows Boltzmann Machines to be robust to corruptions. In the domain of visual recognition  the RoBM is able to accurately deal with occlusions and noise by using multiplicative gating to induce a scale mixture of Gaussians over pixels. Image denoising and in-painting correspond to posterior inference in the RoBM. Our model is trained in an unsupervised fashion with unlabeled noisy data and can learn the spatial structure of the occluders. Compared to standard algorithms  the RoBM is significantly better at recognition and denoising on several face databases.         
2012,Understanding how Deep Belief Networks perform acoustic modelling.,Deep Belief Networks (DBNs) are a very competitive alternative to Gaussian mixture models for relating states of a hidden Markov model to frames of coefficients derived from the acoustic input. They are competitive for three reasons: DBNs can be fine-tuned as neural networks; DBNs have many non-linear hidden layers; and DBNs are generatively pre-trained. This paper illustrates how each of these three aspects contributes to the DBN's good recognition performance using both phone recognition performance on the TIMIT corpus and a dimensionally reduced visualization of the relationships between the feature vectors learned by the DBNs that preserves the similarity structure of the feature vectors at multiple scales. The same two methods are also used to investigate the most suitable type of input representation for a DBN.               
2012,Learning to Label Aerial Images from Noisy Data.,When training a system to label images  the amount of labeled training data tends to be a limiting factor. We consider the task of learning to label aerial images from existing maps. These provide abundant labels  but the labels are often incomplete and sometimes poorly registered. We propose two robust loss functions for dealing with these kinds of label noise and use the loss functions to train a deep neural network on two challenging aerial image datasets. The robust loss functions lead to big improvements in performance and our best system substantially outperforms the best published results on the task we consider.             
2012,Deep Mixtures of Factor Analysers.,An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables. After learning each layer  samples from the posterior distributions for that layer are used as training data for learning the next layer. This approach is commonly used with Restricted Boltzmann Machines  which are undirected graphical models with a single hidden layer  but it can also be used with Mixtures of Factor Analysers (MFAs) which are directed graphical models. In this paper  we present a greedy layer-wise learning algorithm for Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted to an equivalent shallow MFA by multiplying together the factor loading matrices at different levels  learning and inference are much more efficient in a DMFA and the sharing of each lower-level factor loading matrix by many different higher level MFAs prevents overfitting. We demonstrate empirically that DMFAs learn better density models than both MFAs and two types of Restricted Boltzmann Machine on  a wide variety of datasets.          
2012,Deep Lambertian Networks.,Visual perception is a challenging problem in part due to illumination variations. A possible solution is to first estimate an illumination invariant representation before using it for recognition. The object albedo and surface normals are examples of such representations. In this paper  we introduce a multilayer generative model where the latent variables include the albedo  surface normals  and the light source. Combining Deep Belief Nets with the Lambertian reflectance assumption  our model can learn good priors over the albedo from 2D images. Illumination variations can be explained by changing only the lighting latent variable in our model. By transferring learned knowledge from similar objects  albedo and surface normals estimation from a single image is possible in our model. Experiments demonstrate that our model is able to generalize as well as improve over standard baselines in one-shot face recognition.           
2012,ImageNet Classification with Deep Convolutional Neural Networks.,We trained a large  deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data  we achieved top-1 and top-5 error rates of 39.7\% and 18.9\% which is considerably better than the previous state-of-the-art results. The neural network  which has 60 million parameters and 500 000 neurons  consists of five convolutional layers  some of which are followed by max-pooling layers  and two globally connected layers with a final 1000-way softmax. To make training faster  we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.       
2012,A Better Way to Pretrain Deep Boltzmann Machines.,We describe how the pre-training algorithm for Deep Boltzmann Machines (DBMs) is related to the pre-training algorithm for Deep Belief Networks and we show that under certain conditions  the pre-training procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis  we develop a different method of pre-training DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pre-training algorithm allows us to learn better generative models.             
2012,A Practical Guide to Training Restricted Boltzmann Machines.,Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years  the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.              
2012,Conditional Restricted Boltzmann Machines for Structured Output Prediction.,Conditional Restricted Boltzmann Machines (CRBMs) are rich probabilistic models that have recently been applied to a wide range of problems  including collaborative filtering  classification  and modeling motion capture data. While much progress has been made in training non-conditional RBMs  these algorithms are not applicable to conditional models and there has been almost no work on training and generating predictions from conditional RBMs for structured output problems. We first argue that standard Contrastive Divergence-based learning may not be suitable for training CRBMs. We then identify two distinct types of structured output prediction problems and propose an improved learning algorithm for each. The first problem type is one where the output space has arbitrary structure but the set of likely output configurations is relatively small  such as in multi-label classification. The second problem is one where the output space is arbitrarily structured but where the output space variability is much greater  such as in image denoising or pixel labeling. We show that the new learning algorithms can work much better than Contrastive Divergence on both types of problems.         
2012,Products of Hidden Markov Models: It Takes N > 1 to Tango.,Products of Hidden Markov Models(PoHMMs) are an interesting class of generative models which have received little attention since their introduction. This maybe in part due to their more computationally expensive gradient-based learning algorithm and the intractability of computing the log likelihood of sequences under the model. In this paper  we demonstrate how the partition function can be estimated reliably via Annealed Importance Sampling. We perform experiments using contrastive divergence learning on rainfall data and data captured from pairs of people dancing. Our results suggest that advances in learning and evaluation for undirected graphical models and recent increases in available computing power make PoHMMs worth considering for complex time-series modeling tasks.             
2012,Deep Mixtures of Factor Analysers.,An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables. After learning each layer  samples from the posterior distributions for that layer are used as training data for learning the next layer. This approach is commonly used with Restricted Boltzmann Machines  which are undirected graphical models with a single hidden layer  but it can also be used with Mixtures of Factor Analysers (MFAs) which are directed graphical models. In this paper  we present a greedy layer-wise learning algorithm for Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted to an equivalent shallow MFA by multiplying together the factor loading matrices at different levels  learning and inference are much more efficient in a DMFA and the sharing of each lower-level factor loading matrix by many different higher level MFAs prevents overfitting. We demonstrate empirically that DMFAs learn better density models than both MFAs and two types of Restricted Boltzmann Machine on a wide variety of datasets.          
2012,Improving neural networks by preventing co-adaptation of feature detectors.,"When a large feedforward neural network is trained on a small training set  it typically performs poorly on held-out test data. This ""overfitting"" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead  each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random ""dropout"" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.             "
2012,Efficient Parametric Projection Pursuit Density Estimation.,Product models of low dimensional experts are a powerful way to avoid the curse of dimensionality. We present the ``under-complete product of experts' (UPoE)  where each expert models a one dimensional projection of the data. The UPoE is fully tractable and may be interpreted as a parametric probabilistic model for projection pursuit. Its ML learning rules are identical to the approximate learning rules proposed before for under-complete ICA. We also derive an efficient sequential learning algorithm and discuss its relationship to projection pursuit density estimation and feature induction algorithms for additive random field models.              
2011,A better way to learn features: technical perspective.!,N/A
2011,Two Distributed-State Models For Generating High-Dimensional Time Series.,"In this paper we develop a class of nonlinear generative models for high-dimensional time series. We first propose a model based on the restricted Boltzmann machine (RBM) that uses an undirected model with binary latent variables and real-valued ""visible"" variables. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. This ""conditional"" RBM (CRBM) makes on-line inference efficient and allows us to use a simple approximate learning procedure. We demonstrate the power of our approach by synthesizing various sequences from a model trained on motion capture data and by performing on-line filling in of data lost during capture.                "
2011,Discovering Binary Codes for Documents by Learning Deep Generative Models.,We describe a deep generative model in which the lowest layer represents the word‐count vector of a document and the top layer represents a learned binary code for that document. The top two layers of the generative model form an undirected associative memory and the remaining layers form a belief net with directed  top‐down connections. We present efficient learning and inference procedures for this type of generative model and show that it allows more accurate and much faster retrieval than latent semantic analysis. By using our method as a filter for a much slower method called TF‐IDF we achieve higher accuracy than TF‐IDF alone and save several orders of magnitude in retrieval time. By using short binary codes as addresses  we can perform retrieval on very large document sets in a time that is independent of the size of the document set using only one word of memory to describe each document.             
2011,Modeling the joint density of two images under a variety of transformations.,We describe a generative model of the relationship between two images. The model is defined as a factored three-way Boltzmann machine  in which hidden variables collaborate to define the joint correlation matrix for image pairs. Modeling the joint distribution over pairs makes it possible to efficiently match images that are the same according to a learned measure of similarity. We apply the model to several face matching tasks  and show that it learns to represent the input images using task-specific basis functions. Matching performance is superior to previous similar generative models  including recent conditional models of transformations. We also show that the model can be used as a plug-in matching score to perform invariant classification.            
2011,On deep generative models with applications to recognition.,The most popular way to use probabilistic models in vision is first to extract some descriptors of small image patches or object parts using well-engineered features  and then to use statistical learning tools to model the dependencies among these features and eventual labels. Learning probabilistic models directly on the raw pixel values has proved to be much more difficult and is typically only used for regularizing discriminative methods. In this work  we use one of the best  pixel-level  generative models of natural images-a gated MRF-as the lowest level of a deep belief network (DBN) that has several hidden layers. We show that the resulting DBN is very good at coping with occlusion when predicting expression categories from face images  and it can produce features that perform comparably to SIFT descriptors for discriminating different types of scene. The generative ability of the model also makes it easy to see what information is captured and what is lost at each level of representation.          
2011,Using very deep autoencoders for content-based image retrieval.,We show how to learn many layers of features on color images and we use these features to initialize deep autoencoders. We then use the autoencoders to map images to short binary codes. Using semantic hashing  28-bit codes can be used to retrieve images that are similar to a query image in a time that is independent of the size of the database. This extremely fast retrieval makes it possible to search using multiple different transformations of the query image. 256-bit binary codes allow much more accurate matching and can be used to prune the set of images found using the 28-bit codes.               
2011,Transforming Auto-Encoders.,The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast  the computer vision community uses complicated  hand-engineered features  like SIFT [6]  that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position  orientation  scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the hand-engineered features currently used in computer vision because it provides an efficient way of adapting the features to the domain.         
2011,Deep Belief Networks using discriminative features for phone recognition.,Deep Belief Networks (DBNs) are multi-layer generative models. They can be trained to model windows of coefficients extracted from speech and they discover multiple layers of features that capture the higher-order statistical structure of the data. These features can be used to initialize the hidden units of a feed-forward neural network that is then trained to predict the HMM state for the central frame of the window. Initializing with features that are good at generating speech makes the neural network perform much better than initializing with random weights. DBNs have already been used successfully for phone recognition with input coefficients that are MFCCs or filterbank outputs. In this paper  we demonstrate that they work even better when their inputs are speaker adaptive  discriminative features. On the standard TIMIT corpus  they give phone error rates of 19.6% using monophone HMMs and a bigram language model and 19.4% using monophone HMMs and a trigram language model.            
2011,Deep belief nets for natural language call-routing.,This paper considers application of Deep Belief Nets (DBNs) to natural language call routing. DBNs have been successfully applied to a number of tasks  including image  audio and speech classification  thanks to the recent discovery of an efficient learning technique. DBNs learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms; Support Vector machines (SVM)  Boosting and Maximum Entropy (MaxEnt). The DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models even though it currently uses an impoverished representation of the input.           
2011,Learning a better representation of speech soundwaves using restricted boltzmann machines.,State of the art speech recognition systems rely on preprocessed speech features such as Mel cepstrum or linear predictive coding coefficients that collapse high dimensional speech sound waves into low dimensional encodings. While these have been successfully applied in speech recognition systems  such low dimensional encodings may lose some relevant information and express other information in a way that makes it difficult to use for discrimination. Higher dimensional encodings could both improve performance in recognition tasks  and also be applied to speech synthesis by better modeling the statistical structure of the sound waves. In this paper we present a novel approach for modeling speech sound waves using a Restricted Boltzmann machine (RBM) with a novel type of hidden variable and we report initial results demonstrating phoneme recognition performance better than the current state-of-the-art for methods based on Mel cepstrum coefficients.              
2011,Generating Text with Recurrent Neural Networks.,"Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately  recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs  making it possible to apply them successfully to challenging seuence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modelling tasks. The standard RNN architecture  while effective  is not ideally suited for such tasks  so we introduce a new RNN variant that uses multiplicative (or ""gated"") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units  we were able to surpass the performance of the best previous single method for character-level language modeling - a hierachical non-parametric sequence model. To our knowledge this represents the largest recurrent neural network application to date.          "
2011,Conditional Restricted Boltzmann Machines for Structured Output Prediction.,Conditional Restricted Boltzmann Machines (CRBMs) are rich probabilistic models that have recently been applied to a wide range of problems  including collaborative filtering  classification  and modeling motion capture data. While much progress has been made in training non-conditional RBMs  these algorithms are not applicable to conditional models and there has been almost no work on training and generating predictions from conditional RBMs for structured output problems. We first argue that standard Contrastive Divergence-based learning may not be suitable for training CRBMs. We then identify two distinct types of structured output prediction problems and propose an improved learning algorithm for each. The first problem type is one where the output space has arbitrary structure but the set of likely output configurations is relatively small  such as in multi-label classification. The second problem is one where the output space is arbitrarily structured but where the output space variability is much greater  such as in image denoising or pixel labeling. We show that the new learning algorithms can work much better than Contrastive Divergence on both types of problems.         
2010,Learning to Represent Spatial Transformations with Factored Higher-Order Boltzmann Machines.,To allow the hidden units of a restricted Boltzmann machine to model the transformation between two successive images  Memisevic and Hinton (2007) introduced three-way multiplicative interactions that use the intensity of a pixel in the first image as a multiplicative gain on a learned  symmetric weight between a pixel in the second image and a hidden unit. This creates cubically many parameters  which form a three-dimensional interaction tensor. We describe a low-rank approximation to this interaction tensor that uses a sum of factors  each of which is a three-way outer product. This approximation allows efficient learning of transformations between larger image patches. Since each factor can be viewed as an image filter  the model as a whole learns optimal filter pairs for efficiently representing transformations. We demonstrate the learning of optimal filter pairs from various synthetic and real image sequences. We also show how learning about image transformations allows the model to perform a simple visual analogy task  and we show how a completely unsupervised network trained on transformations perceives multiple motions of transparent dot patterns in the same way as humans.         
2010,Comparing Classification Methods for Longitudinal fMRI Studies.,We compare 10 methods of classifying fMRI volumes by applying them to data from a longitudinal study of stroke recovery: adaptive Fisher's linear and quadratic discriminant; gaussian naive Bayes; support vector machines with linear  quadratic  and radial basis function (RBF) kernels; logistic regression; two novel methods based on pairs of restricted Boltzmann machines (RBM); and K-nearest neighbors. All methods were tested on three binary classification tasks  and their out-of-sample classification accuracies are compared. The relative performance of the methods varies considerably across subjects and classification tasks. The best overall performers were adaptive quadratic discriminant  support vector machines with RBF kernels  and generatively trained pairs of RBMs.          
2010,Temporal-Kernel Recurrent Neural Networks.,A Recurrent Neural Network (RNN) is a powerful connectionist model that can be applied to many challenging sequential problems  including problems that naturally arise in language and speech. However  RNNs are extremely hard to train on problems that have long-term dependencies  where it is necessary to remember events for many timesteps before using them to make a prediction.            
In this paper we consider the problem of training RNNs to predict sequences that exhibit significant long-term dependencies, focusing on a serial recall task where the RNN needs to remember a sequence of characters for a large number of steps before reconstructing it. We introduce the Temporal-Kernel Recurrent Neural Network (TKRNN), which is a variant of the RNN that can cope with long-term dependencies much more easily than a standard RNN  and show that the TKRNN develops short-term memory that successfully solves the serial recall task by representing the input string with a stable state of its hidden units.              
2010,Dynamical binary latent variable models for 3D human pose tracking.,We introduce a new class of probabilistic latent variable model called the Implicit Mixture of Conditional Restricted Boltzmann Machines (imCRBM) for use in human pose tracking. Key properties of the imCRBM are as follows: (1) learning is linear in the number of training exemplars so it can be learned from large datasets; (2) it learns coherent models of multiple activities; (3) it automatically discovers atomic “movemes” and (4) it can infer transitions between activities  even when such transitions are not present in the training set. We describe the model and how it is learned and we demonstrate its use in the context of Bayesian filtering for multi-view and monocular pose tracking. The model handles difficult scenarios including multiple activities and transitions among activities. We report state-of-the-art results on the HumanEva dataset.              
2010,Modeling pixel means and covariances using factorized third-order boltzmann machines.,Learning a generative model of natural images is a useful way of extracting features that capture interesting regularities. Previous work on learning such models has focused on methods in which the latent features are used to determine the mean and variance of each pixel independently  or on methods in which the hidden units determine the covariance matrix of a zero-mean Gaussian distribution. In this work  we propose a probabilistic model that combines these two approaches into a single framework. We represent each image using one set of binary latent features that model the image-specific covariance and a separate set that model the mean. We show that this approach provides a probabilistic framework for the widely used simple-cell complex-cell architecture  it produces very realistic samples of natural images and it extracts features that yield state-of-the-art recognition accuracy on the challenging CIFAR 10 dataset.            
2010,Learning to Detect Roads in High-Resolution Aerial Images.,Reliably extracting information from aerial imagery is a difficult problem with many practical applications. One specific case of this problem is the task of automatically detecting roads. This task is a difficult vision problem because of occlusions  shadows  and a wide variety of non-road objects. Despite 30 years of work on automatic road detection  no automatic or semi-automatic road detection system is currently on the market and no published method has been shown to work reliably on large datasets of urban imagery. We propose detecting roads using a neural network with millions of trainable weights which looks at a much larger context than was used in previous attempts at learning the task. The network is trained on massive amounts of data using a consumer GPU. We demonstrate that predictive performance can be substantially improved by initializing the feature detectors using recently developed unsupervised learning methods as well as by taking advantage of the local spatial coherence of the output labels. We show that our method works reliably on two challenging urban datasets that are an order of magnitude larger than what was used to evaluate previous approaches.            
2010,Phone recognition using Restricted Boltzmann Machines.,For decades  Hidden Markov Models (HMMs) have been the state-of-the-art technique for acoustic modeling despite their unrealistic independence assumptions and the very limited representational capacity of their hidden states. Conditional Restricted Boltzmann Machines (CRBMs) have recently proved to be very effective for modeling motion capture sequences and this paper investigates the application of this more powerful type of generative model to acoustic modeling. On the standard TIMIT corpus  one type of CRBM outperforms HMMs and is comparable with the best other methods  achieving a phone error rate (PER) of 26.7% on the TIMIT core test set.            
2010,Rectified Linear Units Improve Restricted Boltzmann Machines.,"Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these ""Stepped Sigmoid Units"" are unchanged. They can be approximated efficiently by noisy  rectified linear units. Compared with binary units  these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units  rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.             "
2010,Binary coding of speech spectrograms using a deep auto-encoder.,This paper reports our recent exploration of the layer-by-layer learning strategy for training a multi-layer generative model of patches of speech spectrograms. The top layer of the generative model learns binary codes that can be used for efficient compression of speech and could also be used for scalable speech recognition or rapid speech content retrieval. Each layer of the generative model is fully connected to the layer below and the weights on these connections are pre-trained efficiently by using the contrastive divergence approximation to the log likelihood gradient. After layer-by-layer pre-training we �unroll� the generative model to form a deep auto-encoder  whose parameters are then fine-tuned using back-propagation. To reconstruct the full-length speech spectrogram  individual spectrogram segments predicted by their respective binary codes are combined using an overlap-and-add method. Experimental results on speech spectrogram coding demonstrate that the binary codes produce a log-spectral distortion that is approximately 2 dB lower than a sub-band vector quantization technique over the entire frequency range of wide-band speech.             
2010,Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine.,Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However  the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation  shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM)  first introduced for modeling natural images  is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work  we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5\%  which is superior to all published results on speaker-independent TIMIT to date.         
2010,Learning to combine foveal glimpses with a third-order Boltzmann machine.,"We describe a model based on a Boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several fixations. The model uses a retina that only has enough high resolution pixels to cover a small area of the image  so it must decide on a sequence of fixations and it must combine the glimpse"" at each fixation with the location of the fixation before integrating the information with information from other glimpses of the same object. We evaluate this model on a synthetic dataset and two image classification datasets  showing that it can perform at least as well as a model trained on whole images.""             "
2010,Gated Softmax Classification.,"We describe a log-bilinear"" model that computes class probabilities by combining an input vector multiplicatively with a vector of binary latent variables. Even though the latent variables can take on exponentially many possible combinations of values  we can efficiently compute the exact probability of each class by marginalizing over the latent variables. This makes it possible to get the exact gradient of the log likelihood. The bilinear score-functions are defined using a three-dimensional weight tensor  and we show that factorizing this tensor allows the model to encode invariances inherent in a task by learning a dictionary of invariant basis functions. Experiments on a set of benchmark problems show that this fully probabilistic model can achieve classification performance that is competitive with (kernel) SVMs  backpropagation  and deep belief nets.""           "
2010,Generating more realistic images using gated MRF's.,Probabilistic models of natural images are usually evaluated by measuring performance on rather indirect tasks  such as denoising and inpainting. A more direct way to evaluate a generative model is to draw samples from it and to check whether statistical properties of the samples match the statistics of natural images. This method is seldom used with high-resolution images  because current models produce samples that are very different from natural images  as assessed by even simple visual inspection. We investigate the reasons for this failure and we show that by augmenting existing models so that there are two sets of latent variables  one set modelling pixel intensities and the other set modelling image-specific pixel covariances  we are able to generate high-resolution images that look much more realistic than before. The overall model can be interpreted as a gated MRF where both pair-wise dependencies and mean intensities of pixels are modulated by the states of latent variables. Finally  we confirm that if we disallow weight-sharing between receptive fields that overlap each other  the gated MRF learns more efficient internal representations  as demonstrated in several recognition tasks.       
2010,Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images.,Deep belief nets have been successful in modeling handwritten characters  but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep belief nets one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3-way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. This provides a probabilistic framework for the widely used simple/complex cell architecture. Our model learns binary features that work very well for object recognition on the “tiny images” data set. Even better features are obtained by then using standard binary RBM’s to learn a deeper model.              
2010,Boltzmann Machines.,"A Boltzmann machine is a network of symmetrically connected  neuron-like units that make stochastic decisions about whether to be on or off. Boltzmann machines have a simple learning algorithm that allows them to discover interesting features that represent complex regularities in the training data. The learning algorithm is very slow in networks with many layers of feature detectors  but it is fast in ""restricted Boltzmann machines"" that have a single layer of feature detectors. Many hidden layers can be learned efficiently by composing restricted Boltzmann machines  using the feature activations of one as the training data for the next. Boltzmann machines are used to solve two quite different computational problems. For a search problem  the weights on the connections are fixed and are used to represent a cost function. The stochastic dynamics of a Boltzmann machine then allow it to sample binary state vectors that have low values of the cost function. For a learning problem  the Boltzmann machine is shown a set of binary data vectors  and it must learn to generate these vectors with high probability. To do this  it must find weights on the connections so that relative to other possible binary vectors  the data vectors have low values of the cost function. To solve a learning problem  Boltzmann machines make many small updates to their weights  and each update requires them to solve many different search problems.      "
2010,Deep Belief Nets.,Deep belief nets are probabilistic generative models that are composed of multiple layers of stochastic latent variables (also called “feature detectors” or “hidden units”). The top two layers have undirected  symmetric connections between them and form an associative memory. The lower layers receive top-down  directed connections from the layer above. Deep belief nets have two important computational properties. First  there is an efficient procedure for learning the top-down  generative weights that specify how the variables in one layer determine the probabilities of variables in the layer below. This procedure learns one layer of latent variables at a time. Second  after learning multiple layers  the values of the latent variables in every layer can be inferred by a single  bottom-up pass that starts with an observed data vector in the bottom layer and uses the generative weights in the reverse direction.        
2009,Semantic hashing.,We show how to learn a deep graphical model of the word-count vectors obtained from a large set of documents. The values of the latent variables in the deepest layer are easy to infer and give a much better representation of each document than Latent Semantic Analysis. When the deepest layer is forced to use a small number of binary variables (e.g. 32)  the graphical model performs “semantic hashing”: Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing  which is the fastest current method. By using semantic hashing to filter the documents given to TF-IDF  we achieve higher accuracy than applying TF-IDF to the entire document set.            
2009,Improving a statistical language model through non-linear prediction.,We show how to improve a state-of-the-art neural network language model that converts the previous “context” words into feature vectors and combines these feature vectors linearly to predict the feature vector of the next word. Significant improvements in predictive accuracy are achieved by using a non-linear subnetwork to modulate the effects of the context words or to produce a non-linear correction term when predicting the feature vector. A log-bilinear language model that incorporates both of these improvements achieves a 26% reduction in perplexity over the best n-gram model on a fairly large dataset.               
2009,Deep belief networks.,Deep belief nets are probabilistic generative models that are composed of multiple layers of stochastic  latent variables. The latent variables typically have binary values and are often called hidden units or feature detectors. The top two layers have undirected  symmetric connections between them and form an associative memory. The lower layers receive top-down  directed connections from the layer above. The states of the units in the lowest layer represent a data vector.The two most significant properties of deep belief nets are: There is an efficient  layer-by-layer procedure for learning the top-down  generative weights that determine how the variables in one layer depend on the variables in the layer above. After learning  the values of the latent variables in every layer can be inferred by a single  bottom-up pass that starts with an observed data vector in the bottom layer and uses the generative weights in the reverse direction. Deep belief nets are learned one layer at a time by treating the values of the latent variables in one layer  when they are being inferred from data  as the data for training the next layer. This efficient  greedy learning can be followed by  or combined with  other learning procedures that fine-tune all of the weights to improve the generative or discriminative performance of the whole network. Discriminative fine-tuning can be performed by adding a final layer of variables that represent the desired outputs and backpropagating error derivatives. When networks with many hidden layers are applied to highly-structured input data  such as images  backpropagation works much better if the feature detectors in the hidden layers are initialized by learning a deep belief net that models the structure in the input data (Hinton & Salakhutdinov  2006).
2009,Learning Generative Texture Models with extended Fields-of-Experts.,We evaluate the ability of the popular Field-of-Experts (FoE) to model structure in images. As a test case we focus on modeling synthetic and natural textures. We find that even for modeling single textures  the FoE provides insufficient flexibility to learn good generative models - it does not perform any better than the much simpler Gaussian FoE. We propose an extended version of the FoE (allowing for bimodal potentials) and demonstrate that this novel formulation  when trained with a better approximation of the likelihood gradient  gives rise to a more powerful generative model of specific visual structure that produces significantly better results for the texture task.             
2009,Modeling pigeon behavior using a Conditional Restricted Boltzmann Machine.,"In an effort to better understand the complex courtship behaviour of pigeons  we have built a model learned from motion capture data. We employ a Conditional Restricted Boltzmann Machine (CRBM) with binary latent features and real-valued visible units. The units are conditioned on information from previous time steps to capture dynamics. We validate a trained model by quantifying the characteristic ""head-bobbing"" present in pigeons. We also show how to predict missing data by marginalizing out the hidden variables and minimizing free energy.              "
2009,Factored conditional restricted Boltzmann Machines for modeling motion style.,The Conditional Restricted Boltzmann Machine (CRBM) is a recently proposed model for time series that has a rich  distributed hidden state and permits simple  exact inference. We present a new model  based on the CRBM that preserves its most important computational properties and includes multiplicative three-way interactions that allow the effective interaction weight between two units to be modulated by the dynamic state of a third unit. We factor the three-way weight tensor implied by the multiplicative model  reducing the number of parameters from O(N3) to O(N2). The result is an efficient  compact model whose effectiveness we demonstrate by modeling human motion. Like the CRBM  our model can capture diverse styles of motion with a single set of parameters  and the three-way interactions greatly improve the model's ability to blend motion styles or to transition smoothly among them.        
2009,Using fast weights to improve persistent contrastive divergence.,"The most commonly used learning algorithm for restricted Boltzmann machines is contrastive divergence which starts a Markov chain at a data point and runs the chain for only a few iterations to get a cheap  low variance estimate of the sufficient statistics under the model. Tieleman (2008) showed that better learning can be achieved by estimating the model's statistics using a small set of persistent ""fantasy particles"" that are not reinitialized to data points after each weight update. With sufficiently small weight updates  the fantasy particles represent the equilibrium distribution accurately but to explain why the method works with much larger weight updates it is necessary to consider the interaction between the weight updates and the Markov chain. We show that the weight updates force the Markov chain to mix fast  and using this insight we develop an even faster mixing chain that uses an auxiliary set of ""fast weights"" to implement a temporary overlay on the energy landscape. The fast weights learn rapidly but also decay rapidly and do not contribute to the normal energy landscape that defines the model.            "
2009,3D Object Recognition with Deep Belief Nets.,We introduce a new type of Deep Belief Net and evaluate it on a 3D object recognition task. The top-level model is a third-order Boltzmann machine  trained using a hybrid algorithm that combines both generative and discriminative gradients. Performance is evaluated on the NORB database(normalized-uniform version)  which contains stereo-pair images of objects under different lighting conditions and viewpoints. Our model achieves 6.5% error on the test set  which is close to the best published result for NORB (5.9%) using a convolutional neural net that has built-in knowledge of translation invariance. It substantially outperforms shallow models such as SVMs (11.6%). DBNs are especially suited for semi-supervised learning  and to demonstrate this we consider a modified version of the NORB recognition task in which additional unlabeled images are created by applying small translations to the images in the database. With the extra unlabeled data (and the same amount of labeled data as before)  our model achieves 5.2% error  making it the current best result for NORB.         
2009,Zero-shot Learning with Semantic Output Codes.,We consider the problem of zero-shot learning  where the goal is to learn a classifier that must predict novel values of Y that were omitted from the training set. To achieve this  we define the notion of a semantic output code classifier (SOC) which utilizes a knowledge base of semantic properties of Y to extrapolate to novel classes. We provide a formalism for this type of classifier and study its theoretical properties in a PAC framework  showing conditions under which the classifier can accurately predict novel classes. As a case study  we build a SOC classifier for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images (fMRI) of their neural activity  even without training examples for those words.          
2009,Replicated Softmax: an Undirected Topic Model.,We show how to model documents as bags of words using family of two-layer  undirected graphical models. Each member of the family has the same number of binary hidden units but a different number of softmax visible units. All of the softmax units in all of the models in the family share the same weights to the binary hidden units. We describe efficient inference and learning procedures for such a family. Each member of the family models the probability distribution of documents of a specific length as a product of topic-specific distributions rather than as a mixture and this gives much better generalization than Latent Dirichlet Allocation for modeling the log probabilities of held-out documents. The low-dimensional topic vectors learned by the undirected family are also much better than LDA topic vectors for retrieving documents that are similar to a query document. The learned topics are more general than those found by LDA because precision is achieved by intersecting many general topics rather than by selecting a single precise topic to generate each word.              
2009,Products of Hidden Markov Models: It Takes N>1 to Tango.,Products of Hidden Markov Models(PoHMMs) are an interesting class of generative models which have received little attention since their introduction. This maybe in part due to their more computationally expensive gradient-based learning algorithm and the intractability of computing the log likelihood of sequences under the model. In this paper  we demonstrate how the partition function can be estimated reliably via Annealed Importance Sampling. We perform experiments using contrastive divergence learning on rainfall data and data captured from pairs of people dancing. Our results suggest that advances in learning and evaluation for undirected graphical models and recent increases in available computing power make PoHMMs worth considering for complex time-series modeling tasks.             
2009,Deep Boltzmann Machines.,We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode  and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.              
2008,"Deep, Narrow Sigmoid Belief Networks Are Universal Approximators.",In this note  we show that exponentially deep belief networks can approximate any distribution over binary vectors to arbitrary accuracy  even when the width of each layer is limited to the dimensionality of the data. We further show that such networks can be greedily learned in an easy yet impractical way.             
2008,Improving a statistical language model by modulating the effects of context words.,"We show how to improve a state-of-the-art neural network language model that converts the previous ""context"" words into feature vectors and combines these feature vectors to predict the feature vector of the next word. Significant improvements in predictive accuracy are achieved by using higher-level features to modulate the effects of the context words. This is more effective than using the higher-level features to directly predict the feature vector of the next word  but it is also possible to combine both methods.              "
2008,Analysis-by-Synthesis by Learning to Invert Generative Black Boxes.,For learning meaningful representations of data  a rich source of prior knowledge may come in the form of a generative black box  e.g. a graphics program that generates realistic facial images. We consider the problem of learning the inverse of a given generative model from data. The problem is non-trivial because it is difficult to create labelled training cases by hand  and the generative mapping is a black box in the sense that there is no analytic expression for its gradient. We describe a way of training a feedforward neural network that starts with just one labelled training example and uses the generative black box to “breed” more training data. As learning proceeds  the training set evolves and the labels that the network assigns to unlabelled training data converge to their correct values. We demonstrate our approach by learning to invert a generative model of eyes and an active appearance model of faces.           
2008,A Scalable Hierarchical Distributed Language Model.,Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words that was two orders of magnitude faster than the non-hierarchical language model it was based on. However  it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical models and achieve state-of-the-art performance.              
2008,Implicit Mixtures of Restricted Boltzmann Machines.,We present a mixture model whose components are Restricted Boltzmann Machines (RBMs). This possibility has not been considered before because computing the partition function of an RBM is intractable  which appears to make learning a mixture of RBMs intractable as well. Surprisingly  when formulated as a third-order Boltzmann machine  such a mixture model can be learned tractably using contrastive divergence. The energy function of the model captures three-way interactions among visible units  hidden units  and a single hidden multinomial unit that represents the cluster labels. The distinguishing feature of this model is that  unlike other mixture models  the mixing proportions are not explicitly parameterized. Instead  they are defined implicitly via the energy function and depend on all the parameters in the model. We present results for the MNIST and NORB datasets showing that the implicit mixture of RBMs learns clusters that reflect the class structure in the data.       
2008,Generative versus discriminative training of RBMs for classification of fMRI images.,Neuroimaging datasets often have a very large number of voxels and a very small number of training cases  which means that overfitting of models for this data can become a very serious problem. Working with a set of fMRI images from a study on stroke recovery  we consider a classification task for which logistic regression performs poorly  even when L1- or L2- regularized. We show that much better discrimination can be achieved by fitting a generative model to each separate condition and then seeing which model is most likely to have generated the data. We compare discriminative training of exactly the same set of models  and we also consider convex blends of generative and discriminative training.           
2008,Using matrices to model symbolic relationship.,We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships  which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn first-order propositions such as  or   and higher-order propositions such as  and  or . We further demonstrate that the system understands how higher-order propositions are related to first-order ones by showing that it can correctly answer questions about first-order propositions involving the relations  or  even though it has not been trained on any first-order examples involving these relations.             
2008,The Recurrent Temporal Restricted Boltzmann Machine.,The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e.  generate nice-looking samples of) several very high dimensional sequences  such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvantage of the TRBM is that exact inference is extremely hard  since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difficulty has necessitated the use of a heuristic inference procedure  that nonetheless was accurate enough for successful learning. In this paper we introduce the Recurrent TRBM  which is a very slight modification of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls.          
2007,Unsupervised Learning of Image Transformations.,We describe a probabilistic model for learning rich  distributed representations of image transformations. The basic model is defined as a gated conditional random field that is trained to predict transformations of its inputs using a factorial set of latent variables. Inference in the model consists in extracting the transformation  given a pair of images  and can be performed exactly and efficiently. We show that  when trained on natural videos  the model develops domain specific motion features  in the form of fields of locally transformed edge filters. When trained on affine  or more general  transformations of still images  the model develops codes for these transformations  and can subsequently perform recognition tasks that are invariant under these transformations. It can also fantasize new transformations on previously unseen images. We describe several variations of the basic model and provide experimental results that demonstrate its applicability to a variety of tasks.     
2007,Three new graphical models for statistical language modelling.,The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram models.               
2007,Restricted Boltzmann machines for collaborative filtering.,Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models  called Restricted Boltzmann Machines (RBM's)  can be used to model tabular data  such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set  containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined  we achieve an error rate that is well over 6% better than the score of Netflix's own system.          
2007,Modeling image patches with a directed hierarchy of Markov random fields.,We describe an efficient learning procedure for multilayer generative models that combine the best aspects of Markov random fields and deep  directed belief nets. The generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers. Each hidden layer has its own MRF whose energy function is modulated by the top-down directed connections from the layer above. To generate from the model  each layer in turn must settle to equilibrium given its top-down input. We show that this type of model is good at capturing the statistics of patches of natural images.             
2007,Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes.,We show how to use unlabeled data and a deep belief net (DBN) to learn a good covariance kernel for a Gaussian process. We first learn a deep generative model of the unlabeled data using the fast  greedy algorithm introduced by Hinton et.al. If the data is high-dimensional and highly-structured  a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classification can then be further improved by using backpropagation through the DBN to discriminatively fine-tune the covariance kernel.             
2007,Visualizing Similarity Data with a Mixture of Maps.,We show how to visualize a set of pairwise similarities between objects by using several different two-dimensional maps  each of which captures different aspects of the similarity structure. When the objects are ambiguous words  for example  different senses of a word occur in different maps  so “river” and “loan” can both be close to “bank” without being at all close to each other. Aspect maps resemble clustering because they model pair-wise similarities as a mixture of different types of similarity  but they also resemble local multi-dimensional scaling because they model each type of similarity by a twodimensional map. We demonstrate our method on a toy example  a database of human wordassociation data  a large set of images of handwritten digits  and a set of feature vectors that represent words.       
2007,Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure.,We show how to pretrain and fine-tune a multilayer neural network to learn a nonlinear transformation from the input space to a lowdimensional feature space in which K-nearest neighbour classification performs well. We also show how the non-linear transformation can be improved using unlabeled data. Our method achieves a much lower error rate than Support Vector Machines or standard backpropagation on a widely used version of the MNIST handwritten digit recognition task. If some of the dimensions of the low-dimensional feature space are not used for nearest neighbor classification  our method uses these dimensions to explicitly represent transformations of the digits that do not affect their identity.              
2007,Learning Multilevel Distributed Representations for High-Dimensional Sequences.,We describe a new family of non-linear sequence models that are substantially more powerful than hidden Markov models or linear dynamical systems. Our models have simple approximate inference and learning procedures that work well in practice. Multilevel representations of sequential data can be learned one hidden layer at a time  and adding extra hidden layers improves the resulting generative models. The models can be trained with very high-dimensional  very non-linear data such as raw pixel sequences. Their performance is demonstrated using synthetic video sequences of two balls bouncing in a box.             
2006,Unsupervised Discovery of Nonlinear Structure Using Contrastive Backpropagation.,We describe a way of modeling high‐dimensional data vectors by using an unsupervised  nonlinear  multilayer neural network in which the activity of each neuron‐like unit makes an additive contribution to a global energy score that indicates how surprised the network is by the data vector. The connection weights that determine how the activity of each unit depends on the activities in earlier layers are learned by minimizing the energy assigned to data vectors that are actually observed and maximizing the energy assigned to “confabulations” that are generated by perturbing an observed data vector in a direction that decreases its energy under the current model.             
2006,Topographic Product Models Applied to Natural Scene Statistics.,We present an energy-based model that uses a product of generalized Student-t distributions to capture the statistical structure in data sets. This model is inspired by and particularly applicable to “natural” data sets such as images. We begin by providing the mathematical framework  where we discuss complete and overcomplete models and provide algorithms for training these models from data. Using patches of natural scenes  we demonstrate that our approach represents a viable alternative to independent component analysis as an interpretive model of biological visual systems. Although the two approaches are similar in flavor  there are also important differences  particularly when the representations are overcomplete. By constraining the interactions within our model  we are also able to study the topographic organization of Gabor-like receptive fields that our model learns. Finally  we discuss the relation of our new approach to previous work—in particular  gaussian scale mixture models and variants of independent components analysis.        
2006,A Fast Learning Algorithm for Deep Belief Nets.,We show how to use “complementary priors” to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors  we derive a fast  greedy algorithm that can learn deep  directed belief networks one layer at a time  provided the top two layers form an undirected associative memory. The fast  greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning  a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory  and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.        
2006,Modeling Human Motion Using Binary Latent Variables.,"We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued ""visible"" variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training  the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture.               "
2005,Improving dimensionality reduction with spectral gradient descent.,We introduce spectral gradient descent  a way of improving iterative dimensionality reduction techniques.1 The method uses information contained in the leading eigenvalues of a data affinity matrix to modify the steps taken during a gradient-based optimization procedure. We show that the approach is able to speed up the optimization and to help dimensionality reduction methods find better local minima of their objective functions. We also provide an interpretation of our approach in terms of the power method for finding the leading eigenvalues of a symmetric matrix and verify the usefulness of the approach in some simple experiments.              
2005,On Contrastive Divergence Learning.,"Maximum-likelihood (ML) learning of Markov random fields is challenging because it requires estimates of averages that have an exponential number of terms. Markov chain Monte Carlo methods typically take a long time to converge on unbiased estimates  but Hinton showed that if the Markov chain is only run for a few steps  the learning can still work well and it approximately minimizes a different function called ""contrastive divergence"" (CD). CD learning has been successfully applied to various types of random fields. Here  we study the properties of CD learning and show that it provides biased estimates in general  but that the bias is typically very small. Fast CD learning can therefore be used to get close to an ML solution and slow ML learning can then be used to fine-tune the CD solution.            "
2005,Learning Causally Linked Markov Random Fields.,We describe a learning procedure for a generative model that contains a hidden Markov Random Field (MRF) which has directed connections to the observable variables. The learning procedure uses a variational approximation for the posterior distribution over the hidden variables. Despite the intractable partition function of the MRF  the weights on the directed connections and the variational approximation itself can be learned by maximizing a lower bound on the log probability of the observed data. The parameters of the MRF are learned by using the mean field version of contrastive divergence. We show that this hybrid model simultaneously learns parts of objects and their inter-relationships from intensity images. We discuss the extension to multiple MRFs linked into a in a chain graph by directed connections.               
2005,What kind of graphical model is the brain?,If neurons are treated as latent variables  our visual systems are non-linear  densely-connected graphical models containing billions of variables and thousands of billions of parameters. Current algorithms would have difficulty learning a graphical model of this scale. Starting with an algorithm that has difficulty learning more than a few thousand parameters  I describe a series of progressively better learning algorithms all of which are designed to run on neuron-like hardware. The latest member of this series can learn deep  multi-layer belief nets quite rapidly. It turns a generic network with three hidden layers and 1.7 million connections into a very good generative model of handwritten digits. After learning  the model gives classification performance that is comparable to the best discriminative methods.           
2005,Inferring Motor Programs from Images of Handwritten Digits.,We describe a generative model for handwritten digits that uses two pairs of opposing springs whose stiffnesses are controlled by a motor program. We show how neural networks can be trained to infer the motor programs required to accurately construct the MNIST digits. The inferred motor programs can be used directly for digit classification  but they can also be used in other ways. By adding noise to the motor program inferred from an MNIST image we can generate a large set of very different images of the same class  thus enlarging the training set available to other methods. We can also use the motor programs as additional  highly informative outputs which reduce overfitting when training a feed-forward classifier.            
2004,Reinforcement Learning with Factored States and Actions.,A novel approximation method is presented for approximating the value function and selecting good actions for Markov decision processes with large state and action spaces. The method approximates state-action values as negative free energies in an undirected graphical model called a product of experts. The model parameters can be learned efficiently because values and derivatives can be efficiently computed for a product of experts. Actions can be found even in large factored action spaces by the use of Markov chain Monte Carlo sampling. Simulation results show that the product of experts approximation can be used to solve large problems. in one simulation it is used to find actions in action spaces of size 2^40.               
2004,Probabilistic sequential independent components analysis.,Under-complete models  which derive lower dimensional representations of input data  are valuable in domains in which the number of input dimensions is very large  such as data consisting of a temporal sequence of images. This paper presents the under-complete product of experts (UPoE)  where each expert models a one-dimensional projection of the data. Maximum-likelihood learning rules for this model constitute a tractable and exact algorithm for learning under-complete independent components. The learning rules for this model coincide with approximate learning rules proposed earlier for under-complete independent component analysis (UICA) models. This paper also derives an efficient sequential learning algorithm from this model and discusses its relationship to sequential independent component analysis (ICA)  projection pursuit density estimation  and feature induction algorithms for additive random field models. This paper demonstrates the efficacy of these novel algorithms on high-dimensional continuous datasets.         
2004,Neighbourhood Components Analysis.,In this paper we propose a novel method for learning a Mahalanobis distance measure to be used in the KNN classification algorithm. The algorithm directly maximizes a stochastic variant of the leave-one-out KNN score on the training set. It can also learn a low-dimensional linear embedding of labeled data that can be used for data visualization and fast classification. Unlike other methods  our classification model is non-parametric  making no assumptions about the shape of the class distributions or the boundaries between them. The performance of the method is demonstrated on several data sets  both for metric learning and linear dimensionality reduction.            
2004,Multiple Relational Embedding.,We describe a way of using multiple different types of similarity relationship to learn a low-dimensional embedding of a dataset. Our method chooses different  possibly overlapping representations of similarity by individually reweighting the dimensions of a common underlying latent space. When applied to a single similarity relation that is based on Euclidean distances between the input data points  the method reduces to simple dimensionality reduction. If additional information is available about the dataset or about subsets of it  we can use this information to clean up or otherwise improve the embedding. We demonstrate the potential usefulness of this form of semi-supervised dimensionality reduction on some simple examples.            
2004,Exponential Family Harmoniums with an Application to Information Retrieval.,"Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields. Although this approach has met with considerable success  the causal semantics of these models can make it difficult to infer the posterior distribution ove the hidden variables. In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undirected models. Inference in these ""exponential family harmoniums"" is fast while learning is performed by minimizing contrastive divergence. A member of this family is then studied as an alternative probabilistic model for latent semantic indexing. In experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords.              "
2003,Energy-Based Models for Sparse Overcomplete Representations.,We present a new way of extending independent components analysis (ICA) to overcomplete representations. In contrast to the causal generative extensions of ICA which maintain marginal independence of sources  we define features as deterministic (linear) functions of the inputs. This assumption results in marginal dependencies among the features  but conditional independence of the features given the inputs. By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution. Free parameters of this model are trained using the contrastive divergence objective (Hinton  2002). When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data. In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech  natural images  hand-written digits and faces.          
2003,Wormholes Improve Contrastive Divergence.,In models that define probabilities via energies  maximum likelihood learning typically involves using Markov Chain Monte Carlo to sample from the model's distribution. If the Markov chain is started at the data distribution  learning often works well even if the chain is only run for a few time steps. But if the data distribution contains modes separated by regions of very low density  brief MCMC will not ensure that different modes have the correct relative energies because it cannot move particles from one mode to another. We show how to improve brief MCMC by allowing long-range moves that are suggested by the data distribution. If the model is approximately correct  these long-range moves have a reasonable acceptance rate.           
2003,Efficient Parametric Projection Pursuit Density Estimation.,Product models of low dimensional experts are a powerful way to avoid the curse of dimensionality. We present the ``under-complete product of experts' (UPoE)  where each expert models a one dimensional projection of the data. The UPoE is fully tractable and may be interpreted as a parametric probabilistic model for projection pursuit. Its ML learning rules are identical to the approximate learning rules proposed before for under-complete ICA. We also derive an efficient sequential learning algorithm and discuss its relationship to projection pursuit density estimation and feature induction algorithms for additive random field models              
2002,Local Physical Models for Interactive Character Animation.,Our goal is to design and build a tool for the creation of expressive character animation. Virtual puppetry  also known as performance animation  is a technique in which the user interactively controls a character's motion. In this paper we introduce local physical models for performance animation and describe how they can augment an existing kinematic method to achieve very effective animation control. These models approximate specific physically‐generated aspects of a character's motion. They automate certain behaviours  while still letting the user override such motion via a PD‐controller if he so desires. Furthermore  they can be tuned to ignore certain undesirable effects  such as the risk of having a character fall over  by ignoring corresponding components of the force. Although local physical models are a quite simple approximation to real physical behaviour  we show that they are extremely useful for interactive character control  and contribute positively to the expressiveness of the character's motion. In this paper  we develop such models at the knees and ankles of an interactively‐animated 3D anthropomorphic character  and demonstrate a resulting animation. This approach can be applied in a straight‐forward way to other joints.     
2002,Training Products of Experts by Minimizing Contrastive Divergence.,It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual “expert” models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert  because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately  a PoE can be trained using a different objective function called “contrastive divergence” whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.             
2002,Recognizing Handwritten Digits Using Hierarchical Products of Experts.,The product of experts learning procedure can discover a set of stochastic binary features that constitute a nonlinear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance  a hierarchy of separate models can be learned  for each digit class. Each model in the hierarchy learns a layer of binary feature detectors that model the probability distribution of vectors of activity of feature detectors in the layer below. The models in the hierarchy are trained sequentially and each model uses a layer of binary feature detectors to learn a generative model of the patterns of feature activities in the preceding layer. After training  each layer of feature detectors produces a separate  unnormalized log probability score. With three layers of feature detectors for each of the 10 digit classes  a test image produces 30 scores which can be used as inputs to a supervised  logistic classification network that is trained on separate data.         
2002,A Desktop Input Device and Interface for Interactive 3D Character Animation.,We present a novel input device and interface for interactively controlling the animation of graphical human character from a desktop environment. The trackers are embedded in a new physical design  which is both simple yet also provides significant benefits  and establishes a tangible interface with coordinate frames inherent to the character. A layered kinematic motion recording strategy accesses subsets of the total degrees of freedom of the character. We present the experiences of three novice users with the system  and that of a long-term user who has prior experience with other complex continuous interfaces.            
2002,A New Learning Algorithm for Mean Field Boltzmann Machines.,We present a new learning algorithm for Mean Field Boltzmann Machines based on the contrastive divergence optimization criterion. In addition to minimizing the divergence between the data distribution and the equilibrium distribution  we maximize the divergence between one-step reconstructions of the data and the equilibrium distribution. This eliminates the need to estimate equilibrium statistics  so we do not need to approximate the multimodal probability distribution of the free network with the unimodal mean field distribution. We test the learning algorithm on the classification of digits.             
2002,Self Supervised Boosting.,"Boosting algorithms and successful applications thereof abound for classification and regression learning problems  but not for unsupervised learning. We propose a sequential approach to adding features to a random field model by training them to improve classification performance between the data and an equal-sized sample of ""negative examples"" generated from the model's current estimate of the data density. Training in each boosting round proceeds in three stages: first we sample negative examples from the model's current Boltzmann distribution. Next  a feature is trained to improve classification performance between data and negative examples. Finally  a coefficient is learned which determines the importance of this feature relative to ones already in the pool. Negative examples only need to be generated once to learn each new features. The validity of the approach is demonstrated on binary digits and continuous synthetic data.            "
2002,Stochastic Neighbor Embedding.,"We describe a probabilistic approach to the task of placing objects  described by high-dimensional vectors or by pairwise dissimilarities  in a low-dimensional space in a way that preserves neighbour identities. . A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional “images” of the objects. . A natural cost function is a sum of Kullback-Leibler divergences  one per object  which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods  this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects  like the document count vector for the word ""bank""  to have versions close to the images of both ""river"" and ""finance"" without forcing the images of outdoor concepts to be located close to those of corporate concepts.        "
2002,Learning Sparse Topographic Representations with Products of Student-t Distributions.,"We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some filter outputs.  We encourage the system to find sparse features by using a Student-t distribution to model each filter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent filters  the system learns a topographic map in which the orientation  spatial frequency and location of the filters change smoothly across the map. Even though maximum likelihood learning is intractable in our model  the product form allows a relatively efficient learning procedure that works well even for highly overcomplete sets of filters. Once the model has been learned it can be used as a prior to derive the ""iterated Wiener filter"" for the purpose of denoising images.            "
2001,Learning Distributed Representations of Concepts Using Linear Relational Embedding.,We introduce linear relational embedding as a means of learning a distributed representation of concepts from data consisting of binary relations between these concepts. The key idea is to represent concepts as vectors  binary relations as matrices  and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient ascent. On a task involving family relationships  learning is fast and leads to good generalization.            
2001,Products of Hidden Markov Models.,We present products of hidden Markov models (PoHMM's)  a way of combining HMM's to form a distributed state time series model.  Inference in a PoHMM is tractable and efficient.  Learning of the parameters  although intractable  can be effectively done using the Product of Experts Learning rule.  The distributed state helps the model to explain data which has multiple causes  and the fact that each model need only explain part of the data means a PoHMM can capture longer range structure than an HMM is capable of.  We show some results on modelling character strings  a simple language task and the symbolic family trees problem  which highlight these advantages.         
2001,Learning Hierarchical Structures with Linear Relational Embedding.,We present Linear Relational Embedding (LRE)  a new method of learning a distributed representation of concepts from data consisting of instances of relations between given concepts. Its final goal is to be able to generalize  i.e. infer new instances of these relations among the concepts. On a task involving family relationships we show that LRE can generalize better than any previously published method. We then show how LRE can be used effectively to find compact distributed representations for variable-sized recursive data structures  such as trees and lists.            
2001,Global Coordination of Local Linear Models.,"High dimensional data that lies on or near a low dimensional manifold can be described by a collection of local linear models. Such a description  however  does not provide a global parameterization of the manifold—arguably an important goal of unsupervised learning. In this paper  we show how to learn a collection of local linear models that solves this more difficult problem. Our local linear models are represented by a mixture of factor analyzers  and the ""global coordination"" of these models is achieved by adding a regularizing term to the standard maximum likelihood objective function. The regularizer breaks a degeneracy in the mixture model's parameter space  favoring models whose internal coordinate systems are aligned in a consistent way. As a result  the internal coordinates change smoothly and continuously as one traverses a connected path on the manifold—even when the path crosses the domains of many different local models. The regularizer takes the form of a Kullback-Leibler divergence and illustrates an unexpected application of variational methods: not to perform approximate inference in intractable probabilistic models  but to learn more useful internal representations in tractable ones        "
2001,Relative Density Nets: A New Way to Combine Backpropagation with HMM's.,Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the Standard method of discriminatively training HMM's.               
2001,Discovering Multiple Constraints that are Frequently Approximately Satisfied.,Some high-dimensional data.sets can be modelled by assuming that there are many different linear constraints  each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations.              
2000,Variational Learning for Switching State-Space Models.,We introduce a new statistical model for time series that iteratively segments data into regimes with approximately linear dynamics and learns the parameters of each of these linear regimes. This model combines and generalizes two of the most widely used stochastic time-series models—hidden Markov models and linear dynamical systems—and is closely related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network (Jacobs  Jordan  Nowlan  & Hinton  1991) to its fully dynamical version  in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable  and therefore the exact expectation maximization (EM) algorithm cannot be applied. However  we present a variational approximation that maximizes a lower bound on the log-likelihood and makes use of both the forward and backward recursions for hidden Markov models and the Kalman filter recursions for linear dynamical systems. We tested the algorithm on artificial data sets and a natural data set of respiration force from a patient with sleep apnea. The results suggest that variational approximations are a viable method for inference and learning in switching state-space models.        
2000,SMEM Algorithm for Mixture Models.,We present a split-and-merge expectation-maximization (SMEM) algorithm to overcome the local maxima problem in parameter estimation of finite mixture models. In the case of mixture models  local maxima often involve having too many components of a mixture model in one part of the space and too few in another  widely separated part of the space. To escape from such configurations  we repeatedly perform simultaneous split-and-merge operations using a new criterion for efficiently selecting the split-and-merge candidates. We apply the proposed algorithm to the training of gaussian mixtures and mixtures of factor analyzers using synthetic and real data and show the effectiveness of using the split- and-merge operations to improve the likelihood of both the training data and of held-out test data. We also show the practical usefulness of the proposed algorithm by applying it to image compression and pattern recognition problems.            
2000,Split and Merge EM Algorithm for Improving Gaussian Mixture Density Estimates.,The EM algorithm for Gaussian mixture models often gets caught in local maxima of the likelihood which involve having too many Gaussians in one part of the space and too few in another  widely separated part of the space. We present a new EM algorithm which performs split and merge operations on the Gaussians to escape from these configurations. This algorithm uses two novel criteria for efficiently selecting the split and merge candidates. Experimental results on synthetic and real data show the effectiveness of using the split and merge operations to improve the likelihood of both the training data and of held-out test data.              
2000,Modeling High-Dimensional Data by Combining Simple Experts.,"It is possible to combine multiple non-linear probabilistic models of the same data by multiplying the probability distributions together and then renormalizing. A ""product of experts"" is a very efficient way to model data that simultaneously satisfies many different constraints. It is difficult to fit a product of experts to data using maximum likelihood because the gradient of the log likelihood is intractable  but there is an efficient way of optimizing a different objective function and this produces good models of high-dimensional data.              "
2000,Learning Distributed Representations by Mapping Concepts and Relations into a Linear Space.,Linear Relational Embedding is a method of learning a distributed representation of concepts from data consisting of binary relations between concepts. Concepts are represented as vectors  binary relations as matrices  and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient ascent. On a task involving family relationships  learning is fast and leads to good generalization            
2000,Extracting Distributed Representations of Concepts and Relations from Positive and Negative Propositions.,Linear relational embedding (LRE) was introduced previously by the authors (1999) as a means of extracting a distributed representation of concepts from relational data. The original formulation cannot use negative information and cannot properly handle data in which there are multiple correct answers. In this paper we propose an extended formulation of LRE that solves both these problems. We present results in two simple domains  which show that learning leads to good generalization.              
2000,Rate-coded Restricted Boltzmann Machines for Face Recognition.,We describe a neurally-inspired  unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded  non-linear feature detectors and it has the property that  given a data vector  the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations.           
2000,Recognizing Hand-written Digits Using Hierarchical Products of Experts.,The product of experts learning procedure [1] can discover a set of stochastic binary features that constitute a non-linear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance  it is helpful to learn a hierarchy of separate models for each digit class. Each model in the hierarchy has one layer of hidden units and the nth level model is trained on data that consists of the activities of the hidden units in the already trained (n - 1)th level model. After training  each level produces a separate  unnormalized log probabilty score. With a three-level hierarchy for each of the 10 digit classes  a test image produces 30 scores which can be used as inputs to a supervised  logistic classification network that is trained on separate data. On the MNIST database  our system is comparable with current state-of-the-art discriminative methods  demonstrating that the product of experts learning procedure can produce effective generative models of high-dimensional data.        
2000,Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task.,The problem of reinforcement learning in large factored Markov decision processes is explored. The Q-value of a state-action pair is approximated by the free energy of a product of experts network. Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs. Actions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. The algorithm is tested on a co-operative multi-agent task. The product of experts model is found to perform comparably to table-based Q-learning for small instances of the task  and continues to perform well when the problem becomes too large for a table-based representation.              
1999,Variational Learning in Nonlinear Gaussian Belief Networks.,We view perceptual tasks such as vision and speech recognition as inference problems where the goal is to estimate the posterior distribution over latent variables (e.g.  depth in stereo vision) given the sensory input. The recent flurry of research in independent component analysis exemplifies the importance of inferring the continuous-valued latent variables of input data. The latent variables found by this method are linearly related to the input  but perception requires nonlinear inferences such as classification and depth estimation. In this article  we present a unifying framework for stochastic neural networks with nonlinear latent variables. Nonlinear units are obtained by passing the outputs of linear gaussian units through various nonlinearities. We present a general variational method that maximizes a lower bound on the likelihood of a training set and give results on two visual feature extraction problems. We also show how the variational method can be used for pattern classification and compare the performance of these nonlinear networks with other methods on the problem of handwritten digit recognition.            
1999,Spiking Boltzmann Machines.,We first show how to represent sharp posterior probability distributions using real valued coefficients on broadly-tuned basis functions. Then we show how the precise times of spikes can be used to convey the real-valued coefficients on the basis functions quickly and accurately. Finally we describe a simple simulation in which spiking neurons learn to model an image sequence by fitting a dynamic generative model.                
1999,Learning to Parse Images.,We describe a class of probabilistic models that we call credibility networks. Using parse trees as internal representations of images  credibility networks are able to perform segmentation and recognition simultaneously  removing the need for ad hoc segmentation heuristics. Promising results in the problem of segmenting handwritten digits were obtained.             
1998,Coaching variables for regression and classification.,In a regression or classification setting where we wish to predict Y from x1 x2 ...  xp  we suppose that an additional set of ‘coaching’ variables z1 z2 ...  zm are available in our training sample. These might be variables that are difficult to measure  and they will not be available when we predict Y from x1 x2 ...  xp in the future. We consider two methods of making use of the coaching variables in order to improve the prediction of Y from x1 x2 ...  xp. The relative merits of these approaches are discussed and compared in a number of examples. 
1998,Glove-TalkII-a neural-network interface which maps gestures to parallel formant speech synthesizer controls.,Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to ten control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently  the best version of Glove-TalkII uses several input devices (including a Cyberglove  a ContactGlove  a three-space tracker  and a foot pedal)  a parallel formant speech synthesizer  and three neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed user-defined relationship between hand position and vowel sound and does not require any training examples from the user. Volume  fundamental frequency  and stop consonants are produced with a fixed mapping from the input devices. One subject has trained to speak intelligibly with Glove-TalkII. He speaks slowly but with far more natural sounding pitch variations than a text-to-speech synthesizer.       
1998,SMEM Algorithm for Mixture Models.,We present a split-and-merge expectation-maximization (SMEM) algorithm to overcome the local maxima problem in parameter estimation of finite mixture models. In the case of mixture models  local maxima often involve having too many components of a mixture model in one part of the space and too few in another  widely separated part of the space. To escape from such configurations  we repeatedly perform simultaneous split-and-merge operations using a new criterion for efficiently selecting the split-and-merge candidates. We apply the proposed algorithm to the training of gaussian mixtures and mixtures of factor analyzers using synthetic and real data and show the effectiveness of using the split- and-merge operations to improve the likelihood of both the training data and of held-out test data. We also show the practical usefulness of the proposed algorithm by applying it to image compression and pattern recognition problems.            
1998,Fast Neural Network Emulation of Dynamical Systems for Computer Animation.,"Computer animation through the numerical simulation of physics-based graphics models offers unsurpassed realism  but it can be computationally demanding. This paper demonstrates the possibility of replacing the numerical simulation of nontrivial dynamic models with a dramatically more efficient ""NeuroAnimator"" that exploits neural networks . NeuroAnimators are automatically trained off-line to emulate physical dynamics through the observation of physics-based models in action. Depending on the model  its neural network emulator can yield physically realistic animation one or two orders of magnitude faster than conventional numerical simulation. We demonstrate NeuroAnimators for a variety of physics-based models.             "
1998,NeuroAnimator: Fast Neural Network Emulation and Control of Physics-based Models.,Animation through the numerical simulation of physicsbased graphics models offers unsurpassed realism  but it can be computationally demanding. Likewise  the search for controllers that enable physics-based models to produce desired animations usually entails formidable computational cost. This paper demonstrates the possibility of replacing the numerical simulation and control of dynamic models with a dramatically more efficient alternative. In particular  we propose the NeuroAnimator  a novel approach to creating physically realistic animation that exploits neural networks. NeuroAnimators are automatically trained off-line to emulate physical dynamics through the observation of physicsbased models in action. Depending on the model  its neural network emulator can yield physically realistic animation one or two orders of magnitude faster than conventional numerical simulation. Furthermore  by exploiting the network structure of the NeuroAnimator  we introduce a fast algorithm for learning controllers that enables either physics-based models or their neural network emulators to synthesize motions satisfying prescribed animation goals. We demonstrate NeuroAnimators for a variety of physics-based models.        
1998,"A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants.",The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective  it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described  and a wide range of other variant algorithms are also seen to be possible.             
1998,A Hierarchical Community of Experts.,We describe a directed acyclic graphical model that contains a hierarchy of linear units and a mechanism for dynamically selecting an appropriate subset of these units to model each observation. The non-linear selection mechanism is a hierarchy of binary units each of which gates the output of one of the linear units. There are no connections from linear units to binary units  so the generative model can be viewed as a logistic belief net (Neal 1992) which selects a skeleton linear model from among the available linear units. We show that Gibbs sampling can be used to learn the parameters of the linear and binary units even when the sampling is so brief that the Markov chain is far from equilibrium.              
1997,Efficient Stochastic Source Coding and an Application to a Bayesian Network Source Model.,"In this paper  we introduce a new algorithm calledbits-back coding' that makes stochastic source codes efficient. For a given one-to-many source code  we show that this algorithm can actually be more ef""cient than the algorithm that always picks the shortest codeword. Optimal ef""ciency is achieved when codewords are chosen according to the Boltzmann distribution based on the codeword lengths. It turns out that a commonly used technique for determining parameters— maximum-likelihood estimation—actually minimizes the bits-back coding cost when codewords are chosen according to the Boltzmann distribution. A tractable approximation to maximum-likelihood estimation—the generalized expectation-maximization algorithm—minimizes the bits-back coding cost. After presenting a binary Bayesian network model that assigns exponentially many codewords to each symbol  we show how a tractable approximation to the Boltzmann distribution can be used for bits-back coding. We illustrate the performance of bits-back coding using non-synthetic data with a binary Bayesian network source model that produces 2 60 possible codewords for each input symbol. The rate for bits-back coding is nearly one half of that obtained by picking the shortest            "
1997,Instantiating Deformable Models with a Neural Net.,Deformable models are an attractive approach to recognizing objects which have considerable within-class variability such as handwritten characters. However  there are severe search problems associated with fitting the models to data which could be reduced if a better starting point for the search were available. We show that by training a neural network to predict how a deformable model should be instantiated from an input image  such improved starting points can be obtained. This method has been implemented for a system that recognizes handwritten digits using deformable models  and the results show that the search time can be significantly reduced without compromising recognition performance.            
1997,Using Expectation-Maximization for Reinforcement Learning.,We discuss Hinton's (1989) relative payoff procedure (RPP)  a static reinforcement learning algorithm whose foundation is not stochastic gradient ascent. We show circumstances under which applying the RPP is guaranteed to increase the mean return  even though it can make large changes in the values of the parameters. The proof is based on a mapping between the RPP and a form of the expectation-maximization procedure of Dempster  Laird  and Rubin (1977).           
1997,A Mobile Robot that Learns its Place.,We show how a neural network can be used to allow a mobile robot to derive an accurate estimate of its location from noisy sonar sensors and noisy motion information. The robot's model of its location is in the form of a probability distribution across a grid of possible locations. This distribution is updated using both the motion information and the predictions of a neural network that maps locations into likelihood distributions across possible sonar readings. By predicting sonar readings from locations  rather than vice versa  the robot can handle the very nongaussian noise in the sonar sensors. By using the constraint provided by the noisy motion information  the robot can use previous readings to improve its estimate of its current location. By treating the resulting estimates as if they were correct  the robot can learn the relationship between location and sonar readings without requiring an external supervision signal that specifies the actual location of the robot. It can learn to locate itself in a new environment with almost no supervision  and it can maintain its location ability even when the environment is nonstationary.          
1997,Modeling the manifolds of images of handwritten digits.,This paper describes two new methods for modeling the manifolds of digitized images of handwritten digits. The models allow a priori information about the structure of the manifolds to be combined with empirical data. Accurate modeling of the manifolds allows digits to be discriminated using the relative probability densities under the alternative models. One of the methods is grounded in principal components analysis  the other in factor analysis. Both methods are based on locally linear low-dimensional approximations to the underlying data manifold. Links with other methods that model the manifold are discussed.              
1997,Glove-talk II - a neural-network interface which maps gestures to parallel formant speech synthesizer controls.,Glove-Talk II is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to ten control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently  the best version of Glove-Talk II uses several input devices  a parallel formant speech synthesizer  and three neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed user-defined relationship between hand position and vowel sound and does not require any training examples from the user. Volume  fundamental frequency  and stop consonants are produced with a fixed mapping from the input devices. With Glove-Talk II  the subject can speak slowly but with far more natural sounding pitch variations than a text-to-speech synthesizer.         
1997,Hierarchical Non-linear Factor Analysis and Topographic Maps.,We first describe a hierarchical  generative model that can be viewed as a non-linear generalisation of factor analysis and can be implemented in a neural network. The model performs perceptual inference in a probabilistically consistent manner by using top-down  bottom-up and lateral connections. These connections can be learned using simple rules that require only locally available information. We then show how to incorporate lateral connections into the generative model. The model extracts a sparse  distributed  hierarchical representation of depth from simplified random-dot stereograms and the localised disparity detectors in the first hidden layer form a topographic map. When presented with image patches from natural scenes  the model develops topographically organised local feature detectors.          
1997,Learning fast neural network emulators for physics-based models.,N/A
1996,Varieties of Helmholtz Machine.,The Helmholtz machine is a new unsupervised learning architecture that uses top-down connections to build probability density models of input and bottom-up connections to build inverses to those models. The wake-sleep learning algorithm for the machine involves just the purely local delta rule. This paper suggests a number of different varieties of Helmholtz machines  each with its own strengths and weaknesses  and relates them to cortical information processing.              
1996,Using Generative Models for Handwritten Digit Recognition.,"We describe a method of recognizing handwritten digits by fitting generative models that are built from deformable B-splines with Gaussian ""ink generators"" spaced along the length of the spline. The splines are adjusted using a novel elastic matching procedure based on the expectation maximization algorithm that maximizes the likelihood of the model generating the data. This approach has many advantages: 1) the system not only produces a classification of the digit but also a rich description of the instantiation parameters which can yield information such as the writing style; 2) the generative models can perform recognition driven segmentation; 3) the method involves a relatively small number of parameters and hence training is relatively easy and fast; and 4) unlike many other recognition schemes  it does not rely on some form of pre-normalization of input images  but can handle arbitrary scalings  translations and a limited degree of image rotation. We have demonstrated that our method of fitting models to images does not get trapped in poor local minima. The main disadvantage of the method is that it requires much more computation than more standard OCR techniques.            "
1996,Free Energy Coding.,We introduce a new approach to the problem of optimal compression when a source code produces multiple codewords for a given symbol. It may seem that the most sensible codeword to use in this case is the shortest one. However  in the proposed free energy approach  random codeword selection yields an effective codeword length that can be less than the shortest codeword length. If the random choices are Boltzmann distributed  the effective length is optimal for the given source code. The expectation-maximization parameter estimation algorithms minimize this effective codeword length. We illustrate the performance of free energy coding on a simple problem where a compression factor of two is gained by using the new method.            
1995,Learning Population Codes by Minimizing Description Length.,The minimum description length (MDL) principle can be used to train the hidden units of a neural network to extract a representation that is cheap to describe but nonetheless allows the input to be reconstructed accurately. We show how MDL can be used to develop highly redundant population codes. Each hidden unit has a location in a low-dimensional implicit space. If the hidden unit activities form a bump of a standard shape in this space  they can be cheaply encoded by the center of this bump. So the weights from the input units to the hidden units in an autoencoder are trained to make the activities form a standard bump. The coordinates of the hidden units in the implicit space are also learned  thus allowing flexibility  as the network develops a discontinuous topography when presented with different input classes.            
1995,The Helmholtz machine.,Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model  independent draws from which are likely to produce the patterns. For all but the simplest generative models  each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.             
1995,GloveTalkII: An Adaptive Gesture-to-Formant Interface.,Glove-TaikII is a system which translates hand gestures-· to speech through an adaptive interface. Hand gestures are mapped continuously to 10 control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary  multiple languages in addition to direct control of fundamental frequency and volume. Currently  the best version of Glove-TaikII uses several input devices (including a Cyberglove  a ContactGlove  a polhemus sensor  and a foot-pedal)  a parallel formant speech synthesizer and 3 neural networks. The gestureto-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed  user-defined relationship between hand-position and vowel sound and does not require any training examples from the user. Volume  fundamental frequency and stop consonants are produced with a fixed mapping from the input devices. One subject has trained for about 100 hours to speak intelligibly with Glove-TalkII. He passed through eight distinct stages while learning to speak. He speaks slowly with speech quality similar to a text-to-speech synthesizer but with far more natural-sounding pitch variations.       
1995,Using Pairs of Data-Points to Define Splits for Decision Trees.,Conventional binary classification trees such as CART either split the data using axis-aligned hyperplanes or they perform a computationally expensive search in the continuous space of hyperplanes with unrestricted orientations. We show that the limitations of the former can be overcome without resorting to the latter. For every pair of training data-points  there is one hyperplane that is orthogonal to the line joining the data-points and bisects this line. Such hyperplanes are plausible candidates for splits. In a comparison on a suite of 12 datasets we found that this method of generating candidate splits outperformed the standard methods  particularly when the training sets were small.             
1995,Does the Wake-sleep Algorithm Produce Good Density Estimators?,The wake-sleep algorithm (Hinton  Dayan  Frey and Neal 1995) is a relatively efficient method of fitting a multilayer stochastic generative model to high-dimensional data. In addition to the top-down connections in the generative model  it makes use of bottom-up connections for approximating the probability distribution over the hidden units given the data  and it trains these bottom-up connections using a simple delta rule. We use a variety of synthetic and real data sets to compare the performance of the wake-sleep algorithm with Monte Carlo and mean field methods for fitting the same generative model and also compare it with other models that are less powerful but easier to fit.           
1994,An Alternative Model for Mixtures of Experts.,We propose an alternative model for mixtures of experts which uses a different parametric form for the gating network. The modified model is trained by the EM algorithm. In comparison with earlier models--trained by either EM or gradient ascent--there is no need to select a learning stepsize. We report simulation experiments which show that the new architecture yields faster convergence. We also apply the new model to two problem domains: piecewise nonlinear function approximation and the combination of multiple previously trained classifiers.               
1994,Glove-TalkII: Mapping Hand Gestures to Speech Using Neural Networks.,Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to 10 control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently  the best version of Glove-TalkII uses several input devices (including a CyberGlove  a ContactGlove  a 3- space tracker  and a foot-pedal)  a parallel formant speech synthesizer and 3 neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed  user-defined relationship between hand-position and vowel sound and does not require any training examples from the user. Volume  fundamental frequency and stop consonants are produced with a fixed mapping from the input devices. One subject has trained to speak intelligibly with Glove-TalkII. He speaks slowly with speech quality similar to a text-to-speech synthesizer but with far more natural-sounding pitch variations.        
1994,Using a neural net to instantiate a deformable model.,Deformable models are an attractive approach to recognizing nonrigid objects which have considerable within class variability. However  there are severe search problems associated with fitting the models to data. We show that by using neural networks to provide better starting points  the search time can be significantly reduced. The method is demonstrated on a character recognition task.              
1994,Recognizing Handwritten Digits Using Mixtures of Linear Models.,We construct a mixture of locally linear generative models of a collection of pixel-based images of digits  and use them for recognition. Different models of a given digit are used to capture different styles of writing  and new images are classified by evaluating their log-likelihoods under each model. We use an EM-based algorithm in which the M-step is computationally straightforward principal components analysis (PCA). Incorporating tangent-plane information [12] about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the PCA  and it demonstrably improves performance.             
1993,Learning Mixture Models of Spatial Coherence.,We have previously described an unsupervised learning procedure that discovers spatially coherent properties of the world by maximizing the information that parameters extracted from different parts of the sensory input convey about some common underlying cause. When given random dot stereograms of curved surfaces  this procedure learns to extract surface depth because that is the property that is coherent across space. It also learns how to interpolate the depth at one location from the depths at nearby locations (Becker and Hinton 1992b). In this paper  we propose two new models that handle surfaces with discontinuities. The first model attempts to detect cases of discontinuities and reject them. The second model develops a mixture of expert interpolators. It learns to detect the locations of discontinuities and to invoke specialized  asymmetric interpolators that do not cross the discontinuities.            
1993,A soft decision-directed LMS algorithm for blind equalization.,An adaptation algorithm for equalizers operating on very distorted channels is presented. The algorithm is based on the idea of adjusting the equalizer tap gains to maximize the likelihood that the equalizer outputs would be generated by a mixture of two Gaussians with known means. The decision-directed least-mean-square algorithm is shown to be an approximation to maximizing the likelihood that the equalizer outputs come from such an independently and identically distributed source. The algorithm is developed in the context of a binary pulse-amplitude-modulation channel  and simulations demonstrate that the algorithm converges in channels for which the decision-directed LMS algorithms does not converge.              
1993,Glove-Talk: a neural network interface between a data-glove and a speech synthesizer.,To illustrate the potential of multilayer neural networks for adaptive interfaces  a VPL Data-Glove connected to a DECtalk speech synthesizer via five neural networks was used to implement a hand-gesture to speech system. Using minor variations of the standard backpropagation learning procedure  the complex mapping of hand movements to speech is learned using data obtained from a single 'speaker' in a simple training phase. With a 203 gesture-to-word vocabulary  the wrong word is produced less than 1% of the time  and no word is produced about 5% of the time. Adaptive control of the speaking rate and word stress is also available. The training times and final performance speed are improved by using small  separate networks for each naturally defined subtask. The system demonstrates that neural networks can be used to develop the complex mappings required in a high bandwidth interface that adapts to the individual user          
1993,Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights.,Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning  it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-o between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear  the exact derivatives can be computed e ciently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of interesting schemes for encoding the weights.             
1993,"Autoencoders, Minimum Description Length and Helmholtz Free Energy.",An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution  where the generative weights define the energy of each possible code vector given the input vector. Unfortunately  if the code vectors use distributed representations  it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor  it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes.           
1993,Developing Population Codes by Minimizing Description Length.,The Minimum Description Length principle (MDL) can be used to train the hidden units of a neural network to extract a representation that is cheap to describe but nonetheless allows the input to be reconstructed accurately. We show how MDL can be used to develop highly redundant population codes. Each hidden unit has a location in a lowdimensional implicit space. If the hidden unit activities form a bump of a standard shape in this space  they can be cheaply encoded by the center of this bump. So the weights from the input units to the hidden units in a self-supervised network are trained to make the activities form a standard bump. The coordinates of the hidden units in the implicit space are also learned  thus allowing exibility  as the network develops a discontinuous topography when presented with di erent input classes. Population-coding in a space other than the input enables a network to extract nonlinear higher-order properties of the inputs. Most existing unsupervised learning algorithms can be understood using the Minimum Description Length (MDL) principle (Rissanen  1989). Given an ensemble of input vectors  the aim of the learning algorithm is to nd a method of coding each input vector that minimizes the total cost  in bits  of communicating the input vectors to a receiver. There are three terms in the total description length: The code-cost is the number of bits required to communicate the code that the algorithm assigns to each input vector. The model-cost is the number of bits required to specify how to reconstruct input vectors from codes        
1992,Simplifying Neural Networks by Soft Weight-Sharing.,One way of simplifying neural networks so they generalize better is to add an extra term to the error function that will penalize complexity. Simple versions of this approach include penalizing the sum of the squares of the weights or penalizing the number of nonzero weights. We propose a more complicated penalty term in which the distribution of weight values is modeled as a mixture of multiple gaussians. A set of weights is simple if the weights have high probability density under the mixture model. This can be achieved by clustering the weights into subsets with the weights in each cluster having very similar values. Since we do not know the appropriate means or variances of the clusters in advance  we allow the parameters of the mixture model to adapt at the same time as the network learns. Simulations on two different problems demonstrate that this complexity term is more effective than previous complexity terms.              
1992,Feudal Reinforcement Learning.,"One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their submanagers who  in turn  learn how to satisfy them. Submanagers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command. We illustrate the system using a simple maze task. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map."
1991,Adaptive Mixtures of Local Experts.,We present a new supervised learning procedure for systems composed of many separate networks  each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network  or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks  each of which can be solved by a very simple expert network.            
1991,Learning to Make Coherent Predictions in Domains with Discontinuities.,We have previously described an unsupervised learning procedure that discovers spatially coherent properties of the world by maximizing the information that parameters extracted from different parts of the sensory input convey about some common underlying cause. When given random dot stereograms of curved surfaces  this procedure learns to extract surface depth because that is the property that is coherent across space. It also learns how to interpolate the depth at one location from the depths at nearby locations (Becker and Hinton  1992). In this paper  we propose two new models which handle surfaces with discontinuities. The first model attempts to detect cases of discontinuities and reject them. The second model develops a mixture of expert interpolators. It learns to detect the locations of discontinuities and to invoke specialized  asymmetric interpolators that do not cross the discontinuities.           
1991,Adaptive Elastic Models for Hand-Printed Character Recognition.,"Hand-printed digits can be modeled as splines that are governed by about 8 control points. For each known digit  the control points have preferred ""home"" locations  and deformations of the digit are generated by moving the control points away from their home locations. Images of digits can be produced by placing Gaussian ink generators uniformly along the spline. Real images can be recognized by finding the digit model most likely to have generated the data. For each digit model we use an elastic matching algorithm to minimize an energy function that includes both the deformation energy of the digit model and the log probability that the model would generate the inked pixels in the image. The model with the lowest total energy wins. If a uniform noise process is included in the model of image generation  some of the inked pixels can be rejected as noise as a digit model is fitting a poorly segmented image. The digit models learn by modifying the home locations of the control points.            "
1991,Adaptive Soft Weight Tying using Gaussian Mixtures.,One way of simplifying neural networks so that they generalize better is to add an extra term to the error function that will penalize complexity. We propose a new penalty term in which the distribution of weight values is modelled as a mixture of multiple gaussians. Under this model  a set of weights is simple if the weights can be clustered into subsets so that the weights in each cluster have similar values. We allow the parameters of the mixture model to adapt at the same time as the network learns. Simulations demonstrate that this complexity term is more effective than previous complexity terms.               
1990,Mapping Part-Whole Hierarchies into Connectionist Networks.,Three different ways of mapping part-whole hierarchies into connectionist networks are described. The simplest scheme uses a fixed mapping and is inadequate for most tasks because it fails to share units and connections between different pieces of the part-whole hierarchy. Two alternative schemes are described  each of which involves a different method of time-sharing connections and units. The scheme we finally arrive at suggests that neural networks have two quite different methods for performing inference. Simple “intuitive” inferences can be performed by a single settling of a network without changing the way in which the world is mapped into the network. More complex “rational” inferences involve a sequence of such settlings with mapping changes after each settling.              
1990,The Bootstrap Widrow-Hoff Rule as a Cluster-Formation Algorithm.,An algorithm that is widely used for adaptive equalization in current modems is the “bootstrap” or “decision-directed” version of the Widrow-Hoff rule. We show that this algorithm can be viewed as an unsupervised clustering algorithm in which the data points are transformed so that they form two clusters that are as tight as possible. The standard algorithm performs gradient ascent in a crude model of the log likelihood of generating the transformed data points from two gaussian distributions with fixed centers. Better convergence is achieved by using the exact gradient of the log likelihood.               
1990,A time-delay neural network architecture for isolated word recognition.,A translation-invariant back-propagation network is described that performs better than a sophisticated continuous acoustic parameter hidden Markov model on a noisy  100-speaker confusable vocabulary isolated word recognition task. The network's replicated architecture permits it to extract precise information from unaligned training patterns selected by a naive segmentation rule.              
1990,Building adaptive interfaces with neural networks: The glove-talk pilot study.,N/A
1990,Discovering Viewpoint-Invariant Relationships That Characterize Objects.,"Using an unsupervised learning procedure  a network is trained on an ensemble of images of the same two-dimensional object at different positions  orientations and sizes. Each half of the network ""sees"" one fragment of the object  and tries to produce as output a set of 4 parameters that have high mutual information with the 4 parameters output by the other half of the network. Given the ensemble of training patterns  the 4 parameters on which the two halves of the network can agree are the position  orientation  and size of the whole object  or some recoding of them. After training  the network can reject instances of other shapes by using the fact that the predictions made by its two halves disagree. If two competing networks are trained on an unlabelled mixture of images of two objects  they cluster the training cases on the basis of the objects' shapes  independently of the position  orientation  and size.   "
1990,Evaluation of Adaptive Mixtures of Competing Experts.,We compare the performance of the modular architecture  composed of competing expert networks  suggested by Jacobs  Jordan  Nowlan and Hinton (1991) to the performance of a single back-propagation network on a complex  but low-dimensional  vowel recognition task. Simulations reveal that this system is capable of uncovering interesting decompositions in a complex task. The type of decomposition is strongly influenced by the nature of the input to the gating network that decides which expert to use for each case. The modular architecture also exhibits consistently better generalization on many variations of the task.         
1989,Connectionist Learning Procedures.,A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative  with respect to the connection strength  of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple  gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger  more realistic tasks.           
1989,Deterministic Boltzmann Learning Performs Steepest Descent in Weight-Space.,The Boltzmann machine learning procedure has been successfully applied in deterministic networks of analog units that use a mean field approximation to efficiently simulate a truly stochastic system (Peterson and Anderson 1987). This type of “deterministic Boltzmann machine” (DBM) learns much faster than the equivalent “stochastic Boltzmann machine” (SBM)  but since the learning procedure for DBM's is only based on an analogy with SBM's  there is no existing proof that it performs gradient descent in any function  and it has only been justified by simulations. By using the appropriate interpretation for the way in which a DBM represents the probability of an output vector given an input vector  it is shown that the DBM performs steepest descent in the same function as the original SBM  except at rare discontinuities. A very simple way of forcing the weights to become symmetrical is also described  and this makes the DBM more biologically plausible than back-propagation (Werbos 1974; Parker 1985; Rumelhart et al. 1986).         
1989,Phoneme recognition using time-delay neural networks.,The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units  a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces  which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task  the speaker-dependent recognition of the phonemes B  D  and G in varying phonetic contexts was chosen. For comparison  several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%.         
1989,Dimensionality Reduction and Prior Knowledge in E-Set Recognition.,It is well known that when an automatic learning algorithm is applied to a fixed corpus of data  the size of the corpus places an upper bound on the number of degrees of freedom that the model can contain if it is to generalize well. Because the amount of hardware in a neural network typically increases with the dimensionality of its inputs  it can be challenging to build a high-performance network for classifying large input patterns. In this paper  several techniques for addressing this problem are discussed in the context of an isolated word recognition task.            
1989,TRAFFIC: Recognizing Objects Using Hierarchical Reference Frame Transformations.,We describe a model that can recognize two-dimensional shapes in an unsegmented image  independent of their orientation  position  and scale. The model  called TRAFFIC  efficiently represents the structural relation between an object and each of its component features by encoding the fixed viewpoint-invariant transformation from the feature's reference frame to the object's in the weights of a connectionist network. Using a hierarchy of such transformations  with increasing complexity of features at each successive layer  the network can recognize multiple objects in parallel. An implementation of TRAFFIC is described  along with experimental results demonstrating the network's ability to recognize constellations of stars in a viewpoint-invariant manner.       
1989,Discovering High Order Features with Mean Field Modules.,"A new form of the deterministic Boltzmann machine (DBM) learning procedure is presented which can efficiently train network modules to discriminate between input vectors according to some criterion. The new technique directly utilizes the free energy of these ""mean field modules"" to represent the probability that the criterion is met  the free energy being readily manipulated by the learning procedure. Although conventional deterministic Boltzmann learning fails to extract the higher order feature of shift at a network bottleneck  combining the new mean field modules with the mutual information objective function rapidly produces modules that perfectly extract this important higher order feature without direct external supervision.             "
1988,A Distributed Connectionist Production System.,DCPS is a connectionist production system interpreter that uses distributed representations. As a connectionist model it consists of many simple  richly interconnected neuron‐like computing units that cooperate to solve problems in parallel. One motivation for constructing DCPS was to demonstrate that connectionist models are capable of representing and using explicit rules. A second motivation was to show how “coarse coding” or “distributed representations” can be used to construct a working memory that requires far fewer units than the number of different facts that can potentially be stored. The simulation we present is intended as a detailed demonstration of the feasibility of certain ideas and should not be viewed as a full implementation of production systems. Our current model only has a few of the many interesting emergent properties that we eventually hope to demonstrate: It is damage‐resistant  it performs matching and variable binding by massively parallel constraint satisfaction  and the capacity of its working memory is dependent on the similarity of the items being stored.            
1988,GEMINI: Gradient Estimation Through Matrix Inversion After Noise Injection.,Learning procedures that measure how random perturbations of unit activities correlate with changes in reinforcement are inefficient but simple to implement in hardware. Procedures like back-propagation (Rumelhart  Hinton and Williams  1986) which compute how changes in activities affect the output error are much more efficient  but require more complex hardware. GEMINI is a hybrid procedure for multilayer networks  which shares many of the implementation advantages of correlational reinforcement procedures but is more efficient. GEMINI injects noise only at the first hidden layer and measures the resultant effect on the output error. A linear network associated with each hidden layer iteratively inverts the matrix which relates the noise to the error change  thereby obtaining the error-derivatives. No back-propagation is involved  thus allowing unknown non-linearities in the system. Two simulations demonstrate the effectiveness of GEMINI         
1987,How Learning Can Guide Evolution.,The assumption that acquired characteristics are not inherited is often taken to imply that the adaptations that an organism learns during its lifetime cannot guide the course of evolution. This inference is incorrect [2]. Learning alters the shape of the search space in which evolution operates and thereby provides good evolutionary paths towards sets of co-adapted alleles. We demonstrate that this affect allows learning organisms to evolve much faster than their non-learning equivalents  even though the characteristics acquired by the phenotype are not communicated to the genotype.              
1987,Connectionist Architectures for Artificial Intelligence.,A number of researchers have begun exploring the use of massively parallel architectures in an attempt to get around the limitations of conventional symbol processing. Many of these parallel architectures are connectionist: The system's collection of permanent knowledge is stored as a pattern of connections or connection strengths among the processing elements  so the knowledge directly determines how the processing elements interact rather that sitting passively in a memory  waiting to be looked at by the CPU. Some connectionist schemes use formal  symbolic representations  while others use more analog approaches. Some even develop their own internal representations after seeing examples of the patterns they are to recognize or the relationships they are to store. Connectionism is somewhat controversial in the AI community. It is new  still unproven in large-scale practical applications  and very different in style from the traditional AI approach. The authors have only begun to explore the behavior and potential of connectionist networks. In this article  the authors describe some of the central issues and ideas of connectionism  and also some of the unsolved problems facing this approach. Part of the motivation for connectionist research is the possible similarity in function between connectionist networks and the neural networks of the human cortex  but they concentrate here on connectionism's potential as a practical technology for building intelligent systems.       
1987,Learning Representations by Recirculation.,"We describe a new learning procedure for networks that contain groups of nonlinear units arranged in a closed loop. The aim of the learning is to discover codes that allow the activity vectors in a ""visible"" group to be represented by activity vectors in a ""hidden"" group. One way to test whether a code is an accurate representation is to try to reconstruct the visible vector from the hidden vector. The difference between the original and the reconstructed visible vectors is called the reconstruction error  and the learning procedure aims to minimize this error. The learning procedure has two passes. On the first pass  the original visible vector is passed around the loop  and on the second pass an average of the original vector and the reconstructed vector is passed around the loop. The learning procedure changes each weight by an amount proportional to the product of the ""presynaptic"" activity and the difference in the post-synaptic activity on the two passes. This procedure is much simpler to implement than methods like back-propagation. Simulations in simple networks show that it usually converges rapidly on a good set of codes  and analysis shows that in certain restricted cases it performs gradient descent in the squared reconstruction error.           "
1987,Learning Translation Invariant Recognition in Massively Parallel Networks.,One major goal of research on massively parallel networks of neuron-like processing elements is to discover efficient methods for recognizing patterns. Another goal is to discover general learning procedures that allow networks to construct the internal representations that are required for complex tasks. This paper describes a recently developed procedure that can learn to perform a recognition task. The network is trained on examples in which the input vector represents an instance of a pattern in a particular position and the required output vector represents its name. After prolonged training  the network develops canonical internal representations of the patterns and it uses these canonical representations to identify familiar patterns in novel positions.              
1986,Learning in Massively Parallel Nets (Panel).,The human brain is very different from a conventional digital computer. It relies on massive parallelism rather than raw speed and it stores long-term knowledge by modifying the way its processing elements interact rather than by setting bits in a passive  general purpose memory. It is robust against minor physical damage and it learns from experience instead of being explicitly programmed. We do not yet know how the brain uses the activities of neurons to represent complex  articulated structures  or how the perceptual system turns the raw input into useful internal representations so rapidly. Nor do we know how the brain learns new representational schemes. But over the past few years there have been a lot of new and interesting theories about these issues. Much of the theorizing has been motivated by the belief that the brain is using computational principles which could also be applied to massively parallel artificial systems  if only we knew what the principles were. One major goal of research on massively parallel networks of neuron-like processing elements is to discover efficient methods for recognizing patterns. Another goal is to discover general learning procedures that allow networks to construct the internal representations that are required for complex tasks. This paper describes a recently developed procedure that can learn to perform a recognition task. The network is trained on examples in which the input vector represents an instance of a pattern in a particular position and the required output vector represents its name. After prolonged training  the network develops canonical internal representations of the patterns and it uses these canonical representations to identify familiar patterns in novel positions.         
1985,A Learning Algorithm for Boltzmann Machines.,The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches  but to use the connections efficiently two conditions must be met: First  a search technique that is suitable for parallel networks must be found. Second  there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method  based on statistical mechanics  and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.          
1985,Symbols Among the Neurons: Details of a Connectionist Inference Architecture.,Pattern matching and variable binding are easily implemented in conventional computer architectures  but not necessarily in all architectures. In a distributed neural network architecture each symbol               
1985,Shape Recognition and Illusory Conjunctions.,"One way to achieve viewpoint invariant shape recognition is to impose a canonical  object based frame of reference on a shape and to describe the positions  sizes and orientations of the shape's features relative to the imposed frame. This computation can be implemented in a parallel network of neuron-like processors  but the network has a tendency to make errors of a peculiar kind: When presented with several shapes it sometimes perceives one shape in the position of another. The parameters can be carefully tuned to avoid these ""illusory conjunctions"" in normal circumstances  but they reappear if the visual input is replaced by a random mask before the network has settled down. Treisman and Shmidt (1982) have shown that people make similar errors.           "
1983,"Massively Parallel Architectures for AI: NETL, Thistle, and Boltzmann Machines.",N/A
1981,A Parallel Computation that Assigns Canonical Object-Based Frames of Reference.,A viewpoint independent description of the shape of an object can be generated by imposing a canonical frame of reference on the object and describing the spatial dispositions of the parts relative to this object-based frame. When a familiar object is in an unusual orientation  the deciding factor in the choice of the canonical object-based frame may be the fact that relative to this frame the object has a familiar shape description. This may suggest that we first hypothesize an object-based frame and then test the resultant shape description for familiarity. However  it is possible to organise the interactions between units in a parallel network so that the pattern of activity in the network simultaneously converges on a representation of the shape and a representation of the object based frame of reference. The connections in the network are determined by the constraints inherent in the image formation process.             
1981,Shape Representation in Parallel Systems.,There has been a recent revival of interest in parallel systems in which computation is performed by excitatory and inhibitory interactions within a network of relatively simple  neuronlike units. At the early stages of visual processing  individual units can represent hypotheses about how small local fragments of the visual input should be interpreted  and interactions between units can encode knowledge about the constraints between local interpretations. Higher up in the visual system  the representational issues are more complex. This paper considers the difficulties involved in representing shapes in parallel systems  and suggests ways of overcoming them. In doing so  it provides a mechanism for shape perception and visual attention which allows a novel interpretation of the Gestalt slogan that the whole is more than the sum of its parts.         
1979,Some Demonstrations of the Effects of Structural Descriptions in Mental Imagery.,A visual imagery task is presented which is beyond the limits of normal human ability  and some of the factors contributing to its difficulty are isolated by comparing the difficulty of related tasks. It is argued that complex objects are assigned hierarchical structural descriptions by being parsed into parts  each of which has its own local system of significant directions. Two quite different schemas for a wire‐frame cube are used to illustrate this theory  and some striking perceptual differences to which they give rise are described. The difficulty of certain mental imagery tasks is shown to depend on which of the alternative structural descriptions of an object is used  and this is interpreted as evidence that structural descriptions are an important component of mental images. Finally  it is argued that analog transformations like mental folding involve changing the values of continuous variables in a structural description.          
1978,Representation and Control in Vision.,
1977,Relaxation and its role in vision.,It is argued that a visual system  especially one which handles imperfect data  needs a way of selecting the best consistent combination from among the many interrelated  locally plausible hypotheses about how parts or aspects of the visual input may be interpreted. A method is presented in which each hypothesis is given a supposition value between 0 and 1. A parallel relaxation I operator  based on the plausibilities of hypotheses and the logical relations between them  is then used to modify the supposition values  and the process is repeated until the best consistent set of hypotheses have supposition values of approximately 1  and the rest have values of approximately 0. The method is incorporated in a program which can interpret configurations of overlapping rectangles as puppets. For this task it is possible to formulate all the potentially relevant hypotheses before using relaxation to select the best consistent set. For more complex tasks  it is necessary to use relaxation on the locally plausible interpretations to guide the search for locally less obvious ones. Ways of doing this are discussed. Finally  an implemented system is presented which allows the user to specify schemas and inference rules  and uses relaxation to control the building of a network of instances of the schemas  when presented with data about some instances and relations between them    
1976,Using Relaxation to find a Puppet.,The problem of finding a puppet in a configuration of overlapping  transparent rectangles is used to show how a relaxation algorithm can extract the globally best figure from a network of conflicting local interpretations.              
