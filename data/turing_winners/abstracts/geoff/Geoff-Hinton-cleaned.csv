2019,analyzing improving representation soft nearest neighbor loss,explore expand soft nearest neighbor loss measure entanglement class manifold representation space ie  close pair point class relative pair point different class demonstrate several use case loss analytical tool provides insight evolution class similarity structure learning surprisingly    find maximizing entanglement representation different class hidden layer beneficial discrimination final layer  possibly encourages representation identify class-independent similarity structure maximizing soft nearest neighbor loss hidden layer lead better-calibrated estimate uncertainty outlier data also marginally improved generalization data training distribution recognized observing hidden layer  fewer normal number neighbor predicted class          
2019,similarity neural network representation revisited,recent work sought understand behavior neural network comparing representation layer different trained model examine method comparing neural network representation based canonical correlation analysis  show cca belongs family statistic measuring multivariate similarity  neither cca statistic invariant invertible linear transformation measure meaningful similarity representation higher dimension number data point introduce similarity index measure relationship representational similarity matrix suffer limitation similarity index equivalent centered kernel alignment  also closely connected cca unlike cca  cka reliably identify correspondence representation network trained different initialization             
2019,analyzing improving representation soft nearest neighbor loss,explore expand soft nearest neighbor loss measure entanglement class manifold representation space ie  close pair point class relative pair point different class demonstrate several use case loss analytical tool provides insight evolution class similarity structure learning surprisingly    find maximizing entanglement representation different class hidden layer beneficial discrimination final layer  possibly encourages representation identify class-independent similarity structure maximizing soft nearest neighbor loss hidden layer lead better-calibrated estimate uncertainty outlier data also marginally improved generalization data training distribution recognized observing hidden layer  fewer normal number neighbor predicted class          
2019,similarity neural network representation revisited,recent work sought understand behavior neural network comparing representation layer different trained model examine method comparing neural network representation based canonical correlation analysis  show cca belongs family statistic measuring multivariate similarity  neither cca statistic invariant invertible linear transformation measure meaningful similarity representation higher dimension number data point introduce similarity index measure relationship representational similarity matrix suffer limitation similarity index equivalent centered kernel alignment  also closely connected cca unlike cca  cka reliably identify correspondence representation network trained different initialization             
2019,cerberus multi-headed derenderer,generalize novel visual scene new viewpoint new object pose  visual system need representation shape part object invariant change viewpoint pose graphic representation disentangle visual factor viewpoint lighting object structure natural way possible learn invert process convert graphic representation image  provided graphic representation available label unlabeled image available  however  learning derender much harder consider simple model set free floating part part relation camera triangular mesh deformed model shape part test time  neural network look single image extract shape part relation camera part viewed one head multi-headed derenderer training  extracted part used input differentiable renderer reconstruction error backpropagated train neural net make learning task easier encouraging deformation part mesh invariant change viewpoint invariant change relative position part occur pose articulated body change cerberus  multi-headed derenderer  outperforms previous method extracting part single image without part annotation  quite well extracting natural part human figure      
2019,learning sparse network using targeted dropout,neural network easier optimise many weight required modelling mapping input output suggests two-stage learning procedure first learns large net prune away connection hidden unit standard training necessarily encourage net amenable pruning introduce targeted dropout  method training neural network robust subsequent pruning computing gradient weight update  targeted dropout stochastically selects set unit weight dropped using simple self-reinforcing sparsity criterion computes gradient remaining weight resulting network robust post hoc pruning weight unit frequently occur dropped set method improves upon complicated sparsifying regularisers simple implement easy tune             
2019,label smoothing help,generalization learning speed multi-class neural network often significantly improved using soft target weighted average hard target uniform distribution label smoothing label way prevents network becoming over-confident label smoothing used many state-of-the-art model  including image classification  language translation speech recognition despite widespread use  label smoothing still poorly understood show empirically addition improving generalization  label smoothing improves model calibration significantly improve beam-search however  also observe teacher network trained label smoothing  knowledge distillation student network much le effective explain observation  visualize label smoothing change representation learned penultimate layer network show label smoothing encourages representation training example class group tight cluster result loss information logits resemblance instance different class  necessary distillation  hurt generalization calibration model prediction      
2019,stacked capsule autoencoders,object composed set geometrically organized part introduce unsupervised capsule autoencoder   explicitly us geometric relationship part reason object since relationship depend viewpoint  model robust viewpoint change scae consists two stage first stage  model predicts presence pose part template directly image try reconstruct image appropriately arranging template second stage  scae predicts parameter object capsule  used reconstruct part pose inference model amortized performed off-the-shelf neural encoders  unlike previous capsule network find object capsule presence highly informative object class  lead state-of-the-art result unsupervised classification svhn  mnist         
2019,detecting diagnosing adversarial image class-conditional capsule reconstruction,adversarial example raise question whether neural network model sensitive visual feature human proposed method mitigating adversarial example subsequently defeated stronger attack motivated issue  take different approach propose instead detect adversarial example based class-conditional reconstruction input method us reconstruction network proposed part capsule network   general enough applied standard convolutional network find adversarial otherwise corrupted image result much larger reconstruction error normal input  prompting simple detection method thresholding reconstruction error based finding  propose reconstructive attack seek cause misclassification low reconstruction error attack produce undetected adversarial example  find capsnets resulting perturbation cause image appear visually like target class suggests capsnets utilize feature aligned human perception address central issue raised adversarial example          
2019,lookahead optimizer k step forward 1 step back,vast majority successful deep neural network trained using variant stochastic gradient descent  algorithm recent attempt improve sgd broadly categorized two approach  adaptive learning rate scheme  adagrad adam   accelerated scheme  heavy-ball nesterov momentum paper  propose new optimization algorithm  lookahead  orthogonal previous approach iteratively update two set weight intuitively  algorithm chooses search direction looking ahead sequence fast weight generated another optimizer show lookahead improves learning stability lower variance inner optimizer negligible computation memory cost empirically demonstrate lookahead significantly improve performance sgd adam  even default hyperparameter setting imagenet  cifar-/  neural machine translation  penn treebank    
2019,cvxnets learnable convex decomposition,solid object decomposed collection convex polytopes  small number convexes used  decomposition thought piece-wise approximation geometry decomposition fundamental real-time physic simulation computer graphic  creates unifying representation dynamic geometry collision detection convex object also property simultaneously explicit implicit representation one interpret explicitly mesh derived computing vertex convex hull  implicitly collection half-space constraint support function implicit representation make particularly well suited neural network training  abstract away topology geometry need represent introduce network architecture represent low dimensional family convexes family automatically derived via autoencoding process investigate application network including automatic convex decomposition  image reconstruction  part-based shape retrieval        
2018,said modeling individual labelers improves classification,data often labeled many different expert expert labeling small fraction data data point labeled several expert reduces workload individual expert also give better estimate unobserved ground truth expert disagree  standard approach treat majority opinion correct label model correct label distribution approach  however  make use potentially valuable information expert produced label make use extra information  propose modeling expert individually learning averaging weight combining  possibly sample-specific way allows u give weight reliable expert take advantage unique strength individual expert classifying certain type data show approach lead improvement computer-aided diagnosis diabetic retinopathy also show method performs better competing algorithm welinder perona   mnih hinton  work offer innovative approach dealing myriad real-world setting use expert opinion define label training         
2018,illustrative language understanding large-scale visual grounding image search,introduce picturebook  large-scale lookup operation ground language via ‘snapshots’ physical world accessed image search word vocabulary  extract top-k image google image search feed image convolutional network extract word embedding introduce multimodal gating function fuse picturebook embeddings word representation also introduce inverse picturebook  mechanism map picturebook embedding back word experiment report result across wide range task word similarity  natural language inference  semantic relatedness  sentiment/topic classification  image-sentence ranking machine translation also show gate activation corresponding picturebook embeddings highly correlated human judgment concreteness rating        
2018,large scale distributed neural network training online distillation,technique ensembling distillation promise model quality improvement paired almost base model however  due increased test-time cost  increased complexity training pipeline   technique challenging use industrial setting paper explore variant distillation relatively straightforward use require complicated multi-stage setup many new hyperparameters first claim online distillation enables u use extra parallelism fit large datasets twice fast crucially  still speed training even already reached point additional parallelism provides benefit synchronous asynchronous stochastic gradient descent two neural network trained disjoint subset data share knowledge encouraging model agree prediction model would made prediction come stale version model safely computed using weight rarely get transmitted second claim online distillation cost-effective way make exact prediction model dramatically reproducible support claim using experiment criteo display ad challenge dataset  imagenet  largest to-date dataset used neural language modeling  containing  token based common crawl repository web data         
2018,matrix capsule em routing,capsule group neuron whose output represent different property entity layer capsule network contains many capsule describe version capsule capsule logistic unit represent presence entity x matrix could learn represent relationship entity viewer  capsule one layer vote pose matrix many different capsule layer multiplying pose matrix trainable viewpoint-invariant transformation matrix could learn represent part-whole relationship vote weighted assignment coefficient coefficient iteratively updated image using expectation-maximization algorithm output capsule routed capsule layer receives cluster similar vote transformation matrix trained discriminatively backpropagating unrolled iteration em pair adjacent capsule layer smallnorb benchmark  capsule reduce number test error  compared state-of-the-art capsule also show far resistance white box adversarial attack baseline convolutional neural network              
2018,assessing scalability biologically-motivated deep learning algorithm architecture,backpropagation error algorithm  impossible implement real brain recent success deep network machine learning ai  however  inspired proposal understanding brain might learn across multiple layer  hence might approximate bp yet  none proposal rigorously evaluated task bp-guided deep learning proved critical  architecture structured simple fully-connected network present result scaling biologically motivated model deep learning datasets need deep network appropriate architecture achieve good performance present result mnist  cifar-  imagenet datasets explore variant target-propagation  feedback alignment  algorithm  explore performance fully- locally-connected architecture also introduce weight-transport-free variant difference target propagation  modified remove backpropagation penultimate layer many algorithm perform well mnist  cifar imagenet find tp fa variant perform significantly worse bp  especially network composed locally connected unit  opening question whether new architecture algorithm required scale approach result implementation detail help establish baseline biologically motivated deep learning scheme going forward    
2018,large scale distributed neural network training online distillation,technique ensembling distillation promise model quality improvement paired almost base model however  due increased test-time cost  increased complexity training pipeline   technique challenging use industrial setting paper explore variant distillation relatively straightforward use require complicated multi-stage setup many new hyperparameters first claim online distillation enables u use extra parallelism fit large datasets twice fast crucially  still speed training even already reached point additional parallelism provides benefit synchronous asynchronous stochastic gradient descent two neural network trained disjoint subset data share knowledge encouraging model agree prediction model would made prediction come stale version model safely computed using weight rarely get transmitted second claim online distillation cost-effective way make exact prediction model dramatically reproducible support claim using experiment criteo display ad challenge dataset  imagenet  largest to-date dataset used neural language modeling  containing × token based common crawl repository web data         
2018,assessing scalability biologically-motivated deep learning algorithm architecture,backpropagation error algorithm  impossible implement real brain recent success deep network machine learning ai  however  inspired proposal understanding brain might learn across multiple layer  hence might approximate bp yet  none proposal rigorously evaluated task bp-guided deep learning proved critical  architecture structured simple fully-connected network present result scaling biologically motivated model deep learning datasets need deep network appropriate architecture achieve good performance present result mnist  cifar-  imagenet datasets explore variant target-propagation  feedback alignment  algorithm  explore performance fully- locally-connected architecture also introduce weight-transport-free variant difference target propagation  modified remove backpropagation penultimate layer many algorithm perform well mnist  cifar imagenet find tp fa variant perform significantly worse bp  especially network composed locally connected unit  opening question whether new architecture algorithm required scale approach result implementation detail help establish baseline biologically motivated deep learning scheme going forward    
2018,darccc detecting adversary reconstruction class conditional capsule,present simple technique allows capsule model detect adversarial image addition trained classify image  capsule model trained reconstruct image pose parameter identity correct top-level capsule adversarial image look like typical member predicted class much larger reconstruction error reconstruction produced top-level capsule class show setting threshold l distance input image reconstruction winning capsule effective detecting adversarial image three different datasets technique work quite well cnns trained reconstruct image part last hidden layer softmax explore stronger  white-box attack take reconstruction error account attack able fool detection technique order make model change prediction another class  attack must typically make adversarial image resemble image class            
2017,imagenet classification deep convolutional neural network,trained large  deep convolutional neural network classify  million high-resolution image imagenet lsvrc- contest  different class test data  achieved top- top- error rate    respectively  considerably better previous state-of-the-art neural network   million parameter   neuron  consists five convolutional layer  followed max-pooling layer  three fully connected layer final -way softmax make training faster  used non-saturating neuron efficient gpu implementation convolution operation reduce overfitting fully connected layer employed recently developed regularization method called dropout proved effective also entered variant model ilsvrc- competition achieved winning top- test error rate   compared  achieved second-best entry    
2017,distilling neural network soft decision tree,deep neural network proved effective way perform classification task excel input data high dimensional  relationship input output complicated  number labeled training example large hard explain learned network make particular classification decision particular test case due reliance distributed hierarchical representation could take knowledge acquired neural net express knowledge model relies hierarchical decision instead  explaining particular decision would much easier describe way using trained neural net create type soft decision tree generalizes better one learned directly training data             
2017,regularizing neural network penalizing confident output distribution,propose regularizing neural network penalizing low entropy output distribution show penalizing low entropy output distribution  shown improve exploration reinforcement learning  act strong regularizer supervised learning connect confidence penalty label smoothing direction kl divergence network output distribution uniform distribution exhaustively evaluate proposed confidence penalty label smoothing   common benchmark image classification   language modeling   machine translation   speech recognition  find label smoothing confidence penalty improve state-of-the-art model across benchmark without modifying existing hyper-parameters          
2017,outrageously large neural network sparsely-gated mixture-of-experts layer,capacity neural network absorb information limited number parameter  conditional computation  part network active per-example basis  proposed theory way dramatically increasing model capacity without proportional increase computation  practice  however  significant algorithmic performance challenge  work  address challenge finally realize promise conditional computation  achieving greater x improvement model capacity minor loss computational efficiency modern gpu cluster  introduce sparsely-gated mixture-of-experts layer   consisting thousand feed-forward sub-networks  trainable gating network determines sparse combination expert use example  apply moe task language modeling machine translation  model capacity critical absorbing vast quantity knowledge available training corpus  present model architecture moe  billion parameter applied convolutionally stacked lstm layer  large language modeling machine translation benchmark  model achieve significantly better result state-of-the-art lower computational cost      
2017,dynamic routing capsule,capsule group neuron whose activity vector represents instantiation parameter specific type entity object object part use length activity vector represent probability entity exists orientation represent instantiation parameter active capsule one level make prediction  via transformation matrix  instantiation parameter higher-level capsule multiple prediction agree  higher level capsule becomes active show discrimininatively trained  multi-layer capsule system achieves state-of-the-art performance mnist considerably better convolutional net recognizing highly overlapping digit achieve result use iterative routing-by-agreement mechanism lower-level capsule prefers send output higher level capsule whose activity vector big scalar product prediction coming lower-level capsule           
2017,boltzmann machine,boltzmann machine network symmetrically connected  neuron-like unit make stochastic decision whether boltzmann machine simple learning algorithm allows discover interesting feature represent complex regularity training data learning algorithm slow network many layer feature detector  fast “restricted boltzmann machines” single layer feature detector many hidden layer learned efficiently composing restricted boltzmann machine  using feature activation one training data next boltzmann machine used solve two quite different computational problem search problem   weight connection fixed used represent cost function stochastic dynamic boltzmann machine allow sample binary state vector low value cost
2017,deep belief net,deep belief net probabilistic generative model composed multiple layer stochastic latent variable  top two layer undirected  symmetric connection form associative memory lower layer receive top-down  directed connection layer deep belief net two important computational property first  efficient procedure learning top-down  generative weight specify variable one layer determine probability variable layer procedure learns one layer latent variable time second  learning multiple layer  value latent variable every layer inferred single  bottom-up pas start observed data vector bottom layer us generative weight reverse direction        
2017,outrageously large neural network sparsely-gated mixture-of-experts layer,capacity neural network absorb information limited number parameter conditional computation  part network active per-example basis  proposed theory way dramatically increasing model capacity without proportional increase computation practice  however  significant algorithmic performance challenge work  address challenge finally realize promise conditional computation  achieving greater x improvement model capacity minor loss computational efficiency modern gpu cluster introduce sparsely-gated mixture-of-experts layer   consisting thousand feed-forward sub-networks trainable gating network determines sparse combination expert use example apply moe task language modeling machine translation  model capacity critical absorbing vast quantity knowledge available training corpus present model architecture moe  billion parameter applied convolutionally stacked lstm layer large language modeling machine translation benchmark  model achieve significantly better result state-of-the-art lower computational cost      
2017,regularizing neural network penalizing confident output distribution,systematically explore regularizing neural network penalizing low entropy output distribution show penalizing low entropy output distribution  shown improve exploration reinforcement learning  act strong regularizer supervised learning furthermore  connect maximum entropy based confidence penalty label smoothing direction kl divergence exhaustively evaluate proposed confidence penalty label smoothing  common benchmark image classification   language modeling   machine translation   speech recognition  find label smoothing confidence penalty improve state-of-the-art model across benchmark without modifying existing hyperparameters  suggesting wide applicability regularizers        
2017,said modeling individual labelers improves classification,data often labeled many different expert expert labeling small fraction data data point labeled several expert reduces workload individual expert also give better estimate unobserved ground truth expert disagree  standard approach treat majority opinion correct label model correct label distribution approach  however  make use potentially valuable information expert produced label make use extra information  propose modeling expert individually learning averaging weight combining  possibly sample-specific way allows u give weight reliable expert take advantage unique strength individual expert classifying certain type data show approach lead improvement computer-aided diagnosis diabetic retinopathy also show method performs better competing algorithm welinder perona   mnih hinton  work offer innovative approach dealing myriad real-world setting use expert opinion define label training         
2017,dynamic routing capsule,capsule group neuron whose activity vector represents instantiation parameter specific type entity object object part use length activity vector represent probability entity exists orientation represent instantiation parameter active capsule one level make prediction  via transformation matrix  instantiation parameter higher-level capsule multiple prediction agree  higher level capsule becomes active show discrimininatively trained  multi-layer capsule system achieves state-of-the-art performance mnist considerably better convolutional net recognizing highly overlapping digit achieve result use iterative routing-by-agreement mechanism lower-level capsule prefers send output higher level capsule whose activity vector big scalar product prediction coming lower-level capsule           
2017,distilling neural network soft decision tree,deep neural network proved effective way perform classification task excel input data high dimensional  relationship input output complicated  number labeled training example large hard explain learned network make particular classification decision particular test case due reliance distributed hierarchical representation could take knowledge acquired neural net express knowledge model relies hierarchical decision instead  explaining particular decision would much easier describe way using trained neural net create type soft decision tree generalizes better one learned directly training data            
2016,attend infer repeat fast scene understanding generative model,present framework efficient inference structured image model explicitly reason object achieve performing probabilistic inference using recurrent neural network attends scene element process one time crucially  model learns choose appropriate number inference step use scheme learn perform inference partially specified model  fully specified model  show model learn identify multiple object - counting  locating classifying element scene - without supervision  eg  decomposing image various number object single forward pas neural network unprecedented speed show network produce accurate inference compared supervised counterpart  structure lead improved generalization          
2016,using fast weight attend recent past,recently  research artificial neural network largely restricted system two type variable neural activity represent current recent input weight learn capture regularity among input  output payoff good reason restriction synapsis dynamic many different time-scales suggests artificial neural network might benefit variable change slower activity much faster standard weight fast weight used store temporary memory recent past provide neurally plausible way implementing type attention past recently proven helpful sequence-to-sequence model using fast weight avoid need store copy neural activity pattern             
2016,attend infer repeat fast scene understanding generative model,present framework efficient inference structured image model explicitly reason object achieve performing probabilistic inference using recurrent neural network attends scene element process one time crucially  model learns choose appropriate number inference step use scheme learn perform inference partially specified model  fully specified model  show model learn identify multiple object - counting  locating classifying element scene - without supervision  eg  decomposing image various number object single forward pas neural network show network produce accurate inference compared supervised counterpart  structure lead improved generalization          
2016,layer normalization,training state-of-the-art  deep neural network computationally expensive one way reduce training time normalize activity neuron recently introduced technique called batch normalization us distribution summed input neuron mini-batch training case compute mean variance used normalize summed input neuron training case significantly reduces training time feed-forward neural network however  effect batch normalization dependent mini-batch size obvious apply recurrent neural network paper  transpose batch normalization layer normalization computing mean variance used normalization summed input neuron layer single training case like batch normalization  also give neuron adaptive bias gain applied normalization non-linearity unlike batch normalization  layer normalization performs exactly computation training test time also straightforward apply recurrent neural network computing normalization statistic separately time step layer normalization effective stabilizing hidden state dynamic recurrent network empirically  show layer normalization substantially reduce training time compared previously published technique         
2016,using fast weight attend recent past,recently  research artificial neural network largely restricted system two type variable neural activity represent current recent input weight learn capture regularity among input  output payoff good reason restriction synapsis dynamic many different time-scales suggests artificial neural network might benefit variable change slower activity much faster standard weight fast weight used store temporary memory recent past provide neurally plausible way implementing type attention past recently proved helpful sequence-to-sequence model using fast weight avoid need store copy neural activity pattern             
2015,deep learning,deep learning allows computational model composed multiple processing layer learn representation data multiple level abstraction method dramatically improved state-of-the-art speech recognition  visual object recognition  object detection many domain drug discovery genomics deep learning discovers intricate structure large data set using backpropagation algorithm indicate machine change internal parameter used compute representation layer representation previous layer deep convolutional net brought breakthrough processing image  video  speech audio  whereas recurrent net shone light sequential data text speech          
2015,grammar foreign language,syntactic constituency parsing fundamental problem natural language processing subject intensive researchand engineering decade result  accurate parser domain specific  complex  inefficient paper showthat domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art result widely used syntacticconstituency parsing dataset  trained large synthetic corpusthat annotated using existing parser also match performance standard parser trained small human-annotated dataset  show model highly data-efficient  contrast sequence-to-sequence model without theattention mechanism parser also fast  processing hundred sentence per second unoptimized cpu implementation        
2015,distilling knowledge neural network,simple way improve performance almost machine learning algorithm train many different model data average prediction unfortunately  making prediction using whole ensemble model cumbersome may computationally expensive allow deployment large number user  especially individual model large neural net caruana collaborator shown possible compress knowledge ensemble single model much easier deploy develop approach using different compression technique achieve surprising result mnist show significantly improve acoustic model heavily used commercial system distilling knowledge ensemble model single model also introduce new type ensemble composed one full model many specialist model learn distinguish fine-grained class full model confuse unlike mixture expert  specialist model trained rapidly parallel            
2015,simple way initialize recurrent network rectified linear unit,learning long term dependency recurrent network difficult due vanishing exploding gradient overcome difficulty  researcher developed sophisticated optimization technique network architecture paper  propose simpler solution use recurrent neural network composed rectified linear unit key solution use identity matrix scaled version initialize recurrent weight matrix find solution comparable lstm four benchmark two toy problem involving long-range temporal structure  large language modeling problem benchmark speech recognition problem            
2014,feature come,possible learn multiple layer non‐linear feature backpropagating error derivative feedforward neural network effective learning procedure huge amount labeled training data  many learning task labeled example available effort overcome need labeled data  several different generative model developed learned interesting feature modeling higher order statistical structure set input vector one generative model  restricted boltzmann machine   connection hidden unit make perceptual inference learning much simpler significantly  layer hidden feature learned  activity feature used training data another rbm applying idea recursively  possible learn deep hierarchy progressively complicated feature without requiring labeled data deep hierarchy treated feedforward neural network discriminatively fine‐tuned using backpropagation using stack rbms initialize weight feedforward neural network allows backpropagation work effectively much deeper network lead much better generalization stack rbms also used initialize deep boltzmann machine many hidden layer combining initialization method new method fine‐tuning weight finally lead first efficient way training boltzmann machine many hidden layer million weight        
2014,dropout simple way prevent neural network overfitting,deep neural net large number parameter powerful machine learning system however  overfitting serious problem network large network also slow use  making difficult deal overfitting combining prediction many different large neural net test time dropout technique addressing problem key idea randomly drop unit  neural network training prevents unit co-adapting much training  dropout sample exponential number different thinned network test time  easy approximate effect averaging prediction thinned network simply using single unthinned network smaller weight significantly reduces overfitting give major improvement regularization method show dropout improves performance neural network supervised learning task vision  speech recognition  document classification computational biology  obtaining state-of-the-art result many benchmark data set        
2014,application deep belief network natural language understanding,application deep belief net  various problem subject number recent study ranging image classification speech recognition audio classification study apply dbns natural language understanding problem recent surge activity area largely spurred development greedy layer-wise pretraining method us efficient learning algorithm called contrastive divergence  cd allows dbns learn multi-layer generative model unlabeled data feature discovered model used initialize feed-forward neural network fine-tuned backpropagation compare dbn-initialized neural network three widely used text classification algorithm support vector machine   boosting maximum entropy  plain dbn-based model give call-routing classification accuracy equal best model however  using additional unlabeled data dbn pre-training combining dbn-based learned feature original feature provides significant gain svms   turn  performed better maxent boosting          
2014,autoregressive product multi-frame prediction improve accuracy hybrid model,describe simple effective way using multi-frame target improve accuracy artificial neural network-hidden markov model  hybrid system approach deep neural network  trained predict forced-alignment state multiple frame using separate softmax frame contrast usual method training dnn predict state central frame sufficient improve accuracy system significantly however  average prediction frame — different context associated — achieve state art result timit using fully connected deep neural network without convolutional architecture dropout training  hour subset wall street journal  using context dependent dnn-hmm system lead relative improvement  dev set   test set               
2014,grammar foreign language,syntactic constituency parsing fundamental problem natural language processing subject intensive research engineering decade result  accurate parser domain specific  complex  inefficient paper show domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art result widely used syntactic constituency parsing dataset  trained large synthetic corpus annotated using existing parser also match performance standard parser trained small human-annotated dataset  show model highly data-efficient  contrast sequence-to-sequence model without attention mechanism parser also fast  processing hundred sentence per second unoptimized cpu implementation        
2013,modeling natural image using gated mrfs,paper describes markov random field real-valued image modelling two set latent variable one set used gate interaction pair pixel second set determines mean intensity pixel powerful model conditional distribution input gaussian mean covariance determined configuration latent variable  unlike previous model restricted use gaussians either fixed mean diagonal covariance matrix thanks increased flexibility  gated mrf generate realistic sample training unconstrained distribution high-resolution natural image futhermore  latent variable model inferred efficiently used effective descriptor recognition task generation discrimination drastically improce layer binary latent variable added model  yielding hierarchical model called deep belief network           
2013,rectified linear unit speech processing,deep neural network recently become gold standard acoustic modeling speech recognition system key computational unit deep network linear projection followed point-wise non-linearity  typically logistic function work  show improve generalization make training deep network faster simpler substituting logistic unit rectified linear unit unit linear input positive zero otherwise supervised setting  successfully train deep net random initialization large vocabulary speech recognition task achieving lower word error rate using logistic network topology similarly unsupervised setting  show learn sparse feature useful discriminative task experiment executed distributed environment using several hundred machine several hundred hour speech data           
2013,speech recognition deep recurrent neural network,recurrent neural network  powerful model sequential data end-to-end training method connectionist temporal classification make possible train rnns sequence labelling problem input-output alignment unknown combination method long short-term memory rnn architecture proved particularly fruitful  delivering state-of-the-art result cursive handwriting recognition however rnn performance speech recognition far disappointing  better result returned deep feedforward network paper investigates deep recurrent neural network  combine multiple level representation proved effective deep network flexible use long range context empowers rnns trained end-to-end suitable regularisation  find deep long short-term memory rnns achieve test set error  timit phoneme recognition benchmark  knowledge best recorded score          
2013,new type deep neural network learning speech recognition related application overview,paper  provide overview invited contributed paper presented special session icassp-  entitled “new type deep neural network learning speech recognition related application ” organized author also describe historical context acoustic model based deep neural network developed technical overview paper presented special session organized five way improving deep learning method  better optimization  better type neural activation function better network architecture  better way determine myriad hyper-parameters deep neural network  appropriate way preprocess speech deep neural network  way leveraging multiple language dialect easily achieved deep neural network gaussian mixture model            
2013,improving deep neural network lvcsr using rectified linear unit dropout,recently  pre-trained deep neural network  outperformed traditional acoustic model based gaussian mixture model  variety large vocabulary speech recognition benchmark deep neural net also achieved excellent result various computer vision task using random “dropout” procedure drastically improves generalization error randomly omitting fraction hidden unit layer since dropout help avoid over-fitting  also successful small-scale phone recognition task using larger neural net however  training deep neural net acoustic model large vocabulary speech recognition take long time dropout likely increase training time neural network rectified linear unit  non-linearities highly successful computer vision task proved faster train standard sigmoid unit  sometimes also improving discriminative performance work  show -hour english broadcast news task modified deep neural network using relus trained dropout frame level training provide  relative improvement dnn trained sigmoid unit   relative improvement strong gmm/hmm system able obtain result minimal human hyper-parameter tuning using publicly available bayesian optimization code         
2013,tensor analyzer,factor analysis statistical method seek explain linear variation data using unobserved latent variable due additive nature  suitable modeling data generated multiple group latent factor interact multiplicatively paper  introduce tensor analyzer multilinear generalization factor analyzer describe efficient way sampling posterior distribution factor value demonstrate sample used em algorithm learning interesting mixture model natural image patch tensor analyzer also accurately recognize face significant pose illumination variation given one previous image face also show tensor analyzer trained unsupervised  semi-supervised  fully supervised setting           
2013,importance initialization momentum deep learning,deep recurrent neural network  powerful model considered almost impossible train using stochastic gradient descent momentum paper  show stochastic gradient descent momentum us well-designed random initialization particular type slowly increasing schedule momentum parameter  train dnns rnns  level performance previously achievable hessian-free optimization find initialization momentum crucial since poorly initialized network cannot trained momentum well-initialized network perform markedly worse momentum absent poorly tuned success training model suggests previous attempt train deep recurrent neural network random initialization likely failed due poor initialization scheme furthermore  carefully tuned momentum method suffice dealing curvature issue deep recurrent network training objective without need sophisticated second-order method            
2013,using autoencoder deformable template discover feature automated speech recognition,paper show discover non-linear feature frame spectrogram using novel autoencoder autoencoder us neural network encoder predicts set prototype called template need transformed reconstruct data  decoder function performs operation transforming prototype reconstructing input demonstrate method spectrogram timit database feature used deep neural network - hidden markov model  hybrid system automatic speech recognition timit monophone recognition task able achieve gain  mel log spectrum  augmenting traditional spectrum predicted transformation parameter  using recently discovered edropoutf training  able achieve phone error rate   dev set  test set   knowledge best reported number task using hybrid system speaking rate normalization lattice-based context-dependent phoneme duration modeling personalized speech recognizers mobile device         
2013,modeling document deep boltzmann machine,introduce deep boltzmann machine model suitable modeling extracting latent semantic representation large unstructured collection document overcome apparent difficulty training dbm judicious parameter tying parameter tying enables efficient pretraining algorithm state initialization scheme aid inference model trained efficiently standard restricted boltzmann machine experiment show model assigns better log probability unseen data replicated softmax model feature extracted model outperform lda  replicated softmax  docnade model document retrieval document classification task             
2013,discovering multiple constraint frequently approximately satisfied,high-dimensional datasets modelled assuming many different linear constraint  frequently approximately satisfied  data probability data vector model proportional product probability constraint violation describe three method learning product constraint using heavy-tailed probability distribution violation              
2013,speech recognition deep recurrent neural network,recurrent neural network  powerful model sequential data end-to-end training method connectionist temporal classification make possible train rnns sequence labelling problem input-output alignment unknown combination method long short-term memory rnn architecture proved particularly fruitful  delivering state-of-the-art result cursive handwriting recognition however rnn performance speech recognition far disappointing  better result returned deep feedforward network paper investigates {deep recurrent neural networks}  combine multiple level representation proved effective deep network flexible use long range context empowers rnns trained end-to-end suitable regularisation  find deep long short-term memory rnns achieve test set error  timit phoneme recognition benchmark  knowledge best recorded score          
2013,modeling document deep boltzmann machine,introduce deep boltzmann machine model suitable modeling extracting latent semantic representation large unstructured collection document overcome apparent difficulty training dbm judicious parameter tying parameter tying enables efficient pretraining algorithm state initialization scheme aid inference model trained efficiently standard restricted boltzmann machine experiment show model assigns better log probability unseen data replicated softmax model feature extracted model outperform lda  replicated softmax  docnade model document retrieval document classification task             
2012,visualizing non-metric similarity multiple map,technique multidimensional scaling visualize object point low-dimensional metric map result  visualization subject fundamental limitation metric space limitation prevent multidimensional scaling faithfully representing non-metric similarity data word association event co-occurrence particular  multidimensional scaling cannot faithfully represent intransitive pairwise similarity visualization  cannot faithfully visualize “central” object paper  present extension recently proposed multidimensional scaling technique called t-sne extension aim address problem traditional multidimensional scaling technique technique used visualize non-metric similarity new technique  called multiple map t-sne  alleviates problem constructing collection map reveal complementary structure similarity data apply multiple map t-sne large data set word association data data set nip co-authorships  demonstrating ability successfully visualize non-metric similarity        
2012,efficient learning procedure deep boltzmann machine,present new learning algorithm boltzmann machine contain many layer hidden variable data-dependent statistic estimated using variational approximation tends focus single mode  data-independent statistic estimated using persistent markov chain use two quite different technique estimating two type statistic enter gradient log likelihood make practical learn boltzmann machine multiple hidden layer million parameter learning made efficient using layer-by-layer pretraining phase initializes weight sensibly pretraining also allows variational inference initialized sensibly single bottom-up pas present result mnist norb data set showing deep boltzmann machine learn good generative model handwritten digit object also show feature discovered deep boltzmann machine effective way initialize hidden layer feedforward neural net  discriminatively fine-tuned             
2012,introduction special section deep learning speech language processing,current speech recognition system  example  typically use gaussian mixture model   estimate observation  probability hidden markov model   gmms generative model one layer latent variable instead developing powerful model  research effort gone finding better way estimating gmm parameter error rate decreased margin different class increased observation hold natural language processing  maximum entropy  model conditional random field  popular last decade approach use shallow model whose success largely depends use carefully handcrafted feature          
2012,acoustic modeling using deep belief network,gaussian mixture model currently dominant technique modeling emission distribution hidden markov model speech recognition show better phone recognition timit dataset achieved replacing gaussian mixture model deep neural network contain many layer feature large number parameter network first pre-trained multi-layer generative model window spectral feature vector without making use discriminative information generative pre-training designed feature  perform discriminative fine-tuning using backpropagation adjust feature slightly make better predicting probability distribution state monophone hidden markov model              
2012,robust boltzmann machine recognition denoising,boltzmann machine successful unsupervised learning density modeling image speech data  sensitive noise data paper  introduce novel model  robust boltzmann machine   allows boltzmann machine robust corruption domain visual recognition  robm able accurately deal occlusion noise using multiplicative gating induce scale mixture gaussians pixel image denoising in-painting correspond posterior inference robm model trained unsupervised fashion unlabeled noisy data learn spatial structure occluders compared standard algorithm  robm significantly better recognition denoising several face database         
2012,understanding deep belief network perform acoustic modelling,deep belief network  competitive alternative gaussian mixture model relating state hidden markov model frame coefficient derived acoustic input competitive three reason dbns fine-tuned neural network dbns many non-linear hidden layer dbns generatively pre-trained paper illustrates three aspect contributes dbns good recognition performance using phone recognition performance timit corpus dimensionally reduced visualization relationship feature vector learned dbns preserve similarity structure feature vector multiple scale two method also used investigate suitable type input representation dbn               
2012,learning label aerial image noisy data,training system label image  amount labeled training data tends limiting factor consider task learning label aerial image existing map provide abundant label  label often incomplete sometimes poorly registered propose two robust loss function dealing kind label noise use loss function train deep neural network two challenging aerial image datasets robust loss function lead big improvement performance best system substantially outperforms best published result task consider             
2012,deep mixture factor analyser,efficient way learn deep density model many layer latent variable learn one layer time using model one layer latent variable learning layer  sample posterior distribution layer used training data learning next layer approach commonly used restricted boltzmann machine  undirected graphical model single hidden layer  also used mixture factor analyser  directed graphical model paper  present greedy layer-wise learning algorithm deep mixture factor analyser  even though dmfa converted equivalent shallow mfa multiplying together factor loading matrix different level  learning inference much efficient dmfa sharing lower-level factor loading matrix many different higher level mfa prevents overfitting demonstrate empirically dmfas learn better density model mfa two type restricted boltzmann machine  wide variety datasets          
2012,deep lambertian network,visual perception challenging problem part due illumination variation possible solution first estimate illumination invariant representation using recognition object albedo surface normal example representation paper  introduce multilayer generative model latent variable include albedo  surface normal  light source combining deep belief net lambertian reflectance assumption  model learn good prior albedo image illumination variation explained changing lighting latent variable model transferring learned knowledge similar object  albedo surface normal estimation single image possible model experiment demonstrate model able generalize well improve standard baseline one-shot face recognition           
2012,imagenet classification deep convolutional neural network,trained large  deep convolutional neural network classify  million high-resolution image lsvrc- imagenet training set  different class test data  achieved top- top- error rate   considerably better previous state-of-the-art result neural network   million parameter   neuron  consists five convolutional layer  followed max-pooling layer  two globally connected layer final -way softmax make training faster  used non-saturating neuron efficient gpu implementation convolutional net reduce overfitting globally connected layer employed new regularization method proved effective       
2012,better way pretrain deep boltzmann machine,describe pre-training algorithm deep boltzmann machine  related pre-training algorithm deep belief network show certain condition  pre-training procedure improves variational lower bound two-hidden-layer dbm based analysis  develop different method pre-training dbms distributes modelling work evenly hidden layer result mnist norb datasets demonstrate new pre-training algorithm allows u learn better generative model             
2012,practical guide training restricted boltzmann machine,restricted boltzmann machine  used generative model many different type data rbms usually trained using contrastive divergence learning procedure requires certain amount practical experience decide set value numerical meta-parameters last year  machine learning group university toronto acquired considerable expertise training rbms guide attempt share expertise machine learning researcher              
2012,conditional restricted boltzmann machine structured output prediction,conditional restricted boltzmann machine  rich probabilistic model recently applied wide range problem  including collaborative filtering  classification  modeling motion capture data much progress made training non-conditional rbms  algorithm applicable conditional model almost work training generating prediction conditional rbms structured output problem first argue standard contrastive divergence-based learning may suitable training crbms identify two distinct type structured output prediction problem propose improved learning algorithm first problem type one output space arbitrary structure set likely output configuration relatively small  multi-label classification second problem one output space arbitrarily structured output space variability much greater  image denoising pixel labeling show new learning algorithm work much better contrastive divergence type problem         
2012,product hidden markov model take n > 1 tango,product hidden markov model interesting class generative model received little attention since introduction maybe part due computationally expensive gradient-based learning algorithm intractability computing log likelihood sequence model paper  demonstrate partition function estimated reliably via annealed importance sampling perform experiment using contrastive divergence learning rainfall data data captured pair people dancing result suggest advance learning evaluation undirected graphical model recent increase available computing power make pohmms worth considering complex time-series modeling task             
2012,deep mixture factor analyser,efficient way learn deep density model many layer latent variable learn one layer time using model one layer latent variable learning layer  sample posterior distribution layer used training data learning next layer approach commonly used restricted boltzmann machine  undirected graphical model single hidden layer  also used mixture factor analyser  directed graphical model paper  present greedy layer-wise learning algorithm deep mixture factor analyser  even though dmfa converted equivalent shallow mfa multiplying together factor loading matrix different level  learning inference much efficient dmfa sharing lower-level factor loading matrix many different higher level mfa prevents overfitting demonstrate empirically dmfas learn better density model mfa two type restricted boltzmann machine wide variety datasets          
2012,improving neural network preventing co-adaptation feature detector,large feedforward neural network trained small training set  typically performs poorly held-out test data overfitting greatly reduced randomly omitting half feature detector training case prevents complex co-adaptations feature detector helpful context several specific feature detector instead  neuron learns detect feature generally helpful producing correct answer given combinatorially large variety internal context must operate random dropout give big improvement many benchmark task set new record speech object recognition             
2012,efficient parametric projection pursuit density estimation,product model low dimensional expert powerful way avoid curse dimensionality present ``under-complete product expert   expert model one dimensional projection data upoe fully tractable may interpreted parametric probabilistic model projection pursuit ml learning rule identical approximate learning rule proposed under-complete ica also derive efficient sequential learning algorithm discus relationship projection pursuit density estimation feature induction algorithm additive random field model              
2011,better way learn feature technical perspective!,n/a
2011,two distributed-state model generating high-dimensional time series,paper develop class nonlinear generative model high-dimensional time series first propose model based restricted boltzmann machine  us undirected model binary latent variable real-valued visible variable latent visible variable time step receive directed connection visible variable last time-steps conditional rbm  make on-line inference efficient allows u use simple approximate learning procedure demonstrate power approach synthesizing various sequence model trained motion capture data performing on-line filling data lost capture                
2011,discovering binary code document learning deep generative model,describe deep generative model lowest layer represents word‐count vector document top layer represents learned binary code document top two layer generative model form undirected associative memory remaining layer form belief net directed  top‐down connection present efficient learning inference procedure type generative model show allows accurate much faster retrieval latent semantic analysis using method filter much slower method called tf‐idf achieve higher accuracy tf‐idf alone save several order magnitude retrieval time using short binary code address  perform retrieval large document set time independent size document set using one word memory describe document             
2011,modeling joint density two image variety transformation,describe generative model relationship two image model defined factored three-way boltzmann machine  hidden variable collaborate define joint correlation matrix image pair modeling joint distribution pair make possible efficiently match image according learned measure similarity apply model several face matching task  show learns represent input image using task-specific basis function matching performance superior previous similar generative model  including recent conditional model transformation also show model used plug-in matching score perform invariant classification            
2011,deep generative model application recognition,popular way use probabilistic model vision first extract descriptor small image patch object part using well-engineered feature  use statistical learning tool model dependency among feature eventual label learning probabilistic model directly raw pixel value proved much difficult typically used regularizing discriminative method work  use one best  pixel-level  generative model natural images-a gated mrf-as lowest level deep belief network  several hidden layer show resulting dbn good coping occlusion predicting expression category face image  produce feature perform comparably sift descriptor discriminating different type scene generative ability model also make easy see information captured lost level representation          
2011,using deep autoencoders content-based image retrieval,show learn many layer feature color image use feature initialize deep autoencoders use autoencoders map image short binary code using semantic hashing  -bit code used retrieve image similar query image time independent size database extremely fast retrieval make possible search using multiple different transformation query image -bit binary code allow much accurate matching used prune set image found using -bit code               
2011,transforming auto-encoders,artificial neural network used recognize shape typically use one layer learned feature detector produce scalar output contrast  computer vision community us complicated  hand-engineered feature  like sift   produce whole vector output including explicit representation pose feature show neural network used learn feature output whole vector instantiation parameter argue much promising way dealing variation position  orientation  scale lighting method currently employed neural network community also promising hand-engineered feature currently used computer vision provides efficient way adapting feature domain         
2011,deep belief network using discriminative feature phone recognition,deep belief network  multi-layer generative model trained model window coefficient extracted speech discover multiple layer feature capture higher-order statistical structure data feature used initialize hidden unit feed-forward neural network trained predict hmm state central frame window initializing feature good generating speech make neural network perform much better initializing random weight dbns already used successfully phone recognition input coefficient mfccs filterbank output paper  demonstrate work even better input speaker adaptive  discriminative feature standard timit corpus  give phone error rate  using monophone hmms bigram language model  using monophone hmms trigram language model            
2011,deep belief net natural language call-routing,paper considers application deep belief net  natural language call routing dbns successfully applied number task  including image  audio speech classification  thanks recent discovery efficient learning technique dbns learn multi-layer generative model unlabeled data feature discovered model used initialize feed-forward neural network fine-tuned backpropagation compare dbn-initialized neural network three widely used text classification algorithm support vector machine   boosting maximum entropy  dbn-based model give call-routing classification accuracy equal best model even though currently us impoverished representation input           
2011,learning better representation speech soundwaves using restricted boltzmann machine,state art speech recognition system rely preprocessed speech feature mel cepstrum linear predictive coding coefficient collapse high dimensional speech sound wave low dimensional encoding successfully applied speech recognition system  low dimensional encoding may lose relevant information express information way make difficult use discrimination higher dimensional encoding could improve performance recognition task  also applied speech synthesis better modeling statistical structure sound wave paper present novel approach modeling speech sound wave using restricted boltzmann machine  novel type hidden variable report initial result demonstrating phoneme recognition performance better current state-of-the-art method based mel cepstrum coefficient              
2011,generating text recurrent neural network,recurrent neural network  powerful sequence model enjoy widespread use extremely difficult train properly fortunately  recent advance hessian-free optimization able overcome difficulty associated training rnns  making possible apply successfully challenging seuence problem paper demonstrate power rnns trained new hessian-free optimizer  applying character-level language modelling task standard rnn architecture  effective  ideally suited task  introduce new rnn variant us multiplicative  connection allow current input character determine transition matrix one hidden state vector next training multiplicative rnn hf optimizer five day  high-end graphic processing unit  able surpass performance best previous single method character-level language modeling - hierachical non-parametric sequence model knowledge represents largest recurrent neural network application date          
2011,conditional restricted boltzmann machine structured output prediction,conditional restricted boltzmann machine  rich probabilistic model recently applied wide range problem  including collaborative filtering  classification  modeling motion capture data much progress made training non-conditional rbms  algorithm applicable conditional model almost work training generating prediction conditional rbms structured output problem first argue standard contrastive divergence-based learning may suitable training crbms identify two distinct type structured output prediction problem propose improved learning algorithm first problem type one output space arbitrary structure set likely output configuration relatively small  multi-label classification second problem one output space arbitrarily structured output space variability much greater  image denoising pixel labeling show new learning algorithm work much better contrastive divergence type problem         
2010,learning represent spatial transformation factored higher-order boltzmann machine,allow hidden unit restricted boltzmann machine model transformation two successive image  memisevic hinton  introduced three-way multiplicative interaction use intensity pixel first image multiplicative gain learned  symmetric weight pixel second image hidden unit creates cubically many parameter  form three-dimensional interaction tensor describe low-rank approximation interaction tensor us sum factor  three-way outer product approximation allows efficient learning transformation larger image patch since factor viewed image filter  model whole learns optimal filter pair efficiently representing transformation demonstrate learning optimal filter pair various synthetic real image sequence also show learning image transformation allows model perform simple visual analogy task  show completely unsupervised network trained transformation perceives multiple motion transparent dot pattern way human         
2010,comparing classification method longitudinal fmri study,compare  method classifying fmri volume applying data longitudinal study stroke recovery adaptive fisher linear quadratic discriminant gaussian naive bayes support vector machine linear  quadratic  radial basis function  kernel logistic regression two novel method based pair restricted boltzmann machine  k-nearest neighbor method tested three binary classification task  out-of-sample classification accuracy compared relative performance method varies considerably across subject classification task best overall performer adaptive quadratic discriminant  support vector machine rbf kernel  generatively trained pair rbms          
2010,temporal-kernel recurrent neural network,recurrent neural network  powerful connectionist model applied many challenging sequential problem  including problem naturally arise language speech however  rnns extremely hard train problem long-term dependency  necessary remember event many timesteps using make prediction            
2010,dynamical binary latent variable model 3d human pose tracking,introduce new class probabilistic latent variable model called implicit mixture conditional restricted boltzmann machine  use human pose tracking key property imcrbm follows  learning linear number training exemplar learned large datasets  learns coherent model multiple activity  automatically discovers atomic “movemes”  infer transition activity  even transition present training set describe model learned demonstrate use context bayesian filtering multi-view monocular pose tracking model handle difficult scenario including multiple activity transition among activity report state-of-the-art result humaneva dataset              
2010,modeling pixel mean covariance using factorized third-order boltzmann machine,learning generative model natural image useful way extracting feature capture interesting regularity previous work learning model focused method latent feature used determine mean variance pixel independently  method hidden unit determine covariance matrix zero-mean gaussian distribution work  propose probabilistic model combine two approach single framework represent image using one set binary latent feature model image-specific covariance separate set model mean show approach provides probabilistic framework widely used simple-cell complex-cell architecture  produce realistic sample natural image extract feature yield state-of-the-art recognition accuracy challenging cifar  dataset            
2010,learning detect road high-resolution aerial image,reliably extracting information aerial imagery difficult problem many practical application one specific case problem task automatically detecting road task difficult vision problem occlusion  shadow  wide variety non-road object despite  year work automatic road detection  automatic semi-automatic road detection system currently market published method shown work reliably large datasets urban imagery propose detecting road using neural network million trainable weight look much larger context used previous attempt learning task network trained massive amount data using consumer gpu demonstrate predictive performance substantially improved initializing feature detector using recently developed unsupervised learning method well taking advantage local spatial coherence output label show method work reliably two challenging urban datasets order magnitude larger used evaluate previous approach            
2010,phone recognition using restricted boltzmann machine,decade  hidden markov model  state-of-the-art technique acoustic modeling despite unrealistic independence assumption limited representational capacity hidden state conditional restricted boltzmann machine  recently proved effective modeling motion capture sequence paper investigates application powerful type generative model acoustic modeling standard timit corpus  one type crbm outperforms hmms comparable best method  achieving phone error rate   timit core test set            
2010,rectified linear unit improve restricted boltzmann machine,restricted boltzmann machine developed using binary stochastic hidden unit generalized replacing binary unit infinite number copy weight progressively negative bias learning inference rule stepped sigmoid unit unchanged approximated efficiently noisy  rectified linear unit compared binary unit  unit learn feature better object recognition norb dataset face verification labeled face wild dataset unlike binary unit  rectified linear unit preserve information relative intensity information travel multiple layer feature detector             
2010,binary coding speech spectrogram using deep auto-encoder,paper report recent exploration layer-by-layer learning strategy training multi-layer generative model patch speech spectrogram top layer generative model learns binary code used efficient compression speech could also used scalable speech recognition rapid speech content retrieval layer generative model fully connected layer weight connection pre-trained efficiently using contrastive divergence approximation log likelihood gradient layer-by-layer pre-training �unroll� generative model form deep auto-encoder  whose parameter fine-tuned using back-propagation reconstruct full-length speech spectrogram  individual spectrogram segment predicted respective binary code combined using overlap-and-add method experimental result speech spectrogram coding demonstrate binary code produce log-spectral distortion approximately  db lower sub-band vector quantization technique entire frequency range wide-band speech             
2010,phone recognition mean-covariance restricted boltzmann machine,straightforward application deep belief net  acoustic modeling produce rich distributed representation speech data useful recognition yield impressive result speaker-independent timit phone recognition task however  first-layer gaussian-bernoulli restricted boltzmann machine  important limitation  shared mixture diagonal-covariance gaussians grbms treat different component acoustic input vector conditionally independent given hidden state mean-covariance restricted boltzmann machine   first introduced modeling natural image  much representationally efficient powerful way modeling covariance structure speech data every configuration precision unit mcrbm specifies different precision matrix conditional distribution acoustic space work  use mcrbm learn feature speech data serve input standard dbn mcrbm feature combined dbns allow u achieve phone error rate   superior published result speaker-independent timit date         
2010,learning combine foveal glimpse third-order boltzmann machine,describe model based boltzmann machine third-order connection learn accumulate information shape several fixation model us retina enough high resolution pixel cover small area image  must decide sequence fixation must combine glimpse fixation location fixation integrating information information glimpse object evaluate model synthetic dataset two image classification datasets  showing perform least well model trained whole image             
2010,gated softmax classification,describe log-bilinear model computes class probability combining input vector multiplicatively vector binary latent variable even though latent variable take exponentially many possible combination value  efficiently compute exact probability class marginalizing latent variable make possible get exact gradient log likelihood bilinear score-functions defined using three-dimensional weight tensor  show factorizing tensor allows model encode invariance inherent task learning dictionary invariant basis function experiment set benchmark problem show fully probabilistic model achieve classification performance competitive  svms  backpropagation  deep belief net           
2010,generating realistic image using gated mrfs,probabilistic model natural image usually evaluated measuring performance rather indirect task  denoising inpainting direct way evaluate generative model draw sample check whether statistical property sample match statistic natural image method seldom used high-resolution image  current model produce sample different natural image  assessed even simple visual inspection investigate reason failure show augmenting existing model two set latent variable  one set modelling pixel intensity set modelling image-specific pixel covariance  able generate high-resolution image look much realistic overall model interpreted gated mrf pair-wise dependency mean intensity pixel modulated state latent variable finally  confirm disallow weight-sharing receptive field overlap  gated mrf learns efficient internal representation  demonstrated several recognition task       
2010,factored 3-way restricted boltzmann machine modeling natural image,deep belief net successful modeling handwritten character  proved difficult apply real image problem lie restricted boltzmann machine  used module learning deep belief net one layer time gaussian-binary rbms used model real-valued data good way model covariance structure natural image propose factored -way rbm us state hidden unit represent abnormality local covariance structure image provides probabilistic framework widely used simple/complex cell architecture model learns binary feature work well object recognition “tiny images” data set even better feature obtained using standard binary rbm’s learn deeper model              
2010,boltzmann machine,boltzmann machine network symmetrically connected  neuron-like unit make stochastic decision whether boltzmann machine simple learning algorithm allows discover interesting feature represent complex regularity training data learning algorithm slow network many layer feature detector  fast restricted boltzmann machine single layer feature detector many hidden layer learned efficiently composing restricted boltzmann machine  using feature activation one training data next boltzmann machine used solve two quite different computational problem search problem  weight connection fixed used represent cost function stochastic dynamic boltzmann machine allow sample binary state vector low value cost function learning problem  boltzmann machine shown set binary data vector  must learn generate vector high probability  must find weight connection relative possible binary vector  data vector low value cost function solve learning problem  boltzmann machine make many small update weight  update requires solve many different search problem      
2010,deep belief net,deep belief net probabilistic generative model composed multiple layer stochastic latent variable  top two layer undirected  symmetric connection form associative memory lower layer receive top-down  directed connection layer deep belief net two important computational property first  efficient procedure learning top-down  generative weight specify variable one layer determine probability variable layer procedure learns one layer latent variable time second  learning multiple layer  value latent variable every layer inferred single  bottom-up pas start observed data vector bottom layer us generative weight reverse direction        
2009,semantic hashing,show learn deep graphical model word-count vector obtained large set document value latent variable deepest layer easy infer give much better representation document latent semantic analysis deepest layer forced use small number binary variable   graphical model performs “semantic hashing” document mapped memory address way semantically similar document located nearby address document similar query document found simply accessing address differ bit address query document way extending efficiency hash-coding approximate matching much faster locality sensitive hashing  fastest current method using semantic hashing filter document given tf-idf  achieve higher accuracy applying tf-idf entire document set            
2009,improving statistical language model non-linear prediction,show improve state-of-the-art neural network language model convert previous “context” word feature vector combine feature vector linearly predict feature vector next word significant improvement predictive accuracy achieved using non-linear subnetwork modulate effect context word produce non-linear correction term predicting feature vector log-bilinear language model incorporates improvement achieves  reduction perplexity best n-gram model fairly large dataset               
2009,deep belief network,deep belief net probabilistic generative model composed multiple layer stochastic  latent variable latent variable typically binary value often called hidden unit feature detector top two layer undirected  symmetric connection form associative memory lower layer receive top-down  directed connection layer state unit lowest layer represent data vectorthe two significant property deep belief net efficient  layer-by-layer procedure learning top-down  generative weight determine variable one layer depend variable layer learning  value latent variable every layer inferred single  bottom-up pas start observed data vector bottom layer us generative weight reverse direction deep belief net learned one layer time treating value latent variable one layer  inferred data  data training next layer efficient  greedy learning followed  combined  learning procedure fine-tune weight improve generative discriminative performance whole network discriminative fine-tuning performed adding final layer variable represent desired output backpropagating error derivative network many hidden layer applied highly-structured input data  image  backpropagation work much better feature detector hidden layer initialized learning deep belief net model structure input data 
2009,learning generative texture model extended fields-of-experts,evaluate ability popular field-of-experts  model structure image test case focus modeling synthetic natural texture find even modeling single texture  foe provides insufficient flexibility learn good generative model - perform better much simpler gaussian foe propose extended version foe  demonstrate novel formulation  trained better approximation likelihood gradient  give rise powerful generative model specific visual structure produce significantly better result texture task             
2009,modeling pigeon behavior using conditional restricted boltzmann machine,effort better understand complex courtship behaviour pigeon  built model learned motion capture data employ conditional restricted boltzmann machine  binary latent feature real-valued visible unit unit conditioned information previous time step capture dynamic validate trained model quantifying characteristic head-bobbing present pigeon also show predict missing data marginalizing hidden variable minimizing free energy              
2009,factored conditional restricted boltzmann machine modeling motion style,conditional restricted boltzmann machine  recently proposed model time series rich  distributed hidden state permit simple  exact inference present new model  based crbm preserve important computational property includes multiplicative three-way interaction allow effective interaction weight two unit modulated dynamic state third unit factor three-way weight tensor implied multiplicative model  reducing number parameter result efficient  compact model whose effectiveness demonstrate modeling human motion like crbm  model capture diverse style motion single set parameter  three-way interaction greatly improve model ability blend motion style transition smoothly among        
2009,using fast weight improve persistent contrastive divergence,commonly used learning algorithm restricted boltzmann machine contrastive divergence start markov chain data point run chain iteration get cheap  low variance estimate sufficient statistic model tieleman  showed better learning achieved estimating model statistic using small set persistent fantasy particle reinitialized data point weight update sufficiently small weight update  fantasy particle represent equilibrium distribution accurately explain method work much larger weight update necessary consider interaction weight update markov chain show weight update force markov chain mix fast  using insight develop even faster mixing chain us auxiliary set fast weight implement temporary overlay energy landscape fast weight learn rapidly also decay rapidly contribute normal energy landscape defines model            
2009,3d object recognition deep belief net,introduce new type deep belief net evaluate object recognition task top-level model third-order boltzmann machine  trained using hybrid algorithm combine generative discriminative gradient performance evaluated norb database  contains stereo-pair image object different lighting condition viewpoint model achieves  error test set  close best published result norb  using convolutional neural net built-in knowledge translation invariance substantially outperforms shallow model svms  dbns especially suited semi-supervised learning  demonstrate consider modified version norb recognition task additional unlabeled image created applying small translation image database extra unlabeled data   model achieves  error  making current best result norb         
2009,zero-shot learning semantic output code,consider problem zero-shot learning  goal learn classifier must predict novel value omitted training set achieve  define notion semantic output code classifier  utilizes knowledge base semantic property extrapolate novel class provide formalism type classifier study theoretical property pac framework  showing condition classifier accurately predict novel class case study  build soc classifier neural decoding task show often predict word people thinking functional magnetic resonance image  neural activity  even without training example word          
2009,replicated softmax undirected topic model,show model document bag word using family two-layer  undirected graphical model member family number binary hidden unit different number softmax visible unit softmax unit model family share weight binary hidden unit describe efficient inference learning procedure family member family model probability distribution document specific length product topic-specific distribution rather mixture give much better generalization latent dirichlet allocation modeling log probability held-out document low-dimensional topic vector learned undirected family also much better lda topic vector retrieving document similar query document learned topic general found lda precision achieved intersecting many general topic rather selecting single precise topic generate word              
2009,product hidden markov model take n>1 tango,product hidden markov model interesting class generative model received little attention since introduction maybe part due computationally expensive gradient-based learning algorithm intractability computing log likelihood sequence model paper  demonstrate partition function estimated reliably via annealed importance sampling perform experiment using contrastive divergence learning rainfall data data captured pair people dancing result suggest advance learning evaluation undirected graphical model recent increase available computing power make pohmms worth considering complex time-series modeling task             
2009,deep boltzmann machine,present new learning algorithm boltzmann machine contain many layer hidden variable data-dependent expectation estimated using variational approximation tends focus single mode  data-independent expectation approximated using persistent markov chain use two quite different technique estimating two type expectation enter gradient log-likelihood make practical learn boltzmann machine multiple hidden layer million parameter learning made efficient using layer-by-layer “pre-training” phase allows variational inference initialized single bottom-up pas present result mnist norb datasets showing deep boltzmann machine learn good generative model perform well handwritten digit visual object recognition task              
2008,deep narrow sigmoid belief network universal approximators,note  show exponentially deep belief network approximate distribution binary vector arbitrary accuracy  even width layer limited dimensionality data show network greedily learned easy yet impractical way             
2008,improving statistical language model modulating effect context word,show improve state-of-the-art neural network language model convert previous context word feature vector combine feature vector predict feature vector next word significant improvement predictive accuracy achieved using higher-level feature modulate effect context word effective using higher-level feature directly predict feature vector next word  also possible combine method              
2008,analysis-by-synthesis learning invert generative black box,learning meaningful representation data  rich source prior knowledge may come form generative black box  eg graphic program generates realistic facial image consider problem learning inverse given generative model data problem non-trivial difficult create labelled training case hand  generative mapping black box sense analytic expression gradient describe way training feedforward neural network start one labelled training example us generative black box “breed” training data learning proceeds  training set evolves label network assigns unlabelled training data converge correct value demonstrate approach learning invert generative model eye active appearance model face           
2008,scalable hierarchical distributed language model,neural probabilistic language model  shown competitive occasionally superior widely-used n-gram language model main drawback nplms extremely long training testing time morin bengio proposed hierarchical language model built around binary tree word two order magnitude faster non-hierarchical language model based however  performed considerably worse non-hierarchical counterpart spite using word tree created using expert knowledge introduce fast hierarchical language model along simple feature-based algorithm automatic construction word tree data show resulting model outperform non-hierarchical model achieve state-of-the-art performance              
2008,implicit mixture restricted boltzmann machine,present mixture model whose component restricted boltzmann machine  possibility considered computing partition function rbm intractable  appears make learning mixture rbms intractable well surprisingly  formulated third-order boltzmann machine  mixture model learned tractably using contrastive divergence energy function model capture three-way interaction among visible unit  hidden unit  single hidden multinomial unit represents cluster label distinguishing feature model  unlike mixture model  mixing proportion explicitly parameterized instead  defined implicitly via energy function depend parameter model present result mnist norb datasets showing implicit mixture rbms learns cluster reflect class structure data       
2008,generative versus discriminative training rbms classification fmri image,neuroimaging datasets often large number voxels small number training case  mean overfitting model data become serious problem working set fmri image study stroke recovery  consider classification task logistic regression performs poorly  even l- l- regularized show much better discrimination achieved fitting generative model separate condition seeing model likely generated data compare discriminative training exactly set model  also consider convex blend generative discriminative training           
2008,using matrix model symbolic relationship,describe way learning matrix representation object relationship goal learning allow multiplication matrix represent symbolic relationship object symbolic relationship relationship  main novelty method demonstrate lead excellent generalization two different domain modular arithmetic family relationship show system learn first-order proposition    higher-order proposition    demonstrate system understands higher-order proposition related first-order one showing correctly answer question first-order proposition involving relation   even though trained first-order example involving relation             
2008,recurrent temporal restricted boltzmann machine,temporal restricted boltzmann machine  probabilistic model sequence able successfully model  several high dimensional sequence  motion capture data pixel low resolution video ball bouncing box major disadvantage trbm exact inference extremely hard  since even computing gibbs update single variable posterior exponentially expensive difficulty necessitated use heuristic inference procedure  nonetheless accurate enough successful learning paper introduce recurrent trbm  slight modification trbm exact inference easy exact gradient learning almost tractable demonstrate rtrbm better analogous trbm generating motion capture video bouncing ball          
2007,unsupervised learning image transformation,describe probabilistic model learning rich  distributed representation image transformation basic model defined gated conditional random field trained predict transformation input using factorial set latent variable inference model consists extracting transformation  given pair image  performed exactly efficiently show  trained natural video  model develops domain specific motion feature  form field locally transformed edge filter trained affine  general  transformation still image  model develops code transformation  subsequently perform recognition task invariant transformation also fantasize new transformation previously unseen image describe several variation basic model provide experimental result demonstrate applicability variety task     
2007,three new graphical model statistical language modelling,supremacy n-gram model statistical language modelling recently challenged parametric model use distributed representation counteract difficulty caused data sparsity propose three new probabilistic language model define distribution next word sequence given several preceding word using distributed representation word show real-valued distributed representation word learned time learning large set stochastic binary hidden feature used predict distributed representation next word previous distributed representation adding connection previous state binary hidden feature improves performance adding direct connection real-valued distributed representation one model significantly outperforms best n-gram model               
2007,restricted boltzmann machine collaborative filtering,existing approach collaborative filtering cannot handle large data set paper show class two-layer undirected graphical model  called restricted boltzmann machine   used model tabular data  user rating movie present efficient learning inference procedure class model demonstrate rbms successfully applied netflix data set  containing  million user/movie rating also show rbms slightly outperform carefully-tuned svd model prediction multiple rbm model multiple svd model linearly combined  achieve error rate well  better score netflixs system          
2007,modeling image patch directed hierarchy markov random field,describe efficient learning procedure multilayer generative model combine best aspect markov random field deep  directed belief net generative model learned one layer time learning complete fast inference procedure computing good approximation posterior distribution hidden layer hidden layer mrf whose energy function modulated top-down directed connection layer generate model  layer turn must settle equilibrium given top-down input show type model good capturing statistic patch natural image             
2007,using deep belief net learn covariance kernel gaussian process,show use unlabeled data deep belief net  learn good covariance kernel gaussian process first learn deep generative model unlabeled data using fast  greedy algorithm introduced hinton etal data high-dimensional highly-structured  gaussian kernel applied top layer feature dbn work much better similar kernel applied raw input performance regression classification improved using backpropagation dbn discriminatively fine-tune covariance kernel             
2007,visualizing similarity data mixture map,show visualize set pairwise similarity object using several different two-dimensional map  capture different aspect similarity structure object ambiguous word  example  different sens word occur different map  “river” “loan” close “bank” without close aspect map resemble clustering model pair-wise similarity mixture different type similarity  also resemble local multi-dimensional scaling model type similarity twodimensional map demonstrate method toy example  database human wordassociation data  large set image handwritten digit  set feature vector represent word       
2007,learning nonlinear embedding preserving class neighbourhood structure,show pretrain fine-tune multilayer neural network learn nonlinear transformation input space lowdimensional feature space k-nearest neighbour classification performs well also show non-linear transformation improved using unlabeled data method achieves much lower error rate support vector machine standard backpropagation widely used version mnist handwritten digit recognition task dimension low-dimensional feature space used nearest neighbor classification  method us dimension explicitly represent transformation digit affect identity              
2007,learning multilevel distributed representation high-dimensional sequence,describe new family non-linear sequence model substantially powerful hidden markov model linear dynamical system model simple approximate inference learning procedure work well practice multilevel representation sequential data learned one hidden layer time  adding extra hidden layer improves resulting generative model model trained high-dimensional  non-linear data raw pixel sequence performance demonstrated using synthetic video sequence two ball bouncing box             
2006,unsupervised discovery nonlinear structure using contrastive backpropagation,describe way modeling high‐dimensional data vector using unsupervised  nonlinear  multilayer neural network activity neuron‐like unit make additive contribution global energy score indicates surprised network data vector connection weight determine activity unit depends activity earlier layer learned minimizing energy assigned data vector actually observed maximizing energy assigned “confabulations” generated perturbing observed data vector direction decrease energy current model             
2006,topographic product model applied natural scene statistic,present energy-based model us product generalized student-t distribution capture statistical structure data set model inspired particularly applicable “natural” data set image begin providing mathematical framework  discus complete overcomplete model provide algorithm training model data using patch natural scene  demonstrate approach represents viable alternative independent component analysis interpretive model biological visual system although two approach similar flavor  also important difference  particularly representation overcomplete constraining interaction within model  also able study topographic organization gabor-like receptive field model learns finally  discus relation new approach previous work—in particular  gaussian scale mixture model variant independent component analysis        
2006,fast learning algorithm deep belief net,show use “complementary priors” eliminate explaining-away effect make inference difficult densely connected belief net many hidden layer using complementary prior  derive fast  greedy algorithm learn deep  directed belief network one layer time  provided top two layer form undirected associative memory fast  greedy algorithm used initialize slower learning procedure fine-tunes weight using contrastive version wake-sleep algorithm fine-tuning  network three hidden layer form good generative model joint distribution handwritten digit image label generative model give better digit classification best discriminative learning algorithm low-dimensional manifold digit lie modeled long ravine free-energy landscape top-level associative memory  easy explore ravine using directed connection display associative memory mind        
2006,modeling human motion using binary latent variable,propose non-linear generative model human motion data us undirected model binary latent variable real-valued visible variable represent joint angle latent visible variable time step receive directed connection visible variable last time-steps architecture make on-line inference efficient allows u use simple approximate learning procedure training  model find single set parameter simultaneously capture several different kind motion demonstrate power approach synthesizing various motion sequence performing on-line filling data lost motion capture               
2005,improving dimensionality reduction spectral gradient descent,introduce spectral gradient descent  way improving iterative dimensionality reduction technique method us information contained leading eigenvalue data affinity matrix modify step taken gradient-based optimization procedure show approach able speed optimization help dimensionality reduction method find better local minimum objective function also provide interpretation approach term power method finding leading eigenvalue symmetric matrix verify usefulness approach simple experiment              
2005,contrastive divergence learning,maximum-likelihood  learning markov random field challenging requires estimate average exponential number term markov chain monte carlo method typically take long time converge unbiased estimate  hinton showed markov chain run step  learning still work well approximately minimizes different function called contrastive divergence  cd learning successfully applied various type random field  study property cd learning show provides biased estimate general  bias typically small fast cd learning therefore used get close ml solution slow ml learning used fine-tune cd solution            
2005,learning causally linked markov random field,describe learning procedure generative model contains hidden markov random field  directed connection observable variable learning procedure us variational approximation posterior distribution hidden variable despite intractable partition function mrf  weight directed connection variational approximation learned maximizing lower bound log probability observed data parameter mrf learned using mean field version contrastive divergence show hybrid model simultaneously learns part object inter-relationships intensity image discus extension multiple mrfs linked chain graph directed connection               
2005,kind graphical model brain,neuron treated latent variable  visual system non-linear  densely-connected graphical model containing billion variable thousand billion parameter current algorithm would difficulty learning graphical model scale starting algorithm difficulty learning thousand parameter  describe series progressively better learning algorithm designed run neuron-like hardware latest member series learn deep  multi-layer belief net quite rapidly turn generic network three hidden layer  million connection good generative model handwritten digit learning  model give classification performance comparable best discriminative method           
2005,inferring motor program image handwritten digit,describe generative model handwritten digit us two pair opposing spring whose stiffness controlled motor program show neural network trained infer motor program required accurately construct mnist digit inferred motor program used directly digit classification  also used way adding noise motor program inferred mnist image generate large set different image class  thus enlarging training set available method also use motor program additional  highly informative output reduce overfitting training feed-forward classifier            
2004,reinforcement learning factored state action,novel approximation method presented approximating value function selecting good action markov decision process large state action space method approximates state-action value negative free energy undirected graphical model called product expert model parameter learned efficiently value derivative efficiently computed product expert action found even large factored action space use markov chain monte carlo sampling simulation result show product expert approximation used solve large problem one simulation used find action action space size ^               
2004,probabilistic sequential independent component analysis,under-complete model  derive lower dimensional representation input data  valuable domain number input dimension large  data consisting temporal sequence image paper present under-complete product expert   expert model one-dimensional projection data maximum-likelihood learning rule model constitute tractable exact algorithm learning under-complete independent component learning rule model coincide approximate learning rule proposed earlier under-complete independent component analysis  model paper also derives efficient sequential learning algorithm model discus relationship sequential independent component analysis   projection pursuit density estimation  feature induction algorithm additive random field model paper demonstrates efficacy novel algorithm high-dimensional continuous datasets         
2004,neighbourhood component analysis,paper propose novel method learning mahalanobis distance measure used knn classification algorithm algorithm directly maximizes stochastic variant leave-one-out knn score training set also learn low-dimensional linear embedding labeled data used data visualization fast classification unlike method  classification model non-parametric  making assumption shape class distribution boundary performance method demonstrated several data set  metric learning linear dimensionality reduction            
2004,multiple relational embedding,describe way using multiple different type similarity relationship learn low-dimensional embedding dataset method chooses different  possibly overlapping representation similarity individually reweighting dimension common underlying latent space applied single similarity relation based euclidean distance input data point  method reduces simple dimensionality reduction additional information available dataset subset  use information clean otherwise improve embedding demonstrate potential usefulness form semi-supervised dimensionality reduction simple example            
2004,exponential family harmonium application information retrieval,directed graphical model one layer observed random variable one layer hidden random variable dominant modelling paradigm many research field although approach met considerable success  causal semantics model make difficult infer posterior distribution ove hidden variable paper propose alternative two-layer model based exponential family distribution semantics undirected model inference exponential family harmonium fast learning performed minimizing contrastive divergence member family studied alternative probabilistic model latent semantic indexing experiment shown perform well document retrieval task provide elegant solution searching keywords              
2003,energy-based model sparse overcomplete representation,present new way extending independent component analysis  overcomplete representation contrast causal generative extension ica maintain marginal independence source  define feature deterministic  function input assumption result marginal dependency among feature  conditional independence feature given input assigning energy feature probability distribution input state defined boltzmann distribution free parameter model trained using contrastive divergence objective  number feature equal number input dimension energy-based model reduces noiseless ica show experimentally proposed learning algorithm able perform blind source separation speech data additional experiment train overcomplete energy-based model extract feature various standard data-sets containing speech  natural image  hand-written digit face          
2003,wormhole improve contrastive divergence,model define probability via energy  maximum likelihood learning typically involves using markov chain monte carlo sample model distribution markov chain started data distribution  learning often work well even chain run time step data distribution contains mode separated region low density  brief mcmc ensure different mode correct relative energy cannot move particle one mode another show improve brief mcmc allowing long-range move suggested data distribution model approximately correct  long-range move reasonable acceptance rate           
2003,efficient parametric projection pursuit density estimation,product model low dimensional expert powerful way avoid curse dimensionality present ``under-complete product expert   expert model one dimensional projection data upoe fully tractable may interpreted parametric probabilistic model projection pursuit ml learning rule identical approximate learning rule proposed under-complete ica also derive efficient sequential learning algorithm discus relationship projection pursuit density estimation feature induction algorithm additive random field model              
2002,local physical model interactive character animation,goal design build tool creation expressive character animation virtual puppetry  also known performance animation  technique user interactively control character motion paper introduce local physical model performance animation describe augment existing kinematic method achieve effective animation control model approximate specific physically‐generated aspect character motion automate certain behaviour  still letting user override motion via pd‐controller desire furthermore  tuned ignore certain undesirable effect  risk character fall  ignoring corresponding component force although local physical model quite simple approximation real physical behaviour  show extremely useful interactive character control  contribute positively expressiveness character motion paper  develop model knee ankle interactively‐animated anthropomorphic character  demonstrate resulting animation approach applied straight‐forward way joint     
2002,training product expert minimizing contrastive divergence,possible combine multiple latent-variable model data multiplying probability distribution together renormalizing way combining individual “expert” model make hard generate sample combined model easy infer value latent variable expert  combination rule ensures latent variable different expert conditionally independent given data product expert  therefore interesting candidate perceptual system rapid inference vital generation unnecessary training poe maximizing likelihood data difficult hard even approximate derivative renormalization term combination rule fortunately  poe trained using different objective function called “contrastive divergence” whose derivative regard parameter approximated accurately efficiently example presented contrastive divergence learning using several type expert several type data             
2002,recognizing handwritten digit using hierarchical product expert,product expert learning procedure discover set stochastic binary feature constitute nonlinear generative model handwritten image digit quality generative model learned way assessed learning separate model class digit comparing unnormalized probability test image  different class-specific model improve discriminative performance  hierarchy separate model learned  digit class model hierarchy learns layer binary feature detector model probability distribution vector activity feature detector layer model hierarchy trained sequentially model us layer binary feature detector learn generative model pattern feature activity preceding layer training  layer feature detector produce separate  unnormalized log probability score three layer feature detector  digit class  test image produce  score used input supervised  logistic classification network trained separate data         
2002,desktop input device interface interactive 3d character animation,present novel input device interface interactively controlling animation graphical human character desktop environment tracker embedded new physical design  simple yet also provides significant benefit  establishes tangible interface coordinate frame inherent character layered kinematic motion recording strategy access subset total degree freedom character present experience three novice user system  long-term user prior experience complex continuous interface            
2002,new learning algorithm mean field boltzmann machine,present new learning algorithm mean field boltzmann machine based contrastive divergence optimization criterion addition minimizing divergence data distribution equilibrium distribution  maximize divergence one-step reconstruction data equilibrium distribution eliminates need estimate equilibrium statistic  need approximate multimodal probability distribution free network unimodal mean field distribution test learning algorithm classification digit             
2002,self supervised boosting,boosting algorithm successful application thereof abound classification regression learning problem  unsupervised learning propose sequential approach adding feature random field model training improve classification performance data equal-sized sample negative example generated model current estimate data density training boosting round proceeds three stage first sample negative example model current boltzmann distribution next  feature trained improve classification performance data negative example finally  coefficient learned determines importance feature relative one already pool negative example need generated learn new feature validity approach demonstrated binary digit continuous synthetic data            
2002,stochastic neighbor embedding,describe probabilistic approach task placing object  described high-dimensional vector pairwise dissimilarity  low-dimensional space way preserve neighbour identity  gaussian centered object high-dimensional space density gaussian  used define probability distribution potential neighbor object aim embedding approximate distribution well possible operation performed low-dimensional “images” object  natural cost function sum kullback-leibler divergence  one per object  lead simple gradient adjusting position low-dimensional image unlike dimensionality reduction method  probabilistic framework make easy represent object mixture widely separated low-dimensional image allows ambiguous object  like document count vector word bank  version close image river finance without forcing image outdoor concept located close corporate concept        
2002,learning sparse topographic representation product student-t distribution,propose model natural image probability image proportional product probability filter output  encourage system find sparse feature using student-t distribution model filter output t-distribution used model combined output set neurally adjacent filter  system learns topographic map orientation  spatial frequency location filter change smoothly across map even though maximum likelihood learning intractable model  product form allows relatively efficient learning procedure work well even highly overcomplete set filter model learned used prior derive iterated wiener filter purpose denoising image            
2001,learning distributed representation concept using linear relational embedding,introduce linear relational embedding mean learning distributed representation concept data consisting binary relation concept key idea represent concept vector  binary relation matrix  operation applying relation concept matrix-vector multiplication produce approximation related concept representation concept relation learned maximizing appropriate discriminative goodness function using gradient ascent task involving family relationship  learning fast lead good generalization            
2001,product hidden markov model,present product hidden markov model   way combining hmms form distributed state time series model  inference pohmm tractable efficient  learning parameter  although intractable  effectively done using product expert learning rule  distributed state help model explain data multiple cause  fact model need explain part data mean pohmm capture longer range structure hmm capable  show result modelling character string  simple language task symbolic family tree problem  highlight advantage         
2001,learning hierarchical structure linear relational embedding,present linear relational embedding   new method learning distributed representation concept data consisting instance relation given concept final goal able generalize  ie infer new instance relation among concept task involving family relationship show lre generalize better previously published method show lre used effectively find compact distributed representation variable-sized recursive data structure  tree list            
2001,global coordination local linear model,high dimensional data lie near low dimensional manifold described collection local linear model description  however  provide global parameterization manifold—arguably important goal unsupervised learning paper  show learn collection local linear model solves difficult problem local linear model represented mixture factor analyzer  global coordination model achieved adding regularizing term standard maximum likelihood objective function regularizer break degeneracy mixture model parameter space  favoring model whose internal coordinate system aligned consistent way result  internal coordinate change smoothly continuously one traverse connected path manifold—even path cross domain many different local model regularizer take form kullback-leibler divergence illustrates unexpected application variational method perform approximate inference intractable probabilistic model  learn useful internal representation tractable one        
2001,relative density net new way combine backpropagation hmms,logistic unit first hidden layer feedforward neural network compute relative probability data point two gaussians lead u consider substituting density model present architecture performing discriminative learning hidden markov model using network many small hmms experiment speech data show superior standard method discriminatively training hmms               
2001,discovering multiple constraint frequently approximately satisfied,high-dimensional datasets modelled assuming many different linear constraint  frequently approximately satisfied  data probability data vector model proportional product probability constraint violation describe three method learning product constraint using heavy-tailed probability distribution violation              
2000,variational learning switching state-space model,introduce new statistical model time series iteratively segment data regime approximately linear dynamic learns parameter linear regime model combine generalizes two widely used stochastic time-series models—hidden markov model linear dynamical systems—and closely related model widely used control econometrics literature also derived extending mixture expert neural network  fully dynamical version  expert gating network recurrent inferring posterior probability hidden state model computationally intractable  therefore exact expectation maximization  algorithm cannot applied however  present variational approximation maximizes lower bound log-likelihood make use forward backward recursion hidden markov model kalman filter recursion linear dynamical system tested algorithm artificial data set natural data set respiration force patient sleep apnea result suggest variational approximation viable method inference learning switching state-space model        
2000,smem algorithm mixture model,present split-and-merge expectation-maximization  algorithm overcome local maximum problem parameter estimation finite mixture model case mixture model  local maximum often involve many component mixture model one part space another  widely separated part space escape configuration  repeatedly perform simultaneous split-and-merge operation using new criterion efficiently selecting split-and-merge candidate apply proposed algorithm training gaussian mixture mixture factor analyzer using synthetic real data show effectiveness using split- and-merge operation improve likelihood training data held-out test data also show practical usefulness proposed algorithm applying image compression pattern recognition problem            
2000,split merge em algorithm improving gaussian mixture density estimate,em algorithm gaussian mixture model often get caught local maximum likelihood involve many gaussians one part space another  widely separated part space present new em algorithm performs split merge operation gaussians escape configuration algorithm us two novel criterion efficiently selecting split merge candidate experimental result synthetic real data show effectiveness using split merge operation improve likelihood training data held-out test data              
2000,modeling high-dimensional data combining simple expert,possible combine multiple non-linear probabilistic model data multiplying probability distribution together renormalizing product expert efficient way model data simultaneously satisfies many different constraint difficult fit product expert data using maximum likelihood gradient log likelihood intractable  efficient way optimizing different objective function produce good model high-dimensional data              
2000,learning distributed representation mapping concept relation linear space,linear relational embedding method learning distributed representation concept data consisting binary relation concept concept represented vector  binary relation matrix  operation applying relation concept matrix-vector multiplication produce approximation related concept representation concept relation learned maximizing appropriate discriminative goodness function using gradient ascent task involving family relationship  learning fast lead good generalization            
2000,extracting distributed representation concept relation positive negative proposition,linear relational embedding  introduced previously author  mean extracting distributed representation concept relational data original formulation cannot use negative information cannot properly handle data multiple correct answer paper propose extended formulation lre solves problem present result two simple domain  show learning lead good generalization              
2000,rate-coded restricted boltzmann machine face recognition,describe neurally-inspired  unsupervised learning algorithm build non-linear generative model pair face image individual individual recognized finding highest relative probability pair among pair consist test image image whose identity known method compare favorably method literature generative model consists single layer rate-coded  non-linear feature detector property  given data vector  true posterior probability distribution feature detector activity inferred rapidly without iteration approximation weight feature detector learned comparing correlation pixel intensity feature activation two phase network observing real data observing reconstruction real data generated feature activation           
2000,recognizing hand-written digit using hierarchical product expert,product expert learning procedure  discover set stochastic binary feature constitute non-linear generative model handwritten image digit quality generative model learned way assessed learning separate model class digit comparing unnormalized probability test image  different class-specific model improve discriminative performance  helpful learn hierarchy separate model digit class model hierarchy one layer hidden unit nth level model trained data consists activity hidden unit already trained th level model training  level produce separate  unnormalized log probabilty score three-level hierarchy  digit class  test image produce  score used input supervised  logistic classification network trained separate data mnist database  system comparable current state-of-the-art discriminative method  demonstrating product expert learning procedure produce effective generative model high-dimensional data        
2000,using free energy represent q-values multiagent reinforcement learning task,problem reinforcement learning large factored markov decision process explored q-value state-action pair approximated free energy product expert network network parameter learned on-line using modified sarsa algorithm minimizes inconsistency q-values consecutive state-action pair action chosen based current value estimate fixing current state sampling action network using gibbs sampling algorithm tested co-operative multi-agent task product expert model found perform comparably table-based q-learning small instance task  continues perform well problem becomes large table-based representation              
1999,variational learning nonlinear gaussian belief network,view perceptual task vision speech recognition inference problem goal estimate posterior distribution latent variable  given sensory input recent flurry research independent component analysis exemplifies importance inferring continuous-valued latent variable input data latent variable found method linearly related input  perception requires nonlinear inference classification depth estimation article  present unifying framework stochastic neural network nonlinear latent variable nonlinear unit obtained passing output linear gaussian unit various nonlinearities present general variational method maximizes lower bound likelihood training set give result two visual feature extraction problem also show variational method used pattern classification compare performance nonlinear network method problem handwritten digit recognition            
1999,spiking boltzmann machine,first show represent sharp posterior probability distribution using real valued coefficient broadly-tuned basis function show precise time spike used convey real-valued coefficient basis function quickly accurately finally describe simple simulation spiking neuron learn model image sequence fitting dynamic generative model                
1999,learning parse image,describe class probabilistic model call credibility network using parse tree internal representation image  credibility network able perform segmentation recognition simultaneously  removing need ad hoc segmentation heuristic promising result problem segmenting handwritten digit obtained             
1998,coaching variable regression classification,regression classification setting wish predict x x   xp  suppose additional set ‘coaching’ variable z z   zm available training sample might variable difficult measure  available predict x x   xp future consider two method making use coaching variable order improve prediction x x   xp relative merit approach discussed compared number example 
1998,glove-talkii-a neural-network interface map gesture parallel formant speech synthesizer control,glove-talkii system translates hand gesture speech adaptive interface hand gesture mapped continuously ten control parameter parallel formant speech synthesizer mapping allows hand act artificial vocal tract produce speech real time give unlimited vocabulary addition direct control fundamental frequency volume currently  best version glove-talkii us several input device   parallel formant speech synthesizer  three neural network gesture-to-speech task divided vowel consonant production using gating network weight output vowel consonant neural network gating network consonant network trained example user vowel network implement fixed user-defined relationship hand position vowel sound require training example user volume  fundamental frequency  stop consonant produced fixed mapping input device one subject trained speak intelligibly glove-talkii speaks slowly far natural sounding pitch variation text-to-speech synthesizer       
1998,smem algorithm mixture model,present split-and-merge expectation-maximization  algorithm overcome local maximum problem parameter estimation finite mixture model case mixture model  local maximum often involve many component mixture model one part space another  widely separated part space escape configuration  repeatedly perform simultaneous split-and-merge operation using new criterion efficiently selecting split-and-merge candidate apply proposed algorithm training gaussian mixture mixture factor analyzer using synthetic real data show effectiveness using split- and-merge operation improve likelihood training data held-out test data also show practical usefulness proposed algorithm applying image compression pattern recognition problem            
1998,fast neural network emulation dynamical system computer animation,computer animation numerical simulation physics-based graphic model offer unsurpassed realism  computationally demanding paper demonstrates possibility replacing numerical simulation nontrivial dynamic model dramatically efficient neuroanimator exploit neural network  neuroanimators automatically trained off-line emulate physical dynamic observation physics-based model action depending model  neural network emulator yield physically realistic animation one two order magnitude faster conventional numerical simulation demonstrate neuroanimators variety physics-based model             
1998,neuroanimator fast neural network emulation control physics-based model,animation numerical simulation physicsbased graphic model offer unsurpassed realism  computationally demanding likewise  search controller enable physics-based model produce desired animation usually entail formidable computational cost paper demonstrates possibility replacing numerical simulation control dynamic model dramatically efficient alternative particular  propose neuroanimator  novel approach creating physically realistic animation exploit neural network neuroanimators automatically trained off-line emulate physical dynamic observation physicsbased model action depending model  neural network emulator yield physically realistic animation one two order magnitude faster conventional numerical simulation furthermore  exploiting network structure neuroanimator  introduce fast algorithm learning controller enables either physics-based model neural network emulator synthesize motion satisfying prescribed animation goal demonstrate neuroanimators variety physics-based model        
1998,view em algorithm justifies incremental sparse variant,em algorithm performs maximum likelihood estimation data variable unobserved present function resembles negative free energy show step maximizes function respect model parameter e step maximizes respect distribution unobserved variable perspective  easy justify incremental variant em algorithm distribution one unobserved variable recalculated e step variant shown empirically give faster convergence mixture estimation problem variant algorithm exploit sparse conditional distribution also described  wide range variant algorithm also seen possible             
1998,hierarchical community expert,describe directed acyclic graphical model contains hierarchy linear unit mechanism dynamically selecting appropriate subset unit model observation non-linear selection mechanism hierarchy binary unit gate output one linear unit connection linear unit binary unit  generative model viewed logistic belief net  selects skeleton linear model among available linear unit show gibbs sampling used learn parameter linear binary unit even sampling brief markov chain far equilibrium              
1997,efficient stochastic source coding application bayesian network source model,paper  introduce new algorithm calledbits-back coding make stochastic source code efficient given one-to-many source code  show algorithm actually efcient algorithm always pick shortest codeword optimal efciency achieved codewords chosen according boltzmann distribution based codeword length turn commonly used technique determining parameters— maximum-likelihood estimation—actually minimizes bits-back coding cost codewords chosen according boltzmann distribution tractable approximation maximum-likelihood estimation—the generalized expectation-maximization algorithm—minimizes bits-back coding cost presenting binary bayesian network model assigns exponentially many codewords symbol  show tractable approximation boltzmann distribution used bits-back coding illustrate performance bits-back coding using non-synthetic data binary bayesian network source model produce   possible codewords input symbol rate bits-back coding nearly one half obtained picking shortest            
1997,instantiating deformable model neural net,deformable model attractive approach recognizing object considerable within-class variability handwritten character however  severe search problem associated fitting model data could reduced better starting point search available show training neural network predict deformable model instantiated input image  improved starting point obtained method implemented system recognizes handwritten digit using deformable model  result show search time significantly reduced without compromising recognition performance            
1997,using expectation-maximization reinforcement learning,discus hintons  relative payoff procedure   static reinforcement learning algorithm whose foundation stochastic gradient ascent show circumstance applying rpp guaranteed increase mean return  even though make large change value parameter proof based mapping rpp form expectation-maximization procedure dempster  laird  rubin            
1997,mobile robot learns place,show neural network used allow mobile robot derive accurate estimate location noisy sonar sensor noisy motion information robot model location form probability distribution across grid possible location distribution updated using motion information prediction neural network map location likelihood distribution across possible sonar reading predicting sonar reading location  rather vice versa  robot handle nongaussian noise sonar sensor using constraint provided noisy motion information  robot use previous reading improve estimate current location treating resulting estimate correct  robot learn relationship location sonar reading without requiring external supervision signal specifies actual location robot learn locate new environment almost supervision  maintain location ability even environment nonstationary          
1997,modeling manifold image handwritten digit,paper describes two new method modeling manifold digitized image handwritten digit model allow priori information structure manifold combined empirical data accurate modeling manifold allows digit discriminated using relative probability density alternative model one method grounded principal component analysis  factor analysis method based locally linear low-dimensional approximation underlying data manifold link method model manifold discussed              
1997,glove-talk ii - neural-network interface map gesture parallel formant speech synthesizer control,glove-talk ii system translates hand gesture speech adaptive interface hand gesture mapped continuously ten control parameter parallel formant speech synthesizer mapping allows hand act artificial vocal tract produce speech real time give unlimited vocabulary addition direct control fundamental frequency volume currently  best version glove-talk ii us several input device  parallel formant speech synthesizer  three neural network gesture-to-speech task divided vowel consonant production using gating network weight output vowel consonant neural network gating network consonant network trained example user vowel network implement fixed user-defined relationship hand position vowel sound require training example user volume  fundamental frequency  stop consonant produced fixed mapping input device glove-talk ii  subject speak slowly far natural sounding pitch variation text-to-speech synthesizer         
1997,hierarchical non-linear factor analysis topographic map,first describe hierarchical  generative model viewed non-linear generalisation factor analysis implemented neural network model performs perceptual inference probabilistically consistent manner using top-down  bottom-up lateral connection connection learned using simple rule require locally available information show incorporate lateral connection generative model model extract sparse  distributed  hierarchical representation depth simplified random-dot stereograms localised disparity detector first hidden layer form topographic map presented image patch natural scene  model develops topographically organised local feature detector          
1997,learning fast neural network emulator physics-based model,n/a
1996,variety helmholtz machine,helmholtz machine new unsupervised learning architecture us top-down connection build probability density model input bottom-up connection build inverse model wake-sleep learning algorithm machine involves purely local delta rule paper suggests number different variety helmholtz machine  strength weakness  relates cortical information processing              
1996,using generative model handwritten digit recognition,describe method recognizing handwritten digit fitting generative model built deformable b-splines gaussian ink generator spaced along length spline spline adjusted using novel elastic matching procedure based expectation maximization algorithm maximizes likelihood model generating data approach many advantage  system produce classification digit also rich description instantiation parameter yield information writing style  generative model perform recognition driven segmentation  method involves relatively small number parameter hence training relatively easy fast  unlike many recognition scheme  rely form pre-normalization input image  handle arbitrary scaling  translation limited degree image rotation demonstrated method fitting model image get trapped poor local minimum main disadvantage method requires much computation standard ocr technique            
1996,free energy coding,introduce new approach problem optimal compression source code produce multiple codewords given symbol may seem sensible codeword use case shortest one however  proposed free energy approach  random codeword selection yield effective codeword length le shortest codeword length random choice boltzmann distributed  effective length optimal given source code expectation-maximization parameter estimation algorithm minimize effective codeword length illustrate performance free energy coding simple problem compression factor two gained using new method            
1995,learning population code minimizing description length,minimum description length  principle used train hidden unit neural network extract representation cheap describe nonetheless allows input reconstructed accurately show mdl used develop highly redundant population code hidden unit location low-dimensional implicit space hidden unit activity form bump standard shape space  cheaply encoded center bump weight input unit hidden unit autoencoder trained make activity form standard bump coordinate hidden unit implicit space also learned  thus allowing flexibility  network develops discontinuous topography presented different input class            
1995,helmholtz machine,discovering structure inherent set pattern fundamental aim statistical inference learning one fruitful approach build parameterized stochastic generative model  independent draw likely produce pattern simplest generative model  pattern generated exponentially many way thus intractable adjust parameter maximize probability observed pattern describe way finessing combinatorial explosion maximizing easily computed lower bound probability observation method viewed form hierarchical self-supervised learning may relate function bottom-up top-down cortical processing pathway             
1995,glovetalkii adaptive gesture-to-formant interface,glove-taikii system translates hand gestures-· speech adaptive interface hand gesture mapped continuously  control parameter parallel formant speech synthesizer mapping allows hand act artificial vocal tract produce speech real time give unlimited vocabulary  multiple language addition direct control fundamental frequency volume currently  best version glove-taikii us several input device   parallel formant speech synthesizer  neural network gestureto-speech task divided vowel consonant production using gating network weight output vowel consonant neural network gating network consonant network trained example user vowel network implement fixed  user-defined relationship hand-position vowel sound require training example user volume  fundamental frequency stop consonant produced fixed mapping input device one subject trained  hour speak intelligibly glove-talkii passed eight distinct stage learning speak speaks slowly speech quality similar text-to-speech synthesizer far natural-sounding pitch variation       
1995,using pair data-points define split decision tree,conventional binary classification tree cart either split data using axis-aligned hyperplanes perform computationally expensive search continuous space hyperplanes unrestricted orientation show limitation former overcome without resorting latter every pair training data-points  one hyperplane orthogonal line joining data-points bisects line hyperplanes plausible candidate split comparison suite  datasets found method generating candidate split outperformed standard method  particularly training set small             
1995,wake-sleep algorithm produce good density estimator,wake-sleep algorithm  relatively efficient method fitting multilayer stochastic generative model high-dimensional data addition top-down connection generative model  make use bottom-up connection approximating probability distribution hidden unit given data  train bottom-up connection using simple delta rule use variety synthetic real data set compare performance wake-sleep algorithm monte carlo mean field method fitting generative model also compare model le powerful easier fit           
1994,alternative model mixture expert,propose alternative model mixture expert us different parametric form gating network modified model trained em algorithm comparison earlier models--trained either em gradient ascent--there need select learning stepsize report simulation experiment show new architecture yield faster convergence also apply new model two problem domain piecewise nonlinear function approximation combination multiple previously trained classifier               
1994,glove-talkii mapping hand gesture speech using neural network,glove-talkii system translates hand gesture speech adaptive interface hand gesture mapped continuously  control parameter parallel formant speech synthesizer mapping allows hand act artificial vocal tract produce speech real time give unlimited vocabulary addition direct control fundamental frequency volume currently  best version glove-talkii us several input device   parallel formant speech synthesizer  neural network gesture-to-speech task divided vowel consonant production using gating network weight output vowel consonant neural network gating network consonant network trained example user vowel network implement fixed  user-defined relationship hand-position vowel sound require training example user volume  fundamental frequency stop consonant produced fixed mapping input device one subject trained speak intelligibly glove-talkii speaks slowly speech quality similar text-to-speech synthesizer far natural-sounding pitch variation        
1994,using neural net instantiate deformable model,deformable model attractive approach recognizing nonrigid object considerable within class variability however  severe search problem associated fitting model data show using neural network provide better starting point  search time significantly reduced method demonstrated character recognition task              
1994,recognizing handwritten digit using mixture linear model,construct mixture locally linear generative model collection pixel-based image digit  use recognition different model given digit used capture different style writing  new image classified evaluating log-likelihoods model use em-based algorithm m-step computationally straightforward principal component analysis  incorporating tangent-plane information  expected local deformation requires adding tangent vector sample covariance matrix pca  demonstrably improves performance             
1993,learning mixture model spatial coherence,previously described unsupervised learning procedure discovers spatially coherent property world maximizing information parameter extracted different part sensory input convey common underlying cause given random dot stereograms curved surface  procedure learns extract surface depth property coherent across space also learns interpolate depth one location depth nearby location  paper  propose two new model handle surface discontinuity first model attempt detect case discontinuity reject second model develops mixture expert interpolators learns detect location discontinuity invoke specialized  asymmetric interpolators cross discontinuity            
1993,soft decision-directed lm algorithm blind equalization,adaptation algorithm equalizer operating distorted channel presented algorithm based idea adjusting equalizer tap gain maximize likelihood equalizer output would generated mixture two gaussians known mean decision-directed least-mean-square algorithm shown approximation maximizing likelihood equalizer output come independently identically distributed source algorithm developed context binary pulse-amplitude-modulation channel  simulation demonstrate algorithm converges channel decision-directed lm algorithm converge              
1993,glove-talk neural network interface data-glove speech synthesizer,illustrate potential multilayer neural network adaptive interface  vpl data-glove connected dectalk speech synthesizer via five neural network used implement hand-gesture speech system using minor variation standard backpropagation learning procedure  complex mapping hand movement speech learned using data obtained single speaker simple training phase  gesture-to-word vocabulary  wrong word produced le  time  word produced  time adaptive control speaking rate word stress also available training time final performance speed improved using small  separate network naturally defined subtask system demonstrates neural network used develop complex mapping required high bandwidth interface adapts individual user          
1993,keeping neural network simple minimizing description length weight,supervised neural network generalize well much le information weight output vector training case learning  important keep weight simple penalizing amount information contain amount information weight controlled adding gaussian noise noise level adapted learning optimize trade-o expected squared error network amount information weight describe method computing derivative expected squared error amount information noisy weight network contains layer non-linear hidden unit provided output unit linear  exact derivative computed e ciently without time-consuming monte carlo simulation idea minimizing amount information required communicate weight neural network lead number interesting scheme encoding weight             
1993,autoencoders minimum description length helmholtz free energy,autoencoder network us set recognition weight convert input vector code vector us set generative weight convert code vector approximate reconstruction input vector derive objective function training autoencoders based minimum description length  principle aim minimize information required describe code vector reconstruction error show information minimized choosing code vector stochastically according boltzmann distribution  generative weight define energy possible code vector given input vector unfortunately  code vector use distributed representation  exponentially expensive compute boltzmann distribution involves possible code vector show recognition weight autoencoder used compute approximation boltzmann distribution approximation give upper bound description length even bound poor  used lyapunov function learning generative recognition weight demonstrate approach used learn factorial code           
1993,developing population code minimizing description length,minimum description length principle  used train hidden unit neural network extract representation cheap describe nonetheless allows input reconstructed accurately show mdl used develop highly redundant population code hidden unit location lowdimensional implicit space hidden unit activity form bump standard shape space  cheaply encoded center bump weight input unit hidden unit self-supervised network trained make activity form standard bump coordinate hidden unit implicit space also learned  thus allowing exibility  network develops discontinuous topography presented di erent input class population-coding space input enables network extract nonlinear higher-order property input existing unsupervised learning algorithm understood using minimum description length  principle  given ensemble input vector  aim learning algorithm nd method coding input vector minimizes total cost  bit  communicating input vector receiver three term total description length code-cost number bit required communicate code algorithm assigns input vector model-cost number bit required specify reconstruct input vector code        
1992,simplifying neural network soft weight-sharing,one way simplifying neural network generalize better add extra term error function penalize complexity simple version approach include penalizing sum square weight penalizing number nonzero weight propose complicated penalty term distribution weight value modeled mixture multiple gaussians set weight simple weight high probability density mixture model achieved clustering weight subset weight cluster similar value since know appropriate mean variance cluster advance  allow parameter mixture model adapt time network learns simulation two different problem demonstrate complexity term effective previous complexity term              
1992,feudal reinforcement learning,one way speed reinforcement learning enable learning happen simultaneously multiple resolution space time paper show create q-learning managerial hierarchy high level manager learn set task submanagers  turn  learn satisfy submanagers need initially understand manager command simply learn maximise reinforcement context current command illustrate system using simple maze task system learns get around satisfying command multiple level explores efficiently standard flat q-learning build comprehensive map
1991,adaptive mixture local expert,present new supervised learning procedure system composed many separate network  learns handle subset complete set training case new procedure viewed either modular version multilayer supervised network  associative version competitive learning therefore provides new link two apparently different approach demonstrate learning procedure divide vowel discrimination task appropriate subtasks  solved simple expert network            
1991,learning make coherent prediction domain discontinuity,previously described unsupervised learning procedure discovers spatially coherent property world maximizing information parameter extracted different part sensory input convey common underlying cause given random dot stereograms curved surface  procedure learns extract surface depth property coherent across space also learns interpolate depth one location depth nearby location  paper  propose two new model handle surface discontinuity first model attempt detect case discontinuity reject second model develops mixture expert interpolators learns detect location discontinuity invoke specialized  asymmetric interpolators cross discontinuity           
1991,adaptive elastic model hand-printed character recognition,hand-printed digit modeled spline governed  control point known digit  control point preferred home location  deformation digit generated moving control point away home location image digit produced placing gaussian ink generator uniformly along spline real image recognized finding digit model likely generated data digit model use elastic matching algorithm minimize energy function includes deformation energy digit model log probability model would generate inked pixel image model lowest total energy win uniform noise process included model image generation  inked pixel rejected noise digit model fitting poorly segmented image digit model learn modifying home location control point            
1991,adaptive soft weight tying using gaussian mixture,one way simplifying neural network generalize better add extra term error function penalize complexity propose new penalty term distribution weight value modelled mixture multiple gaussians model  set weight simple weight clustered subset weight cluster similar value allow parameter mixture model adapt time network learns simulation demonstrate complexity term effective previous complexity term               
1990,mapping part-whole hierarchy connectionist network,three different way mapping part-whole hierarchy connectionist network described simplest scheme us fixed mapping inadequate task fails share unit connection different piece part-whole hierarchy two alternative scheme described  involves different method time-sharing connection unit scheme finally arrive suggests neural network two quite different method performing inference simple “intuitive” inference performed single settling network without changing way world mapped network complex “rational” inference involve sequence settling mapping change settling              
1990,bootstrap widrow-hoff rule cluster-formation algorithm,algorithm widely used adaptive equalization current modem “bootstrap” “decision-directed” version widrow-hoff rule show algorithm viewed unsupervised clustering algorithm data point transformed form two cluster tight possible standard algorithm performs gradient ascent crude model log likelihood generating transformed data point two gaussian distribution fixed center better convergence achieved using exact gradient log likelihood               
1990,time-delay neural network architecture isolated word recognition,translation-invariant back-propagation network described performs better sophisticated continuous acoustic parameter hidden markov model noisy  -speaker confusable vocabulary isolated word recognition task network replicated architecture permit extract precise information unaligned training pattern selected naive segmentation rule              
1990,building adaptive interface neural network glove-talk pilot study,n/a
1990,discovering viewpoint-invariant relationship characterize object,using unsupervised learning procedure  network trained ensemble image two-dimensional object different position  orientation size half network see one fragment object  try produce output set  parameter high mutual information  parameter output half network given ensemble training pattern   parameter two half network agree position  orientation  size whole object  recoding training  network reject instance shape using fact prediction made two half disagree two competing network trained unlabelled mixture image two object  cluster training case basis object shape  independently position  orientation  size   
1990,evaluation adaptive mixture competing expert,compare performance modular architecture  composed competing expert network  suggested jacob  jordan  nowlan hinton  performance single back-propagation network complex  low-dimensional  vowel recognition task simulation reveal system capable uncovering interesting decomposition complex task type decomposition strongly influenced nature input gating network decides expert use case modular architecture also exhibit consistently better generalization many variation task         
1989,connectionist learning procedure,major goal research network neuron-like processing unit discover efficient learning procedure allow network construct complex internal representation environment learning procedure must capable modifying connection strength way internal unit part input output come represent important feature task domain several interesting gradient-descent procedure recently discovered connection computes derivative  respect connection strength  global measure error performance network strength adjusted direction decrease error relatively simple  gradient-descent learning procedure work well small task new challenge find way improving convergence rate generalization ability applied larger  realistic task           
1989,deterministic boltzmann learning performs steepest descent weight-space,boltzmann machine learning procedure successfully applied deterministic network analog unit use mean field approximation efficiently simulate truly stochastic system  type “deterministic boltzmann machine”  learns much faster equivalent “stochastic boltzmann machine”   since learning procedure dbms based analogy sbms  existing proof performs gradient descent function  justified simulation using appropriate interpretation way dbm represents probability output vector given input vector  shown dbm performs steepest descent function original sbm  except rare discontinuity simple way forcing weight become symmetrical also described  make dbm biologically plausible back-propagation          
1989,phoneme recognition using time-delay neural network,author present time-delay neural network  approach phoneme recognition characterized two important property  using three-layer arrangement simple computing unit  hierarchy constructed allows formation arbitrary nonlinear decision surface  tdnn learns automatically using error backpropagation  time-delay arrangement enables network discover acoustic-phonetic feature temporal relationship independently position time therefore blurred temporal shift input recognition task  speaker-dependent recognition phoneme b   g varying phonetic context chosen comparison  several discrete hidden markov model  trained perform task performance evaluation  testing token three speaker showed tdnn achieves recognition rate  correct rate obtained best hmms          
1989,dimensionality reduction prior knowledge e-set recognition,well known automatic learning algorithm applied fixed corpus data  size corpus place upper bound number degree freedom model contain generalize well amount hardware neural network typically increase dimensionality input  challenging build high-performance network classifying large input pattern paper  several technique addressing problem discussed context isolated word recognition task            
1989,traffic recognizing object using hierarchical reference frame transformation,describe model recognize two-dimensional shape unsegmented image  independent orientation  position  scale model  called traffic  efficiently represents structural relation object component feature encoding fixed viewpoint-invariant transformation feature reference frame object weight connectionist network using hierarchy transformation  increasing complexity feature successive layer  network recognize multiple object parallel implementation traffic described  along experimental result demonstrating network ability recognize constellation star viewpoint-invariant manner       
1989,discovering high order feature mean field module,new form deterministic boltzmann machine  learning procedure presented efficiently train network module discriminate input vector according criterion new technique directly utilizes free energy mean field module represent probability criterion met  free energy readily manipulated learning procedure although conventional deterministic boltzmann learning fails extract higher order feature shift network bottleneck  combining new mean field module mutual information objective function rapidly produce module perfectly extract important higher order feature without direct external supervision             
1988,distributed connectionist production system,dcps connectionist production system interpreter us distributed representation connectionist model consists many simple  richly interconnected neuron‐like computing unit cooperate solve problem parallel one motivation constructing dcps demonstrate connectionist model capable representing using explicit rule second motivation show “coarse coding” “distributed representations” used construct working memory requires far fewer unit number different fact potentially stored simulation present intended detailed demonstration feasibility certain idea viewed full implementation production system current model many interesting emergent property eventually hope demonstrate damage‐resistant  performs matching variable binding massively parallel constraint satisfaction  capacity working memory dependent similarity item stored            
1988,gemini gradient estimation matrix inversion noise injection,learning procedure measure random perturbation unit activity correlate change reinforcement inefficient simple implement hardware procedure like back-propagation  compute change activity affect output error much efficient  require complex hardware gemini hybrid procedure multilayer network  share many implementation advantage correlational reinforcement procedure efficient gemini injects noise first hidden layer measure resultant effect output error linear network associated hidden layer iteratively inverts matrix relates noise error change  thereby obtaining error-derivatives back-propagation involved  thus allowing unknown non-linearities system two simulation demonstrate effectiveness gemini         
1987,learning guide evolution,assumption acquired characteristic inherited often taken imply adaptation organism learns lifetime cannot guide course evolution inference incorrect  learning alters shape search space evolution operates thereby provides good evolutionary path towards set co-adapted allele demonstrate affect allows learning organism evolve much faster non-learning equivalent  even though characteristic acquired phenotype communicated genotype              
1987,connectionist architecture artificial intelligence,number researcher begun exploring use massively parallel architecture attempt get around limitation conventional symbol processing many parallel architecture connectionist system collection permanent knowledge stored pattern connection connection strength among processing element  knowledge directly determines processing element interact rather sitting passively memory  waiting looked cpu connectionist scheme use formal  symbolic representation  others use analog approach even develop internal representation seeing example pattern recognize relationship store connectionism somewhat controversial ai community new  still unproven large-scale practical application  different style traditional ai approach author begun explore behavior potential connectionist network article  author describe central issue idea connectionism  also unsolved problem facing approach part motivation connectionist research possible similarity function connectionist network neural network human cortex  concentrate connectionisms potential practical technology building intelligent system       
1987,learning representation recirculation,describe new learning procedure network contain group nonlinear unit arranged closed loop aim learning discover code allow activity vector visible group represented activity vector hidden group one way test whether code accurate representation try reconstruct visible vector hidden vector difference original reconstructed visible vector called reconstruction error  learning procedure aim minimize error learning procedure two pass first pas  original visible vector passed around loop  second pas average original vector reconstructed vector passed around loop learning procedure change weight amount proportional product presynaptic activity difference post-synaptic activity two pass procedure much simpler implement method like back-propagation simulation simple network show usually converges rapidly good set code  analysis show certain restricted case performs gradient descent squared reconstruction error           
1987,learning translation invariant recognition massively parallel network,one major goal research massively parallel network neuron-like processing element discover efficient method recognizing pattern another goal discover general learning procedure allow network construct internal representation required complex task paper describes recently developed procedure learn perform recognition task network trained example input vector represents instance pattern particular position required output vector represents name prolonged training  network develops canonical internal representation pattern us canonical representation identify familiar pattern novel position              
1986,learning massively parallel net panel,human brain different conventional digital computer relies massive parallelism rather raw speed store long-term knowledge modifying way processing element interact rather setting bit passive  general purpose memory robust minor physical damage learns experience instead explicitly programmed yet know brain us activity neuron represent complex  articulated structure  perceptual system turn raw input useful internal representation rapidly know brain learns new representational scheme past year lot new interesting theory issue much theorizing motivated belief brain using computational principle could also applied massively parallel artificial system  knew principle one major goal research massively parallel network neuron-like processing element discover efficient method recognizing pattern another goal discover general learning procedure allow network construct internal representation required complex task paper describes recently developed procedure learn perform recognition task network trained example input vector represents instance pattern particular position required output vector represents name prolonged training  network develops canonical internal representation pattern us canonical representation identify familiar pattern novel position         
1985,learning algorithm boltzmann machine,computational power massively parallel network simple processing element resides communication bandwidth provided hardware connection element connection allow significant fraction knowledge system applied instance problem short time one kind computation massively parallel network appear well suited large constraint satisfaction search  use connection efficiently two condition must met first  search technique suitable parallel network must found second  must way choosing internal representation allow preexisting hardware connection used efficiently encoding constraint domain searched describe general parallel search method  based statistical mechanic  show lead general learning rule modifying connection strength incorporate knowledge task domain efficient way describe simple example learning algorithm creates internal representation demonstrably efficient way using preexisting connectivity structure          
1985,symbol among neuron detail connectionist inference architecture,pattern matching variable binding easily implemented conventional computer architecture  necessarily architecture distributed neural network architecture symbol               
1985,shape recognition illusory conjunction,one way achieve viewpoint invariant shape recognition impose canonical  object based frame reference shape describe position  size orientation shape feature relative imposed frame computation implemented parallel network neuron-like processor  network tendency make error peculiar kind presented several shape sometimes perceives one shape position another parameter carefully tuned avoid illusory conjunction normal circumstance  reappear visual input replaced random mask network settled treisman shmidt  shown people make similar error           
1983,massively parallel architecture ai netl thistle boltzmann machine,n/a
1981,parallel computation assigns canonical object-based frame reference,viewpoint independent description shape object generated imposing canonical frame reference object describing spatial disposition part relative object-based frame familiar object unusual orientation  deciding factor choice canonical object-based frame may fact relative frame object familiar shape description may suggest first hypothesize object-based frame test resultant shape description familiarity however  possible organise interaction unit parallel network pattern activity network simultaneously converges representation shape representation object based frame reference connection network determined constraint inherent image formation process             
1981,shape representation parallel system,recent revival interest parallel system computation performed excitatory inhibitory interaction within network relatively simple  neuronlike unit early stage visual processing  individual unit represent hypothesis small local fragment visual input interpreted  interaction unit encode knowledge constraint local interpretation higher visual system  representational issue complex paper considers difficulty involved representing shape parallel system  suggests way overcoming  provides mechanism shape perception visual attention allows novel interpretation gestalt slogan whole sum part         
1979,demonstration effect structural description mental imagery,visual imagery task presented beyond limit normal human ability  factor contributing difficulty isolated comparing difficulty related task argued complex object assigned hierarchical structural description parsed part  local system significant direction two quite different schema wire‐frame cube used illustrate theory  striking perceptual difference give rise described difficulty certain mental imagery task shown depend alternative structural description object used  interpreted evidence structural description important component mental image finally  argued analog transformation like mental folding involve changing value continuous variable structural description          
1978,representation control vision,n/a
1977,relaxation role vision,argued visual system  especially one handle imperfect data  need way selecting best consistent combination among many interrelated  locally plausible hypothesis part aspect visual input may interpreted method presented hypothesis given supposition value   parallel relaxation operator  based plausibility hypothesis logical relation  used modify supposition value  process repeated best consistent set hypothesis supposition value approximately   rest value approximately  method incorporated program interpret configuration overlapping rectangle puppet task possible formulate potentially relevant hypothesis using relaxation select best consistent set complex task  necessary use relaxation locally plausible interpretation guide search locally le obvious one way discussed finally  implemented system presented allows user specify schema inference rule  us relaxation control building network instance schema  presented data instance relation    
1976,using relaxation find puppet,problem finding puppet configuration overlapping  transparent rectangle used show relaxation algorithm extract globally best figure network conflicting local interpretation              
