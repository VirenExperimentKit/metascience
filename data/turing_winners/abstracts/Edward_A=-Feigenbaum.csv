2017,Shakey: From Conception to History.,"Shakey the Robot, conceived fifty years ago, was a seminal contribution to AI. Shakey perceived its world, planned how to achieve a goal, and acted to carry out that plan. This was revolutionary. At the Twenty-Ninth AAAI Conference on Artificial Intelligence, attendees gathered to celebrate Shakey, and to gain insights into how the AI revolution moves ahead. The celebration included a panel, chaired by Benjamin Kuipers and featuring AI pioneers Ed Feigenbaum, Peter Hart, and Nils Nilsson. This article includes written versions of the contributions of those panelists."
2012,"McCarthy as Scientist and Engineer, with Personal Recollections.","In the late 1950s and early 1960s, there were very few people actually doing AI researchómostly the handful of founders (McCarthy, Minsky, and Selfridge in Boston, Newell and Simon in Pittsburgh) plus their students, and that included me. Everyone knew everyone else, and saw them at the few conference panels that were held. At one of those conferences, I met John. We renewed contact upon his re-arrival at Stanford, and that was to have major consequences for my professional life."
2012,Human and Machine Intelligence.,"In his 1950 Mind paper, Alan Turing reframed the question of whether machines could think as an operational or behavioral question: Could a computer be built that was indistinguishable from people in playing the ""imitation game,"" now known as ""the Turing Test""? He conjectured that by the end of the 20th century ""one [would] be able to speak of machines thinking without expecting to be contradicted"" and that computers would succeed in the Turing Test.
Turing's first conjecture proved right. Although his second has not yet been realized, research in Artifi cial Intelligence (AI) has generated a variety of algorithms and techniques regularly deployed in systems enabling them to behave in ways that are broadly considered to be intelligent. The performances of Watson, Siri, and driverless cars are but a few examples in the public eye. This session's panelists will highlight some of the major accomplishments of research in AI and its infl uential role in the development of computer science and computer systems more broadly, considering not only progress in individual subfi elds, but also designs for integrating these into well-functioning systems. They will also consider the ways in which AI theories and methods have infl uenced research on human cognition in behavioral sciences and neuroscience as well as scientifi c research more generally, and they will discuss major challenges and opportunities for the decades ahead."
2003,Some challenges and grand challenges for computational intelligence.,n/a
2001,Knowledge Is Power: The Semantic Web Vision.,"Abstract
Good science periodically revisits old results in the context of new discoveries and technologies. In this way, new understanding is gained of the earlier results and, sometimes, new insights can be gained into current work. This in turn can lead to new discoveries, and so the process continues. In this paper, we revisit the generalization known as the ‚Äúknowledge principle,‚Äù introduced more than twenty years ago to explain the source of power of expert systems. We show that in a new context, the power of knowledge will come from the distribution and decentralization of knowledge that is ubiquitously developed and applied. In the new semantic web concept, tools are provided to the large population of WWW users that allow those individuals (perhaps millions of them) to encode small bodies of knowledge that can be integrated into an effective large knowledge base. The metaphor of ‚Äúknowledge is power‚Äù thus changes from one of the centralized power to one of distributed power."
1998,Innovation and Obstacles: The Future of Computing.,n/a
1994,"Knowledge-Based Systems Research and Applications in Japan, 1992.",n/a
1994,Knowledge-Based Systems in Japan (Report of the JTEC Panel).,n/a
1993,DENDRAL and Meta-DENDRAL: Roots of Knowledge Systems and Expert System Applications.,"During AI's first decade (1956-1966), the task environments in which AI scientists investigated their basic science issues were generally idealized ""clean"" task environments, such as propositional calculus theorem proving and puzzle solving. After the mid-1960s, a bolder and more applied inclination to choose complex real-world problems as task environments became evident. These efforts were both successful and exciting, in two ways. First, the AI programs were achieving high levels of competence at solving certain problems that human specialists found challenging (the excitement was that our AI techniques were indeed powerful and that we were taking the first steps toward the dream of the very smart machine). Second, these complex real-world task environments were proving to be excellent at stimulating basic science questions for the AI science, in knowledge representation, problem solving, and machine learning. To recognize and illuminate this trend, the Artificial Intelligence Journal in 1978 sponsored a special issue on applications of artificial intelligence."
1993,DENDRAL: A Case Study of the First Expert System for Scientific Hypothesis Formation.,n/a
1993,"The Japanese national Fifth Generation project: Introduction, survey, and evaluation.","Projecting a great vision of intelligent systems in the service of the economy and society, the Japanese government in 1982 launched the national Fifth Generation Computer Systems (FGCS) project. The project was carried out by a central research institute, ICOT, with personnel from its member-owners, the Japanese computer manufacturers (JCMs) and other electronics industry firms. The project was planned for ten years, but continues through year eleven and beyond. ICOT chose to focus its efforts on language issues and programming methods for logic programming, supported by special hardware. Sequential ëinference machinesí (PSI) and parallel ëinference machinesí (PIM) were built. Performances of the hardware-software hybrid was measured in the range planned (150 million logical inferences per second). An excellent system for logic programming on parallel machines was constructed (KL1). However, applications were done in demonstration form only (not deployed). The lack of a stream of applications that computer customers found effective, and the sole use of a language outside the mainstream, Prolog, led to disenchantment among the JCMs.
"
1993,Tiger in a Cage: The Applications of Knowledge-based Systems (1993) - Abstract.,"Some pioneers of Artificial Intelligence dreamed of the super-intelligent computer, whose problem solving performance would rival or exceed human performance. Their dream has been partially realized, for narrow areas of human endeavor, in the programs called expert systems, whose behavior is often at world-class levels of competence. Their dream was partially transformed by programs that give intelligent help to humans with problems (rather than perform super-intelligently). These are called knowledge systems.
"
1991,On the Thresholds of Knowledge.,"We articulate the three major findings and hypotheses of AI to date:
1.
(1) The Knowledge Principle: If a program is to perform a complex task well, it must know a great deal about the world in which it operates. In the absence of knowledge, all you have left is search and reasoning, and that isn't enough.

2.
(2) The Breadth Hypothesis: To behave intelligently in unexpected situations, an agent must be capable of falling back on increasingly general knowledge and analogizing to specific but superficially far-flung knowledge. (This is an extension of the preceding principle.)

3.
(3) AI as Empirical Inquiry: Premature mathematization, or focusing on toy problems, washes out details from reality that later turn out to be significant. Thus, we must test our ideas experimentally, falsifiably, on large problems.


We present evidence for these propositions, contrast them with other strategic approaches to AI, point out their scope and limitations, and discuss the future directions they mandate for the main enterprise of AI research."
1987,On the Thresholds of Knowledge.,"We articulate the three major findings and hypotheses of AI to date:
1.
(1) The Knowledge Principle: If a program is to perform a complex task well, it must know a great deal about the world in which it operates. In the absence of knowledge, all you have "
1984,EPAM-like Models of Recognition and Learning.,"A description is provided of EPAM?III, a theory in the form of a computer program for simulating human verbal learning, along with a summary of the empirical evidence for its validity. Criticisms leveled against the theory in a recent paper by Barsalou and Bower are shown to derive largely from their misconception that EPAM?III employed a binary, rather than n?ary branching discrimination net. It is shown that Barsalou and Bower also failed to understand how the recursive structure of EPAM?III eliminates the need to duplicate test nodes that are used to recognize subobjects, and how the possibility of redundant recognition paths controls the sensitivity of EPAM to noticing order. EPAM is also compared briefly with other theories of human discrimination and discrimination learning, including PANDEMONIUM?like systems and dataflow nets.
"
1982,Signal-to-Symbol Transformation: HASP/SIAP Case Study.,"Artificial intelligence is that part of computer science that concerns itself with the concepts and methods of symbolic inference and symbolic representation of knowledge. Its point of departure -- it's most fundamental concept -- is what Newell and Simon called (in their Turing Award Lecture) ""the physical symbol system."" But within the last fifteen years, it has concerned itself also with signals -- with the interpretation or understanding of signal data. AI researchers have discussed ""signal-to symbol transformations,"" and their programs have shown how appropriate use of symbolic manipulations can be of great use in making signal processing more effective and efficient. Indeed, the programs for signal understanding have been fruitful, powerful, and among the most widely recognized of AI's achievements."
1980,Expert Systems: Looking Back and Looking Ahead.,"Abstract
This paper will discuss the applied artificial intelligence work that is sometimes called ‚Äúknowledge engineering‚Äù. The work is based on computer programs that do symbolic manipulations and symbolic inference, not calculation. The programs I will discuss do essentially no numerical calculation. They discover qualitative lines-of-reasoning leading to solutions to problems stated symbolically."
1979,Representation of Dynamic Clinical Knowledge: Measurement Interpretation in the Intensive Care Unit.,"This paper reports work in progress on a program to provide diagnostic and therapeutic suggestions about patients in the Intensive Care Unit (ICU). The Ventilator Manager program (VM) dynamically interprets the clinical significance of quantitative data from the ICU. This data is used to manage post-surgical patients receiving mechanical ventilatory assistance. An extension of a physiological monitoring system, VM (1) provides a summary of the patient's physiological status appropriate for the clinician, (2) recognizes untoward events in the patient/machine system and provides suggestions for corrective action, (3) suggests adjustments to ventilatory therapy based on a long-term assessment of the patient status and therapeutic goals, (4) detects possible measurement errors, and (5) maintains a set of patient-specific expectations and goals for future evaluation. The program produces interpretations of the physiological measurements over time, using a model of the therapeutic procedures in the ICU and clinical knowledge about the diagnostic implications of the data. These therapeutic guidelines are represented by a knowledge base of rules created by clinicians with extensive ICU experience."
1978,Dendral and Meta-Dendral: Their Applications Dimension.,n/a
1978,The art of artificial intelligence - Themes and case studies of knowledge engineering.,"The knowledge engineer practices the art of bringing the principles and tools of AI research to bear on difficult applications problems requiring experts' knowledge for their solution. The technical issues of acquiring this knowledge, representing it, and using it appropriately to construct and explain lines-of-reasoning, are important problems in the design of knowledge-based systems. Various systems that have achieved expert level performance in scientific and medical inference illuminate the art of knowledge engineering and its parent science, Artificial Intelligence."
1978,Use of artificial intelligence for interpretation of physiological measurements-Pulmonary function diagnosis and I.C.U. ventialator (Abstract of presentation).,n/a
1977,Rule-based understanding of signals.,"SU/X and SU/P are knowledge-based programs which employ pattern-invoked inference methods. Both tasks are concerned with the interpretation of large quantities of digitized signal data. The task of SU/X is to understand ""continuous signals"", that is, signals which persist over time. The task of SU/P is to interpret protein x-ray crystallographic data. Some features of the design are: (1) incremental interpretation of data employing many different pattern-invoked sources of knowledge, (2) production rule representation of knowledge, including high level strategy knowledge, (3) ""opportunistic"" hypothesis formation using both data-driven and model-driven techniques within a general hypothesize-and-test paradigm; and (4) multilevel representation of the solution hypothesis."
1977,The Art of Artificial Intelligence: Themes and Case Studies of Knowledge Engineering.,"The knowledge engineer practices the art of bringing the principles and tools of AI research to bear on difficult applications problems requiring experts' knowledge for their solution. The technical issues of acquiring this knowledge, representing it, and"
1974,SUMEX: a resource for applications of artificial intelligence in medicine.,"In partnership with the Biotechnology Resources Branch (BRB) of the Division of Research Resources of the National Institutes of Health, Stanford University is developing a nationally shared computing resource to promote research in artificial intelligence (AI) oriented to bio-medical problems. Based on an initial national community of projects and taking advantage of current data communications technologies, the SUMEX (Stanford University Medical Experimental Computer) project hopes to provide an efficient engineering support team and software library for such investigations, and to promote a more systematic exchange of research tools, products, and ideas among existing projects."
1971,A Heuristic Programming Study of Theory Formation in Science.,"The meta-DENDRAL program is a vehicle for studying problems of theory formation in science. The general strategy of meta-DENDRAL is to reason from data to plausible generalizations and then to organize the generalizations into a unified theory. Three main subproblems are discussed: (1) explain the experimental data for each individual chemical structure, (2) generalize the results from each structure to all structures, and (3) organize the generalizations into a unified theory. The program is built upon the concepts and programmed routines already available in the heuristic DENDRAL performance program, but goes beyond the performance program in attempting to formulate the theory which the performance program will use."
1968,Artificial intelligence: themes in the second decade.,"In this survey of artificial intelligence research, the substantive focus is heuristic programming, problem solving and closely associated learning models. The focus in time is the period 1963-1968. Brief tours are made over a variety of topics: generality, integrated robots, game playing, theorem proving, semantic information processing, etc. One program, which employs the heuristic search paradigm to generate explanatory hypotheses in the analysis of mass spectra of organic molecules, is described in some detail. The problem of representation for problem solving systems is discussed. Various centers of excellence in the artificial intelligence research area is mentioned. A bibliography of 76 references is given."
1963,Artificial intelligence research.,n/a
1962,Simulation of human verbal learning behavior.,"An information processing model of elementary human symbolic learning is given a precise statement as a computer program, called Elementary Perceiver and Memorizer (EPAM). The program simulates the behavior of subjects in experiments involving the rote learning of nonsense syllables. A discrimination net which grows is the basis of EPAM's associative memory. Fundamental information processes include processes for discrimination, discrimination learning, memorization, association using cues, and response retrieval with cues. Many well-known phenomena of rote learning are to be found in EPAM's experimental behavior, including some rather complex forgetting phenomena. EPAM is programmed in Information Processing Language V. H. A. Simon has described some current research in the simulation of human higher mental processes and has discussed some of the techniques and problems which have emerged from this research. The purpose of this paper is to place these general issues in the context of a particular problem by describing in detail a simulation of elementary human symbolic learning processes. The information processing model of mental functions employed is realized by a computer program called Elementary Perceiver and Memorizer (EPAM). The EPAM program is the precise statement of an information processing theory of verbal learning that provides an alternative to other verbal learning theories which have been proposed. It is the result of an attempt to state quite precisely a parsimonious and plausible mechanism sufficient to account for the rote learning of nonsense syllables. The critical evaluation of EPAM must ultimately depend not upon the interest which it may have as a learning machine, but upon its ability to explain and predict the phenomena of verbal learning.
I should like to preface my discussion of the simulation of verbal learning with some brief remarks about the class of information processing models of which EPAM is a member.
a. These are models of mental processes, not brain hardware. They are psychological models of mental function. No physiological or neurological assumptions are made, nor is any attempt made to explain information processes in terms of more elementary neural processes. b. These models conceive of the brain as an information processor with sense organs as input channels, effector organs as output devices, and with internal programs for testing, comparing, analyzing, rearranging, and storing information.c. The central processing mechanism is assumed to be serial; i.e., capable of doing only one (or a very few) things at a time. d. These models use as a basic unit the information symbol; i.e., a pattern of bits which is assumed to be the brain's internal representation of environmental data. e. These models are essentially deterministic, not probabilistic. Random variables play no fundamental role in them.





"
1962,Generalization of an Elementary Perceiving and Memorizing Machine.,n/a
1961,"Soviet cybernetics and computer sciences, 1960.","This article records observations on Soviet research and technology in cybernetics and computer science, made by the author during a visit to the Soviet Union as a delegate to the IFAC Congress on Automatic Control held in Moscow in the summer of 1960."
1961,Soviet Cybernetics and Computer Sciencesﬂû1960.,"Abstract:
This is the author's report of his visit to the Soviet Union in June and July, 1960. The purpose of the trip was to attend the First Congress of the International Federation of Automatic Control (IFAC) as an official American delegate. The author also arranged to meet with certain scientists in psychology, physiology, and the computer sciences, and to visit some Russian research institutions doing work in these areas. Soviet research in cybernetics, neuro-cybernetics, artificial intelligence, mechanical translation, and automatic programming are discussed, and new developments in Soviet computing machines are described. The author describes his discussions with several important Soviet personalities in the computer sciences, which dealt with their particular work and the work of their research institutions. He concludes that Soviet research in the computer sciences lags behind Western developments, but that the gap is neither large nor based on a lack of understanding of fundamental principles. He believes that the Soviets will move ahead rapidly if and when priority, in terms of accessibility to computing machines, is given to their research."
