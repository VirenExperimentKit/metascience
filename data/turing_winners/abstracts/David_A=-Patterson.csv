2019,BROOM: An Open-Source Out-of-Order Processor With Resilient Low-Voltage Operation in 28-nm CMOS.,"Abstract:
The Berkeley resilient out-of-order machine (BROOM) is a resilient, wide-voltage-range implementation of an open-source out-of-order (OoO) RISC-V processor implemented in an ASIC flow. A 28-nm test-chip contains a BOOM OoO core and a 1-MiB level-2 (L2) cache, enhanced with architectural error tolerance for low-voltage operation. It was implemented by using an agile design methodology, where the initial OoO architecture was transformed to perform well in a high-performance, low-leakage CMOS process, informed by synthesis, place, and route data by using foundry-provided standard-cell library and memory compiler. The two-person-team productivity was improved in part thanks to a number of open-source artifacts: The Chisel hardware construction language, the RISC-V instruction set architecture, the rocket-chip SoC generator, and the open-source BOOM core. The resulting chip, taped out using TSMC's 28-nm HPM process, runs at 1.0 GHz at 0.9 V, and is able to operate down to 0.47 V."
2019,FPGA Accelerated INDEL Realignment in the Cloud.,"Abstract:
The amount of data being generated in genomics is predicted to be between 2 and 40 exabytes per year for the next decade, making genomic analysis the new frontier and the new challenge for precision medicine. This paper explores targeted deployment of hardware accelerators in the cloud to improve the runtime and throughput of immensescale genomic data analyses. In particular, INDEL (INsertion/DELetion) realignment is a critical operation that enables diagnostic testings of cancer through error correction prior to variant calling. It is the slowest part of the somatic (cancer) genomic analysis pipeline, the alignment refinement pipeline, and represents roughly one-third of the execution time of timesensitive diagnostics for acute cancer patients. To accelerate genomic analysis, this paper describes a hardware accelerator for INDEL realignment (IR), and a hardware-software framework leveraging FPGAs-as-a-service in the cloud. We chose to implement genomics analytics on FPGAs because genomic algorithms are still rapidly evolving (e.g. the de facto standard ‚ÄúGATK Best Practices‚Äù has had five releases since January of this year). We chose to deploy genomics accelerators in the cloud to reduce capital expenditure and to provide a more quantitative performance and cost analysis. We built and deployed a sea of IR accelerators using our hardware-software accelerator development framework on AWS EC2 F1 instances. We show that our IR accelerator system performed 81√ó better than multi-threaded genomic analysis software while being 32√ó more cost efficient."
2018,A New Golden Age in Computer Architecture: Empowering the Machine-Learning Revolution.,"Abstract:
The end of Moores law and Dennard scaling has led to the end of rapid improvement in general-purpose program performance. Machine learning (ML), and in particular deep learning, is an attractive alternative for architects to explore. It has recently revolutionized vision, speech, language understanding, and many other fields, and it promises to help with the grand challenges facing our society. The computation at its core is low-precision linear algebra. Thus, ML is both broad enough to apply to many domains and narrow enough to benefit from domain-specific architectures, such as Googles Tensor Processing Unit (TPU). Moreover, the growth in demand for ML computing exceeds Moores law at its peak, just as it is fading. Hence, ML experts and computer architects must work together to design the computing systems required to deliver on the potential of ML. This article offers motivation, suggestions, and warnings to computer architects on how to best contribute to the ML revolution."
2018,Motivation for and Evaluation of the First Tensor Processing Unit.,"Abstract:
The first-generation tensor processing unit (TPU) runs deep neural network (DNN) inference 15-30 times faster with 30-80 times better energy efficiency than contemporary CPUs and GPUs in similar semiconductor technologies. This domain-specific architecture (DSA) is a custom chip that has been deployed in Google datacenters since 2015, where it serves billions of people."
2018,50 Years of computer architecture: From the mainframe CPU to the domain-specific tpu and the open RISC-V instruction set.,"Abstract:
IBM had four incompatible computer lines. Each had its own unique instruction-set architecture (ISA); I/O system; system software (assemblers, compilers, libraries); and market niches (business, scientific, real time). IBM engineers bet that they could invent a single ISA that would work for customers of all four lines. Moreover, the same program would run correctly on any implementation of that ISA, though at different speed and cost. That vision required a new way to build computers that would be binary compatible from the cheapest 8-bit model to the fastest 64-bit version."
2018,An Out-of-Order RISC-V Processor with Resilient Low-Voltage Operation in 28NM CMOS.,"Abstract:
An open-source out-of-order superscalar processor implements the 64-bit RISC-V instruction set architecture (ISA) and achieves 3.77 CoreMark/MHz. The 2.7 mm√ó1.8 mm chip includes one core operating at 1.0 GHz at nominal 0.9 V with 1 MB of level-2 (L2) cache in a 28 nm HPM process. A line recycling (LR) technique reuses faulty cache lines that fail at low voltages to correct errors with only 0.77% L2 area overhead. LR reduces minimum operating voltage to 0.47 V, improving energy efficiency by 43% with negligible impact on CPI."
2017,Reduced Instruction Set Computers Then and Now.,"Abstract:
A widely cited Computer article published in 1982 described the reduced instruction set computer (RISC) as an alternative to the general trend at the time toward increasingly complex instruction sets. A RISC executes most instructions in a single short cycle."
2017,Reducing Pagerank Communication via Propagation Blocking.,"Abstract:
Reducing communication is an important objective, as it can save energy or improve the performance of a communication-bound application. The graph algorithm PageRank computes the importance of vertices in a graph, and it serves as an important benchmark for graph algorithm performance. If the input graph to PageRank has poor locality, the execution will need to read many cache lines from memory, some of which may not be fully utilized. We present propagation blocking, an optimization to improve spatial locality, and we demonstrate its application to PageRank. In contrast to cache blocking which partitions the graph, we partition the data transfers between vertices (propagations). If the input graph has poor locality, our approach will reduce communication. Our approach reduces communication more than conventional cache blocking if the input graph is sufficiently sparse or if number of vertices is sufficiently large relative to the cache size. To evaluate our approach, we use both simple analytic models to gain insights and precise hardware performance counter measurements to compare implementations on a suite of 8 real-world and synthetic graphs. We demonstrate our parallel implementations substantially outperform prior work in execution time and communication volume. Although we present results for PageRank, propagation blocking could be generalized to SpMV (sparse matrix multiplying dense vector) or other graph programming models."
2017,In-Datacenter Performance Analysis of a Tensor Processing Unit.,"Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC-called a Tensor Processing Unit (TPU)-deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15Xñ30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30Xñ80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU."
2016,GenAp: a distributed SQL interface for genomic data.,"Background
The impressively low cost and improved quality of genome sequencing provides to researchers of genetic diseases, such as cancer, a powerful tool to better understand the underlying genetic mechanisms of those diseases and treat them with effective targeted therapies. Thus, a number of projects today sequence the DNA of large patient populations each of which produces at least hundreds of terra-bytes of data. Now the challenge is to provide the produced data on demand to interested parties.

Results
In this paper, we show that the response to this challenge is a modified version of Spark SQL, a distributed SQL execution engine, that handles efficiently joins that use genomic intervals as keys. With this modification, Spark SQL serves such joins more than 50◊ faster than its existing brute force approach and 8◊ faster than similar distributed implementations. Thus, Spark SQL can replace existing practices to retrieve genomic data and, as we show, allow users to reduce the number of lines of software code that needs to be developed to query such data by an order of magnitude."
2016,An Agile Approach to Building RISC-V Microprocessors.,"Abstract:
The final phase of CMOS technology scaling provides continued increases in already vast transistor counts, but only minimal improvements in energy efficiency, thus requiring innovation in circuits and architectures. However, even huge teams are struggling to complete large, complex designs on schedule using traditional rigid development flows. This article presents an agile hardware development methodology, which the authors adopted for 11 RISC-V microprocessor tape-outs on modern 28-nm and 45-nm CMOS processes in the past five years. The authors discuss how this approach enabled small teams to build energy-efficient, cost-effective, and industry-competitive high-performance microprocessors in a matter of months. Their agile methodology relies on rapid iterative improvement of fabricatable prototypes using hardware generators written in Chisel, a new hardware description language embedded in a modern programming language. The parameterized generators construct highly customized systems based on the free, open, and extensible RISC-V platform. The authors present a case study of one such prototype featuring a RISC-V vector microprocessor integrated with a switched-capacitor DC-DC converter alongside an adaptive clock generator in a 28-nm, fully depleted silicon-on-insulator process."
2016,Proprietary versus Open Instruction Sets.,"Abstract:
This article presents position statements and a question-and-answer session by panelists at the 4th Workshop on Computer Architecture Research Directions. The subject of the debate was proprietary versus free and open instruction set architectures."
2015,Do-it-yourself textbook publishing.,"Comparing experiences publishing textbooks using traditional publishers and do-it-yourself methods.
"
2015,The NIH BD2K center for big data in translational genomics.,"The worldís genomics data will never be stored in a single repository ñ rather, it will be distributed among many sites in many countries. No one site will have enough data to explain genotype to phenotype relationships in rare diseases; therefore, sites must share data. To accomplish this, the genetics community must forge common standards and protocols to make sharing and computing data among many sites a seamless activity. Through the Global Alliance for Genomics and Health, we are pioneering the development of shared application programming interfaces (APIs) to connect the worldís genome repositories. In parallel, we are developing an open source software stack (ADAM) that uses these APIs. This combination will create a cohesive genome informatics ecosystem. Using containers, we are facilitating the deployment of this software in a diverse array of environments. Through benchmarking efforts and big data driver projects, we are ensuring ADAMís performance and utility.
"
2015,DIABLO: A Warehouse-Scale Computer Network Simulator using FPGAs.,"Motivated by rapid software and hardware innovation in warehouse-scale computing (WSC), we visit the problem of warehouse-scale network design evaluation. A WSC is composed of about 30 arrays or clusters, each of which contains about 3000 servers, leading to a total of about 100,000 servers per WSC. We found many prior experiments have been conducted on relatively small physical testbeds, and they often assume the workload is static and that computations are only loosely coupled with the adaptive networking stack. We present a novel and cost-efficient FPGAbased evaluation methodology, called Datacenter-In-A-Box at LOw cost (DIABLO), which treats arrays as whole computers with tightly integrated hardware and software. We have built a 3,000-node prototype running the full WSC software stack. Using our prototype, we have successfully reproduced a few WSC phenomena, such as TCP Incast and memcached request latency long tail, and found that results do indeed change with both scale and with version of the full software stack."
2015,Scientific computing meets big data technology: An astronomy use case.,"Abstract:
Scientific analyses commonly compose multiple single-process programs into a dataflow. An end-to-end dataflow of single-process programs is known as a many-task application. Typically, tools from the HPC software stack are used to parallelize these analyses. In this work, we investigate an alternate approach that uses Apache Spark - a modern big data platform - to parallelize many-task applications. We present Kira, a flexible and distributed astronomy image processing toolkit using Apache Spark. We then use the Kira toolkit to implement a Source Extractor application for astronomy images, called Kira SE. With Kira SE as the use case, we study the programming flexibility, dataflow richness, scheduling capacity and performance of Apache Spark running on the EC2 cloud. By exploiting data locality, Kira SE achieves a 3.7 œá speedup over an equivalent C program when analyzing a 1TB dataset using 512 cores on the Amazon EC2 cloud. Furthermore, we show that by leveraging software originally designed for big data infrastructure, Kira SE achieves competitive performance to the C implementation running on the NERSC Edison supercomputer. Our experience with Kira indicates that emerging Big Data platforms such as Apache Spark are a performant alternative for many-task scientific applications."
2015,MAGIC: Massive Automated Grading in the Cloud.,"We describe our experience developing and using a specific category of cloud-based autograder (automatic evaluator of student programming assignments) for software engineering. To establish our position in the landscape, our autograder is fully automatic rather than assisting the instructor in performing manual grading, and test based, in that it exercises student code under controlled conditions rather than relying on static analysis or comparing only the output of student programs against reference output. We include a brief description of the course for which the autograders were built, Engineering Software as a Service, and the rationale for building them in the first place, since we had to surmount some new obstacles related to the scale and delivery mechanism of the course. In three years of using the autograders in conjunction with both a software engineering MOOC and the residential course on which the MOOC is based, they have reliably graded hundreds of thousands of student assignments, and are currently being refactored to make their code more easily extensible and maintainable. We have found cloud-based autograding to be scalable, sandboxable, and reliable, and students value the near-instant feedback and opportunities to resubmit homework assignments more than once. Our autograder architecture and implementation are open source, cloud-based, LMS-agnostic, and easily extensible with new types of grading engines. Our goal is not to make specific research claims on behalf of our system, but to extract from our experience engineering lessons for others interested in building or adapting similar systems."
2015,Locality Exists in Graph Processing: Workload Characterization on an Ivy Bridge Server.,"Abstract:
Graph processing is an increasingly important application domain and is typically communication-bound. In this work, we analyze the performance characteristics of three high-performance graph algorithm codebases using hardware performance counters on a conventional dual-socket server. Unlike many other communication-bound workloads, graph algorithms struggle to fully utilize the platform's memory bandwidth and so increasing memory bandwidth utilization could be just as effective as decreasing communication. Based on our observations of simultaneous low compute and bandwidth utilization, we find there is substantial room for a different processor architecture to improve performance without requiring a new memory system."
2015,GAIL: the graph algorithm iron law.,"As new applications for graph algorithms emerge, there has been a great deal of research interest in improving graph processing. However, it is often difficult to understand how these new contributions improve performance. Execution time, the most commonly reported metric, distinguishes which alternative is the fastest but does not give any insight as to why. A new contribution may have an algorithmic innovation that allows it to examine fewer graph edges. It could also have an implementation optimization that reduces communication. It could even have optimizations that allow it to increase its memory bandwidth utilization. More interestingly, a new innovation may simultaneously affect all three of these factors (algorithmic work, communication volume, and memory bandwidth utilization). We present the Graph Algorithm Iron Law (GAIL) to quantify these tradeoffs to help understand graph algorithm performance.
"
2015,Rethinking Data-Intensive Science Using Scalable Analytics Systems.,"""Next generation"" data acquisition technologies are allowing scientists to collect exponentially more data at a lower cost. These trends are broadly impacting many scientific fields, including genomics, astronomy, and neuroscience. We can attack the problem caused by exponential data growth by applying horizontally scalable techniques from current analytics systems to accelerate scientific processing pipelines.

In this paper, we describe ADAM, an example genomics pipeline that leverages the open-source Apache Spark and Parquet systems to achieve a 28x speedup over current genomics pipelines, while reducing cost by 63%. From building this system, we were able to distill a set of techniques for implementing scientific analyses efficiently using commodity ""big data"" systems. To demonstrate the generality of our architecture, we then implement a scalable astronomy image processing system which achieves a 2.8--8.9x improvement over the state-of-the-art MPI-based system."
2015,Past and future of hardware and architecture.,"We start by looking back at 50 years of computer architecture, where philosophical debates on instruction sets (RISC vs. CISC, VLIW vs. RISC) and parallel architectures (NUMA vs clusters) were settled with billion dollar investments on both sides. In the second half, we look forward. First, Moore's Law is ending, so the free ride is over software-oblivious increasing performance. Since we've already played the multicore card, the most-likely/only path left is domain-specific processors. The memory system is radically changing too. First, Jim Gray's decade-old prediction is finally true: ""Tape is dead; flash is disk; disk is tape."" New ways to connect to DRAM and new non-volatile memory technologies promise to make the memory hierarchy even deeper. Finally, and surprisingly, there is now widespread agreement on instruction set architecture, namely Reduced Instruction Set Computers. However, unlike most other fields, despite this harmony has been no open alternative to proprietary offerings from ARM and Intel. RISC-V (""RISC Five"") is the proposed free and open champion. It has a small base of classic RISC instructions that run a full open-source software stack; opcodes reserved for tailoring an System-On-a-Chip (SOC) to applications; standard instruction extensions optionally included in an SoC; and it is unrestricted: there is no cost, no paperwork, and anyone can use it. The ability to prototype using ever-more-powerful FPGAs and astonishingly inexpensive custom chips combined with collaboration on open-source software and hardware offers hope of a new golden era for hardware/software systems."
2014,SMaSH: a benchmarking toolkit for human genome variant calling.,"MOTIVATION
Computational methods are essential to extract actionable information from raw sequencing data, and to thus fulfill the promise of next-generation sequencing technology. Unfortunately, computational tools developed to call variants from human sequencing data disagree on many of their predictions, and current methods to evaluate accuracy and computational performance are ad hoc and incomplete. Agreement on benchmarking variant calling methods would stimulate development of genomic processing tools and facilitate communication among researchers.


RESULTS
We propose SMaSH, a benchmarking methodology for evaluating germline variant calling algorithms. We generate synthetic datasets, organize and interpret a wide range of existing benchmarking data for real genomes and propose a set of accuracy and computational performance metrics for evaluating variant calling methods on these benchmarking data. Moreover, we illustrate the utility of SMaSH to evaluate the performance of some leading single-nucleotide polymorphism, indel and structural variant calling algorithms."
2014,How to build a bad research center.,"Sharing lessons learned from experiences creating successful multidisciplinary research centers.
"
2014,Changepoint Analysis for Efficient Variant Calling.,"Abstract
We present CAGe, a statistical algorithm which exploits high sequence identity between sampled genomes and a reference assembly to streamline the variant calling process. Using a combination of changepoint detection, classification, and online variant detection, CAGe is able to call simple variants quickly and accurately on the 90-95% of a sampled genome which differs little from the reference, while correctly learning the remaining 5-10% that must be processed using more computationally expensive methods. CAGe runs on a deeply sequenced human whole genome sample in approximately 20 minutes, potentially reducing the burden of variant calling by an order of magnitude after one memory-efficient pass over the data."
2013,Using clouds for MapReduce measurement assignments.,"We describe our experiences teaching MapReduce in a large undergraduate lecture course using public cloud services and the standard Hadoop API. Using the standard API, students directly experienced the quality of industrial big-data tools. Using the cloud, every student could carry out scalability benchmarking assignments on realistic hardware, which would have been impossible otherwise. Over two semesters, over 500 students took our course. We believe this is the first large-scale demonstration that it is feasible to use pay-as-you-go billing in the cloud for a large undergraduate course. Modest instructor effort was sufficient to prevent students from overspending. Average per-pupil expenses in the Cloud were under $45. Students were excited by the assignment: 90% said they thought it should be retained in future course offerings.
"
2013,Is the New Software Engineering Curriculum Agile?,"Abstract:
As the last standardization effort was done in 2004, the software engineering curriculum is currently being revised. Haven't we reached the point where agile development should be part of all software engineering curricula? And if so, shouldn't new curriculum standards ensure that it is? Thus, the answer to the question in the title of this article can be affirmative even if the computer science standards committee is absent-minded. Instructors can follow the initial call of the standard for projects by student teams while using an agile process, which is the most natural match. As long as you review both plan-and-document and agile processes in lecture, students can become familiar with both sets of terms and concepts. The more demanding outcomes can be met by the project as well, provided you look to the deeper meaning behind the plan-and-document terms to see where agile can fit."
2013,Direction-optimizing breadth-first search.,"Breadth-First Search is an important kernel used by many graph-processing applications. In many of these emerging applications of BFS, such as analyzing social networks, the input graphs are low-diameter and scale-free. We propose a hybrid approach that is advantageous for low-diameter graphs, which combines a conventional top-down algorithm along with a novel bottom-up algorithm. The bottom-up algorithm can dramatically reduce the number of edges examined, which in turn accelerates the search as a whole. On a multi-socket server, our hybrid approach demonstrates speedups of 3.3ñ7.8 on a range of standard synthetic graphs and speedups of 2.4ñ4.6 on graphs from real social networks when compared to a strong baseline. We also typically double the performance of prior leading shared memory (multicore and GPU) implementations."
2013,Distributed Memory Breadth-First Search Revisited: Enabling Bottom-Up Search.,"Abstract:
Breadth-first search (BFS) is a fundamental graph primitive frequently used as a building block for many complex graph algorithms. In the worst case, the complexity of BFS is linear in the number of edges and vertices, and the conventional top-down approach always takes as much time as the worst case. A recently discovered bottom-up approach manages to cut down the complexity all the way to the number of vertices in the best case, which is typically at least an order of magnitude less than the number of edges. The bottom-up approach is not always advantageous, so it is combined with the top-down approach to make the direction-optimizing algorithm which adaptively switches from top-down to bottom-up as the frontier expands. We present a scalable distributed-memory parallelization of this challenging algorithm and show up to an order of magnitude speedups compared to an earlier purely top-down code. Our approach also uses a 2D decomposition of the graph that has previously been shown to be superior to a 1D decomposition. Using the default parameters of the Graph500 benchmark, our new algorithm achieves a performance rate of over 240 billion edges per second on 115 thousand cores of a Cray XE6, which makes it over 7√ó faster than a conventional top-down algorithm using the same set of optimizations and data distribution."
2013,A hardware evaluation of cache partitioning to improve utilization and energy-efficiency while preserving responsiveness.,"Computing workloads often contain a mix of interactive, latency-sensitive foreground applications and recurring background computations. To guarantee responsiveness, interactive and batch applications are often run on disjoint sets of resources, but this incurs additional energy, power, and capital costs. In this paper, we evaluate the potential of hardware cache partitioning mechanisms and policies to improve efficiency by allowing background applications to run simultaneously with interactive foreground applications, while avoiding degradation in interactive responsiveness. We evaluate these tradeoffs using commercial x86 multicore hardware that supports cache partitioning, and find that real hardware measurements with full applications provide different observations than past simulation-based evaluations. Co-scheduling applications without LLC partitioning leads to a 10% energy improvement and average throughput improvement of 54% compared to running tasks separately, but can result in foreground performance degradation of up to 34% with an average of 6%. With optimal static LLC partitioning, the average energy improvement increases to 12% and the average throughput improvement to 60%, while the worst case slowdown is reduced noticeably to 7% with an average slowdown of only 2%. We also evaluate a practical low-overhead dynamic algorithm to control partition sizes, and are able to realize the potential performance guarantees of the optimal static approach, while increasing background throughput by an additional 19%.
"
2013,Generalized scale independence through incremental precomputation.,"Developers of rapidly growing applications must be able to anticipate potential scalability problems before they cause performance issues in production environments. A new type of data independence, called scale independence, seeks to address this challenge by guaranteeing a bounded amount of work is required to execute all queries in an application, independent of the size of the underlying data. While optimization strategies have been developed to provide these guarantees for the class of queries that are scale-independent when executed using simple indexes, there are important queries for which such techniques are insufficient.

Executing these more complex queries scale-independently requires precomputation using incrementally-maintained materialized views. However, since this precomputation effectively shifts some of the query processing burden from execution time to insertion time, a scale-independent system must be careful to ensure that storage and maintenance costs do not threaten scalability. In this paper, we describe a scale-independent view selection and maintenance system, which uses novel static analysis techniques that ensure that created views do not themselves become scaling bottlenecks. Finally, we present an empirical analysis that includes all the queries from the TPC-W benchmark and validates our implementation's ability to maintain nearly constant high-quantile query and update latency even as an application scales to hundreds of machines.

References"
2012,Crossing the software education chasm.,"An Agile approach that exploits cloud computing.
"
2012,"For better or worse, benchmarks shape a field: technical perspective.",n/a
2012,Diversity within the crowd.,"Though crowdsourcing holds great promise, many struggle with framing tasks and determining which members of the crowd should be recruited to obtain reliable output. In some cases, expert knowledge is desired but, given the time and cost constraints of the problem, may not be available. In this case, it would be beneficial to augment the expert input that is available with input from members of the general population. We believe that reduced reliance on experts will in some cases lead to acceptable performance while reducing cost and latency. In this work, we show that we are able to approach the performance of an expert group for an image labeling task, while reducing our reliance on experts by incorporating non-expert responses.
"
2012,Direction-optimizing breadth-first search.,"Abstract:
Breadth-First Search is an important kernel used by many graph-processing applications. In many of these emerging applications of BFS, such as analyzing social networks, the input graphs are low-diameter and scale-free. We propose a hybrid approach that is advantageous for low-diameter graphs, which combines a conventional top-down algorithm along with a novel bottom-up algorithm. The bottom-up algorithm can dramatically reduce the number of edges examined, which in turn accelerates the search as a whole. On a multi-socket server, our hybrid approach demonstrates speedups of 3.3 -- 7.8 on a range of standard synthetic graphs and speedups of 2.4 -- 4.6 on graphs from real social networks when compared to a strong baseline. We also typically double the performance of prior leading shared memory (multicore and GPU) implementations."
2012,Experiences teaching MapReduce in the cloud.,"We describe our experiences teaching MapReduce in a large undergraduate lecture course using public cloud services. Using the cloud, every student could carry out scalability benchmarking assignments on realistic hardware, which would have been impossible otherwise. Over two semesters, over 500 students took our course. We believe this is the first large-scale demonstration that it is feasible to use pay-as-you-go billing in the Cloud for a large undergraduate course. Modest instructor effort was sufficient to prevent students from overspending. Average per-pupil expenses in the Cloud were under $45, less than half our available grant funding. Students were excited by the assignment: 90% said they thought it should be retained in future course offerings.
"
2011,PIQL: Success-Tolerant Query Processing in the Cloud.,"Newly-released web applications often succumb to a ""Success Disaster,"" where overloaded database machines and resulting high response times destroy a previously good user experience. Unfortunately, the data independence provided by a traditional relational database system, while useful for agile development, only exacerbates the problem by hiding potentially expensive queries under simple declarative expressions. As a result, developers of these applications are increasingly abandoning relational databases in favor of imperative code written against distributed key/value stores, losing the many benefits of data independence in the process. Instead, we propose PIQL, a declarative language that also provides scale independence by calculating an upper bound on the number of key/value store operations that will be performed for any query. Coupled with a service level objective (SLO) compliance prediction model and PIQL's scalable database architecture, these bounds make it easy for developers to write success-tolerant applications that support an arbitrarily large number of users while still providing acceptable performance. In this paper, we present the PIQL query processing system and evaluate its scale independence on hundreds of machines using two benchmarks, TPC-W and SCADr. "
2011,The SCADS Director: Scaling a Distributed Storage System Under Stringent Performance Requirements.,"Elasticity of cloud computing environments provides an economic incentive for automatic resource allocation of stateful systems running in the cloud. However, these systems have to meet strict performance Service-Level Objectives (SLOs) expressed using upper percentiles of request latency, such as the 99th. Such latency measurements are very noisy, which complicates the design of the dynamic resource allocation. We design and evaluate the SCADS Director, a control framework that reconfigures the storage system on-the-fly in response to workload changes using a performance model of the system. We demonstrate that such a framework can respond to both unexpected data hotspots and diurnal workload patterns without violating strict performance SLOs.
"
2011,CUDA-level Performance with Python-level Productivity for Gaussian Mixture Model Applications.,"Typically, scientists with computational needs prefer to use high-level languages such as Python or MATLAB; however, large computationally-intensive problems must eventually be recoded in a low level language such as C or Fortran by expert programmers in order to achieve sufficient performance. In addition, multiple strategies may exist for mapping a problem onto parallel hardware; unless the hardware geometry and problem dimensions are both taken into account, large factors of performance may be left on the table. We show how to preserve the productivity of high-level languages while obtaining the performance of the best low-level language code variant for a given hardware platform and problem size using SEJITS (Selective Embedded Just-in-Time Specialization), a set of techniques that leverages just-in-time code generation and compilation combined with reflection and metaprogramming. As a case study, we demonstrate our technique for Gaussian Mixture Model training using the EM algorithm. With the addition of one line of code to import our framework, a domain programmer using an existing Python GMM library can run her program unmodified on a GPU-equipped computer and achieve performance that meets or beats GPU code hand-crafted by a human expert. We also show that despite the overhead of allowing the domain expert's program to use Python and the overhead of just-in-time code generation and compilation, our approach still results in performance competitive with hand-crafted GPU code."
2010,A view of cloud computing.,"Clearing the clouds away from the true potential and obstacles posed by this computing capability.
"
2010,Diverse connections.,n/a
2010,"Ubiquitous Parallel Computing from Berkeley, Illinois, and Stanford.","Abstract:
The ParLab at Berkeley, UPCRC-Illinois, and the Pervasive Parallel Laboratory at Stanford are studying how to make parallel programming succeed given industry's recent shift to multicore computing. All three centers assume that future microprocessors will have hundreds of cores and are working on applications, programming environments, and architectures that will meet this challenge. This article briefly surveys the similarities and difference in their research."
2010,The case for PIQL: a performance insightful query language.,"Large-scale, user-facing applications are increasingly moving from relational databases to distributed key/value stores for high-request-rate, low-latency workloads. Often, this move is motivated not only by key/value stores' ability to scale simply by adding more hardware, but also by the easy to understand predictable performance they provide for all operations. For complex queries, this approach often requires onerous explicit index management and imperative data lookup by the developer. We propose PIQL, a Performance Insightful Query Language that allows developers to express many queries found on these websites while still providing strict bounds on the number of I/O operations that will be performed.
"
2010,"Characterizing, modeling, and generating workload spikes for stateful services.","Evaluating the resiliency of stateful Internet services to significant workload spikes and data hotspots requires realistic workload traces that are usually very difficult to obtain. A popular approach is to create a workload model and generate synthetic workload, however, there exists no characterization and model of stateful spikes. In this paper we analyze five workload and data spikes and find that they vary significantly in many important aspects such as steepness, magnitude, duration, and spatial locality. We propose and validate a model of stateful spikes that allows us to synthesize volume and data spikes and could thus be used by both cloud computing users and providers to stress-test their infrastructure.
"
2010,RAMP gold: an FPGA-based architecture simulator for multiprocessors.,"We present RAMP Gold, an economical FPGA-based architecture simulator that allows rapid early design-space exploration of manycore systems. The RAMP Gold prototype is a high-throughput, cycle-accurate full-system simulator that runs on a single Xilinx Virtex-5 FPGA board, and which simulates a 64-core shared-memory target machine capable of booting real operating systems. To improve FPGA implementation efficiency, functionality and timing are modeled separately and host multithreading is used in both models. We evaluate the prototype's performance using a modern parallel benchmark suite running on our manycore research operating system, achieving two orders of magnitude speedup compared to a widely-used software-based architecture simulator.
"
2010,Statistics-driven workload modeling for the Cloud.,"Abstract:
A recent trend for data-intensive computations is to use pay-as-you-go execution environments that scale transparently to the user. However, providers of such environments must tackle the challenge of configuring their system to provide maximal performance while minimizing the cost of resources used. In this paper, we use statistical models to predict resource requirements for Cloud computing applications. Such a prediction framework can guide system design and deployment decisions such as scale, scheduling, and capacity. In addition, we present initial design of a workload generator that can be used to evaluate alternative configurations without the overhead of reproducing a real workload. This paper focuses on statistical modeling and its application to data-intensive workloads."
2010,Detecting Large-Scale System Problems by Mining Console Logs.,"Surprisingly, console logs rarely help operators detect problems in large-scale datacenter services, for they often consist of the voluminous intermixing of messages from many software components written by independent developers. We propose a general methodology to mine this rich source of information to automatically detect system runtime problems. We first parse console logs by combining source code analysis with information retrieval to create composite features. We then analyze these features using machine learning to detect operational problems. We show that our method enables analyses that are impossible with previous methods because of its superior ability to create sophisticated features. We also show how to distill the results of our analysis to an operator-friendly one-page decision tree showing the critical messages associated with the detected problems. We validate our approach using the Darkstar online game server and the Hadoop File System, where we detect numerous real problems with high accuracy and few false positives. In the Hadoop case, we are able to analyze 24 million lines of console logs in 3 minutes. Our methodology works on textual console logs of any size and requires no changes to the service software, no human input, and no knowledge of the software's internals"
2010,A case for FAME: FPGA architecture model execution.,"Given the multicore microprocessor revolution, we argue that the architecture research community needs a dramatic increase in simulation capacity. We believe FPGA Architecture Model Execution (FAME) simulators can increase the number of useful architecture research experiments per day by two orders of magnitude over Software Architecture Model Execution (SAME) simulators. To clear up misconceptions about FPGA-based simulation methodologies, we propose a FAME taxonomy to distinguish the costperformance of variations on these ideas. We demonstrate our simulation speedup claim with a case study wherein we employ a prototype FAME simulator, RAMP Gold, to research the interaction between hardware partitioning mechanisms and operating system scheduling policy. The study demonstrates FAME's capabilities: we run a modern parallel benchmark suite on a research operating system, simulate 64-core target architectures with multi-level memory hierarchy timing models, and add experimental hardware mechanisms to the target machine. The simulation speedup achieved by our adoption of FAME-250◊-enables experiments with more realistic time scales and data set sizes thanare possible with SAME.
"
2010,A Graphical Representation for Identifier Structure in Logs.,"Application console logs are a ubiquitous tool for diagnosing system failures and anomalies. While several techniques exist to interpret logs, describing and assessing log quality remains relatively unexplored. In this paper, we describe an abstract graphical representation of console logs called the identifier graph and a visualization based on this representation. Our representation breaks logs into message types and identifier fields and shows the interrelation between the two.

We describe two applications of this visualization. We apply it to Hadoop logs from two different deployments, showing that we capture important properties of Hadoop's logging as well as relevant differences between the two sites. We also apply our technique to logs from two other systems under development. We show that our representation helps highlight flaws in the underlying application logging."
2010,PIQL: a performance insightful query language.,"Large-scale websites are increasingly moving from relational databases to distributed key-value stores for high request rate, low latency workloads. Often this move is motivated not only by key-value stores' ability to scale simply by adding more hardware, but also by the easy to understand predictable performance they provide for all operations. While this data model works well, lookups are only done by primary key. More complex queries require onerous, explicit index management and imperative data lookups by the developer. We demonstrate PIQL, a Performance Insightful Query Language that allows developers to express many of the queries found on these websites, while still providing strict bounds on the number of I/O operations for any query.
"
2010,The Parallel Revolution Has Started: Are You Part of the Solution or Part of the Problem? - An Overview of Research at the Berkeley Parallel Computing Laboratory.,"Abstract
The Par Lab started in 2008, based on an earlier technical report ‚ÄúThe Berkeley View‚Äù on the parallel computing challenge. (K. Asanovic, R. Bodik, B. C. Catanzaro, J. J. Gebis, P. Husbands, K. Keutzer, D. A. Patterson, W. L. Plishker, J. Shalf, S. W. Williams, and K. A. Yelick. The landscape of parallel computing research: A view from Berkeley. Technical Report UCB/EECS-2006-183, EECS Department, University of California, Berkeley, December 18 2006.) This talk gives an update on where we are two years in the Par Lab. We picked five applications to drive our research, and believe they collectively capture many of the important features of future client applications even if they themselves do not become the actual future ‚Äúkiller app‚Äù. The Personalized Medicine application focuses on detailed modeling of individual‚Äôs responses to treatments, representing the important health market. The Music application emphasizes real-time responsiveness to rich human input, with high-performance many-channel audio synthesis. The Speech application focuses on making speech input work well in the real-world noisy environments where mobile devices will be operated. The Content-Based Image Recognition (CBIR) application represents the growing practical use of machine vision. Finally, the Parallel Web Browser is currently perhaps the most important single application on client devices, as well as representative of many other interactive rich-document processing tasks.
Our first step in attacking the parallel programming challenge was to analyze a wide range of applications, including workloads from embedded computing, desktop computing, games, databases, machine learning, and scientific computing, as well as our five driving applications. We discovered a surprisingly compact set of recurring computational patterns, which we termed ‚Äúmotifs‚Äù. We have greatly expanded on this work, and now believe that any successful software architecture, parallel or serial, can be described as a hierarchy of patterns. We divide patterns into either computational patterns, which describe a computation to be performed, or structural patterns, which describe how computations are composed. The patterns have proven central to ourresearch effort, serving as both a common human vocabulary for multidisciplinary discussions spanning application developers to hardware architects, as well as an organizing structure for software development. Another organizing principle in our original proposal was to divide the software development stack into two layers: efficiency and productivity. Programmers working in the efficiency layer are generally experts in achieving high performance from the underlying hardware, but are not necessarily knowledgeable of any given application domain. Programmers working in the productivity layer are generally knowledgeable about an application domain, but are less concerned with hardware details. The patterns bridge these two layers. Efficiency programmers develop libraries and frameworks that efficiently implement the standard patterns, and productivity programmers can decompose an application into patterns and use high-level languages to compose corresponding libraries and frameworks to form applications.
To improve the quality and portability of efficiency-level libraries, we proposed to leverage our earlier work on autotuning. Autotuning is an automatic search-based optimization process whereby multiple variants of a routine are generated and empirically evaluated on the hardware platform. We have also included a major effort on parallel program correctness to help programmers test, verify, and debug their code. Different correctness techniques apply at the efficiency layer, where low-level data races and deadlocks are of concern, and at the productivity layer, where we wish to ensure semantic determinism and atomicity. Our whole pattern-based component approach to the software stack hinges on the ability to efficiently and flexibly compose software modules. We developed a low-level user-level scheduling substrate called ‚ÄúLithe‚Äù to support efficient sharing of processing resources between arbitrary modules, even those written in different languages and to different programming models.
Our operating system and architecture research is devoted to supporting the software stack. The OS is based on space-time partitioning, which exports stable partitions of the machine resources with quality-of-service guarantees to an application, and two-level scheduling, which allows a user-level scheduler, such as Lithe, to perform detailed application-specific scheduling within a partition. Our architecture research focuses on techniques to support OS resource partitioning, performance counters to support application adaptivity, software-managed memory hierarchies to increase memory efficiency, and scalable coherence and synchronization mechanisms to lower parallel system overheads. To experiment with the behavior of our new software stack on our new OS and hardware mechanisms, we have developed an FPGA-based simulation environment, ‚ÄúRAMP Gold‚Äù. By running our full application and OS software environment on our fast architectural simulator, we can quickly iterate across levels in our system stack."
2010,Software knows best: portable parallelism requires standardized measurements of transparent hardware.,"The hardware trend of the last 15 years of dynamically trying to improve performance with little software visibility is not only irrelevant today, its counterproductive; adaptivity must be at the software level if parallel software is going to be portable, fast, and energy-efficient. A portable parallel program is an oxymoron today; there is no reason to be parallel if it's slow, and parallel can't be fast if it's portable. Hence, portable parallel programs of the future must be able to understand and measure /any/ computer on which it runs so that it can adapt effectively, which suggests that hardware measurement should be standardized and processor performance and energy consumption should become transparent.

In addition to software-controlled adaptivity for execution efficiency by using techniques like autotuning and dynamic scheduling, modern software environments adapt to improve /programmer/ efficiency [1]. Classic examples include dynamic linking, dynamic memory allocation, garbage collection, interpreters, just-in-time compilers, and debugger-support. Examples that are more recent are selective embedded just in time specialization (SEJITS) [2] for highly productive languages like Python and Ruby. Thus, the future of programming is likely to involve program generators at many levels of the hierarchy tailoring the application to the machine. These productivity advances via adaptivity should be reflected in modern benchmarks: virtually no one writes the statically linked, highest-level-optimized C programs that are the foundation of most benchmark suites.

The dream is to improve productivity without sacrificing too much performance. Indeed, how often have you heard the claim that a new productive environment is now ""almost as fast as C"" or ""almost as fast as Java?"" The implication of the necessary tie between productivity and performance in the manycore era is that these modern environments must be able to utilize manycore well, or the gap between highly efficient code and highly productive code will grow with the number of cores.

For industry's bet on manycore to win, therefore, both very high level and very low level programming environments will need to be able to understand and measure their underlying hardware and adapt their execution so as to be portable, relatively fast, and energy-efficient.

Hence, we argue that a standard of accurate hardware operation trackers (SHOT) would have a huge positive impact on making parallel software portable with good performance and energy efficiency, similar to the impact of the IEEE-754 standard had on portability of numerical software. In particular, we believe SHOT will lead to much larger improvements in portability, performance, energy efficiency of parallel codes than recent architectural fads like opportunistic ""turbo modes,"" transactional memory, or reconfigurable computing."
2009,Viewpoint - Your students are your legacy.,"This Viewpoint boils down into a few magazine pages what I've learned in my 32 years of mentoring Ph.D. students.
"
2009,Roofline: an insightful visual performance model for multicore architectures.,The Roofline model offers insight on how to improve the performance of software and hardware. 
2009,A view of the parallel computing landscape.,"Writing programs that scale with increasing numbers of cores should be as easy as writing programs for sequential computers.
"
2009,SCADS: Scale-Independent Storage for Social Computing Applications.,"Collaborative web applications such as Facebook, Flickr and Yelp present new challenges for storing and querying large amounts of data. As users and developers are focused more on performance than single copy consistency or the ability to perform ad-hoc queries, there exists an opportunity for a highly-scalable system tailored specifically for relaxed consistency and pre-computed queries. The Web 2.0 development model demands the ability to both rapidly deploy new features and automatically scale with the number of users. There have been many successful distributed key-value stores, but so far none provide as rich a query language as SQL. We propose a new architecture, SCADS, that allows the developer to declaratively state application specific consistency requirements, takes advantage of utility computing to provide cost effective scale-up and scale-down, and will use machine learning models to introspectively anticipate performance problems and predict the resource requirements of new queries before execution."
2009,Statistical Machine Learning Makes Automatic Control Practical for Internet Datacenters.,"Horizontally-scalable Internet services on clusters of commodity computers appear to be a great fit for automatic control: there is a target output (service-level agreement), observed output (actual latency), and gain controller (adjusting the number of servers). Yet few datacenters are automated this way in practice, due in part to well-founded skepticism about whether the simple models often used in the research literature can capture complex real-life workload/performance relationships and keep up with changing conditions that might invalidate the models. We argue that these shortcomings can be fixed by importing modeling, control, and analysis techniques from statistics and machine learning. In particular, we apply rich statistical models of the application's performance, simulation-based methods for finding an optimal control policy, and change-point methods to find abrupt changes in performance. Preliminary results running aWeb 2.0 benchmark application driven by real workload traces on Amazon's EC2 cloud show that our method can effectively control the number of servers, even in the face of performance anomalies."
2009,Predicting Multiple Metrics for Queries: Better Decisions Enabled by Machine Learning.,"Abstract:
One of the most challenging aspects of managing a very large data warehouse is identifying how queries will behave before they start executing. Yet knowing their performance characteristics - their runtimes and resource usage - can solve two important problems. First, every database vendor struggles with managing unexpectedly long-running queries. When these long-running queries can be identified before they start, they can be rejected or scheduled when they will not cause extreme resource contention for the other queries in the system. Second, deciding whether a system can complete a given workload in a given time period (or a bigger system is necessary) depends on knowing the resource requirements of the queries in that workload. We have developed a system that uses machine learning to accurately predict the performance metrics of database queries whose execution times range from milliseconds to hours. For training and testing our system, we used both real customer queries and queries generated from an extended set of TPC-DS templates. The extensions mimic queries that caused customer problems. We used these queries to compare how accurately different techniques predict metrics such as elapsed time, records used, disk I/Os, and message bytes. The most promising technique was not only the most accurate, but also predicted these metrics simultaneously and using only information available prior to query execution. We validated the accuracy of this machine learning technique on a number of HP Neoview configurations. We were able to predict individual query elapsed time within 20% of its actual time for 85% of the test queries. Most importantly, we were able to correctly identify both the short and long-running (up to two hour) queries to inform workload management and capacity planning."
2009,Online System Problem Detection by Mining Patterns of Console Logs.,"Abstract:
We describe a novel application of using data mining and statistical learning methods to automatically monitor and detect abnormal execution traces from console logs in an online setting. Different from existing solutions, we use a two stage detection system. The first stage uses frequent pattern mining and distribution estimation techniques to capture the dominant patterns (both frequent sequences and time duration). The second stage use principal component analysis based anomaly detection technique to identify actual problems. Using real system data from a 203-node Hadoop cluster, we show that we can not only achieve highly accurate and fast problem detection, but also help operators better understand execution patterns in their system."
2009,Detecting large-scale system problems by mining console logs.,"Surprisingly, console logs rarely help operators detect problems in large-scale datacenter services, for they often consist of the voluminous intermixing of messages from many software components written by independent developers. We propose a general methodology to mine this rich source of information to automatically detect system runtime problems. We first parse console logs by combining source code analysis with information retrieval to create composite features. We then analyze these features using machine learning to detect operational problems. We show that our method enables analyses that are impossible with previous methods because of its superior ability to create sophisticated features. We also show how to distill the results of our analysis to an operator-friendly one-page decision tree showing the critical messages associated with the detected problems. We validate our approach using the Darkstar online game server and the Hadoop File System, where we detect numerous real problems with high accuracy and few false positives. In the Hadoop case, we are able to analyze 24 million lines of console logs in 3 minutes. Our methodology works on textual console logs of any size and requires no changes to the service software, no human input, and no knowledge of the software's internals."
2008,Technical perspective: the data center is the computer.,"Internet services are already significant forces in searching, retail purchases, music downloads, and auctions. One vision of 21st century IT is that most users will be accessing such services over a descendant of the cell phone rather than running shrink-wrapped software on a descendant of the PC. "
2008,Design and implementation trade-offs for wide-area resource discovery.,"This paper describes the design and implementation of SWORD, a scalable resource discovery service for wide-area distributed systems. In contrast to previous systems, SWORD allows users to describe desired resources as a topology of interconnected groups with required intragroup, intergroup, and per-node characteristics, along with the utility that the application derives from various ranges of values of those characteristics. This design gives users the flexibility to find geographically distributed resources for applications that are sensitive to both node and network characteristics, and allows the system to rank acceptable configurations based on their quality for that application. We explore a variety of architectures to deliver SWORD's functionality in a scalable and highly-available manner. A 1000-node ModelNet evaluation using a workload of measurements collected from PlanetLab shows that an architecture based on 4-node server cluster sites at network peering facilities outperforms a decentralized DHT-based resource discovery infrastructure for all but the smallest number of sites. While such a centralized architecture shows significant promise, we find that our decentralized implementation, both in emulation and running continuously on over 200 PlanetLab nodes, performs well while benefiting from the DHT's self-healing properties."
2008,Mining Console Logs for Large-Scale System Problem Detection.,"The console logs generated by an application contain messages that the application developers believed would be useful in debugging or monitoring the application. Despite the ubiquity and large size of these logs, they are rarely exploited in a systematic way for monitoring and debugging because they are not readily machine-parsable. In this paper, we propose a novel method for mining this rich source of information. First, we combine log parsing and text mining with source code analysis to extract structure from the console logs. Second, we extract features from the structured information in order to detect anomalous patterns in the logs using Principal Component Analysis (PCA). Finally, we use a decision tree to distill the results of PCA-based anomaly detection to a format readily understandable by domain experts (e.g. system operators) who need not be familiar with the anomaly detection algorithms. As a case study, we distill over one million lines of console logs from the Hadoop file system to a simple decision tree that a domain expert can readily understand; the process requires no operator intervention and we detect a large portion of runtime anomalies that are commonly overlooked."
2008,Stencil computation optimization and auto-tuning on state-of-the-art multicore architectures.,"Abstract:
Understanding the most efficient design and utilization of emerging multicore systems is one of the most challenging questions faced by the mainstream and scientific computing industries in several decades. Our work explores multicore stencil (nearest-neighbor) computations - a class of algorithms at the heart of many structured grid codes, including PDE solvers. We develop a number of effective optimization strategies, and build an auto-tuning environment that searches over our optimizations and their parameters to minimize runtime, while maximizing performance portability. To evaluate the effectiveness of these strategies we explore the broadest set of multicore architectures in the current HPC literature, including the Intel Clovertown, AMD Barcelona, Sun Victoria Falls, IBM QS22 PowerXCell 8i, and NVIDIA GTX280. Overall, our auto-tuning optimization methodology results in the fastest multicore stencil performance to date. Finally, we present several key insights into the architectural tradeoffs of emerging multicore designs and their implications on scientific algorithm development."
2007,Embracing and Extending 20th-Century Instruction Set Architectures.,"Abstract:
A vector case study shows how new functionality can be added to extend the 80times86 and PowerPC architectures to support a full vector architecture, primarily by enhancing their multimedia extensions to provide a better model for compilers and an easier-to-understand model for programmers"
2007,RAMP: Research Accelerator for Multiple Processors.,"Abstract:
The RAMP project's goal is to enable the intensive, multidisciplinary innovation that the computing industry will need to tackle the problems of parallel processing. RAMP itself is an open-source, community-developed, FPGA-based emulator of parallel architectures. its design framework lets a large, collaborative community develop and contribute reusable, composable design modules. three complete designs - for transactional memory, distributed systems, and distributed-shared memory - demonstrate the platform's potential."
2007,The parallel computing landscape: a Berkeley view.,n/a
2006,New Directions for CACM?,"Much has changed since the 1980s when the current format of Communications of the ACM was cast. What should be the mission of ACM's flagship publication in the 21st century?
"
2006,Offshoring: finally facts vs. folklore.,"The ACM Job Migration study was released last month, and it includes many recommendations for current and future computing professionals and educators.
"
2006,Computer science education in the 21st century.,"To draw students to CS, we must first look to create a curriculum that reflects the exciting opportunities and challenges of IT today versus the 1970s. Future students and faculty would greatly benefit from a reinvigorated CS curriculum.
"
2006,Reviving your favorite CS books.,n/a
2006,RAMP: research accelerator for multiple processors - a community vision for a shared experimental parallel HW/SW platform.,"Abstract:
Summary form only given. The vast majority of computer architects believe the future of the microprocessor is hundreds to thousands of processors (""cores"") on a chip. Given such widespread agreement, it's surprising how much research remains to be done in algorithms, computer architecture, networks, operating systems, file systems, compilers, programming languages, applications, and so on to realize this vision. Fortunately, Moore's law has not only enabled dense multi-core chips, it has also enabled extremely dense FPGAs. Today, one to two dozen soft cores can be programmed into a single FPGA. With multiple FPGAs on a board and multiple boards in a system, 1000-processor designs can be economically and rapidly explored. To make this happen, however, requires a significant amount of infrastructure in hardware, software, and what we call ""gateware"", the register-transfer level models that fill the FGPAs. By using the Berkeley Emulation Engine boards that were created for other purposes, the hardware is already done. A group of architects plan to design the gateware, create this infrastructure, and share the results in an open-source fashion so that every institution could have their own. Such a system would not just invigorate multiprocessors research in the architecture community. Since processors cores can run at 100 to 200 MHz, a large scale multiprocessor would be fast enough to run operating systems and large programs at speeds sufficient to support software research. Moreover, there is a new generation of FPGAs every 18 months with capacity for twice as many cores and run them faster, so future multiboard FPGA systems are even more attractive. Hence, we believe such a system would accelerate research across all the fields that touch multiple processors. Thus the acronynm RAMP, for Research Accelerator for Multiple Processors. RAMP has the potential to transform the parallel computing community in computer science from a simulation-driven to a prototype-driven d...
(View more)"
2006,The Berkeley View: A New Framework and a New Platform for Parallel Research.,"Abstract
The recent switch to parallel microprocessors is a milestone in history of computing. Industry has laid out a roadmap for multicore designs that preserve the programming paradigm of the past via binary-compatibility and cache-coherence. Conventional wisdom is now to double the number of cores on a chip with each silicon generation. A multidisciplinary group of Berkeley researchers met for 18 months to discuss this change. Our investigations into the future opportunities in led to the follow recommendations which are more revolutionary what industry plans to do:
The target should be 1000s of cores per chip, as this hardware is the most efficient in MIPS per watt, MIPS per area of silicon, and MIPS per development dollar.
To maximize application efficiency, programming models should support a wide range of data types and successful models of parallelism: data-level parallelism, independent task parallelism, and instructionlevel parallelism.
Should play a larger role than conventional compilers in translating parallel programs.
The conventional path to architecture innovation is to study a benchmark suite like SPEC or Splash to guide and evaluate innovation. A problem for innovation in parallelism iit best. Hence, it seems unwise to let a set of old programs from the past drive an investigation into parallel computing of the future."
2006,Windows XP Kernel Crash Analysis.,"PC users have started viewing crashes as a fact of life rather than a problem. To improve operating system dependability, systems designers and programmers must analyze and understand failure data. In this paper, we analyze Windows XP kernel crash data collected from a population of volunteers who contribute to the Berkeley Open Infrastructure for Network Computing (BOINC) project. We found that OS crashes are predominantly caused by poorly-written device driver code. Users as well as product developers will benefit from understanding the crash behaviors elaborated in this paper."
2006,Service Placement in a Shared Wide-Area Platform.,"Emerging federated computing environments offer attractive platforms to test and deploy global-scale distributed applications. When nodes in these platforms are time-shared among competing applications, available resources vary
across nodes and over time. Thus, one open architectural question in such systems is how to map applications to available nodes--that is, how to discover and select resources. Using a six-month trace of PlanetLab resource utilization data and of resource demands from three long-running PlanetLab services, we quantitatively characterize resource availability and application usage behavior across nodes and over time, and investigate the potential to mitigate the application impact of resource variability through intelligent service placement and migration.
We find that usage of CPU and network resources is heavy and highly variable. We argue that this variability calls for intelligently mapping applications to available nodes. Further, we find that node placement decisions can become ill-suited after about 30 minutes, suggesting that some applications can benefit from migration at that timescale, and that placement and migration decisions can be safely based on data collected at roughly that timescale. We find that inter-node latency is stable and is a good predictor of available bandwidth; this observation argues for collecting latency data at relatively coarse timescales and bandwidth data at even coarser timescales, using the former to predict the latter between measurements. Finally, we find that although the utilization of a particular resource on a particular node is a good predictor of that node's utilization of that resource in the near future, there do not exist correlations to support predicting one resource's availability based on availability of other resources on the same node at the same time, on availability of the same resource on other nodes at the same site, or on time-series forecasts that assume a daily or weekly regression to the mean."
2005,Minority-minority and minority-majority technology transfer.,"Finding technology solutions to help those in need can often lead to welcomed answers for all if we look at the big picture.
"
2005,20th century vs. 21st century C&C: the SPUR manifesto.,"As technologists, we must confront the current weaknesses and deliver on the potential opportunities of computer and communication technologies in the 21st century. Consider this a call to arms for tackling that challenge.
"
2005,The state of funding for new initiatives in computer science and engineering.,"The intellectual opportunities are huge, the social benefits transforming, yet redesigning a workable funding model for CS&E research will require our collective imagination and collaboration between many IT sectors.
"
2005,Recognizing individual excellence helps us all.,"In addition to honoring individuals who have advanced our field, recognizing technical excellence enhances the image of our profession and helps attract the best and brightest to our field.
"
2005,Reflections on a programming Olympiad.,"Does ACM's programming contest give insight into the relative strengths of IT education around the world?
"
2005,Does ACM support matter to conferences or journals?,"ACM is one of many organizations that sponsors IT conferences and journals. So, why use ACM?
"
2005,Restoring the popularity of computer science.,"Inaccurate impressions of the opportunities of 21st century CS are shrinking the next generation of IT professionals. You can help by dispelling incorrect beliefs about employment and by helping improve pre-college education.
"
2005,"The new Professional Development Centre boasts 1, 000 courses, O'Reilly Books, and CS classics.","ACM is expanding the PDC along three axes: Doubling the number of online courses, adding 500 books from Safari (including O'Reilly's library), and classic computer science books that you can help select.
"
2005,"Rescuing our families, our neighbors, and ourselves.","Should computer scientists and engineers take on a greater responsibility to help reduce the loss of life and property damage from natural disasters?
"
2005,Robots in the desert: a research parable for our times.,"DARPA's grand challenge spurred advances in the application of AI and, ironically, acted as a referendum on its de-emphasis on academic research.
"
2005,Computing research: a looming crisis.,"On June 11, Vint Cerf and Bob Kahn received computing's highest prize, the A.M. Turing Award, from the Association for Computing Machinery. Their Transmission Control Protocol (TCP), created in 1973, became the language of the Internet.In the May 6 issue of Science [1], we used this as the ""news hook"" for an invited editorial on the current state of computer science research in the United States. ""Where will the next generation of groundbreaking innovations in IT arise?"" we asked. ""Where will the Turing Awardees 30 years hence reside?"" Our conclusion: ""Given current trends, the answers to both questions will likely be 'not in the United States.'""We take this opportunity to explore in greater depth the issues we raised in that editorial. What are the trends that concern us? What can all of us, as computer scientists, do to reverse them?
"
2005,Guest Editors' Introduction: Approaches to Recovery-Oriented Computing.,"Abstract:
Given that hardware fails, software has bugs, and human operators make mistakes, researchers must increasingly consider recovery-oriented approaches to dependability. The articles in this issue's theme section describe how a range of techniques based on these perspectives canaugment and complement other efforts to improve dependability."
2005,Stop whining about outsourcing!,"I'm sick of hearing all the whining about how outsourcing is going to migrate all IT jobs to the country with the lowest wages.
"
2005,Crash Data Collection: A Windows Case Study.,"Abstract:
Reliability is a rapidly growing concern in contemporary personal computer (PC) industry, both for computer users as well as product developers. To improve dependability, systems designers and programmers must consider failure and usage data for operating systems as well as applications. In this paper, we discuss our experience with crash and usage data collection for Windows machines. We analyze results based on crashes in the UC Berkeley EECS department."
2005,Control Considerations for Scalable Event Processing.,"Abstract
The growth in the scale of systems and networks has created many challenges for their management, especially for event processing. Our premise is that scaling event processing requires parallelism. To this end, we observe that event processing can be divided into intra-event processing such as filtering and inter-event processing such as root cause analysis. Since intra-event processing is easily parallelized, we propose an architecture in which intra-event processing elements (IAPs) are replicated to scale to larger event input rates. We address two challenges in this architecture. First, the IAPs are subject to overloads that require effective flow control, a capability that was not present in the components we used to build IAPs. Second, we need to balance the loads on IAPs to avoid creating resource bottlenecks. These challenges are further complicated by the presence of disturbances such as CPU intensive administrative tasks that reduce event processing rates. We address these challenges using designs based on control theory, a technique for analyzing stability, accuracy, and settling times. We demonstrate the effectiveness of our approaches with testbed experiments that include a disturbance in the form of a CPU intensive application."
2005,Design and implementation tradeoffs for wide-area resource discovery.,"Abstract:
This paper describes the design and implementation of SWORD, a scalable resource discovery service for wide-area distributed systems. In contrast to previous systems, SWORD allows users to describe desired resources as a topology of interconnected groups with required intragroup, intergroup, and per-node characteristics, along with the utility that the application derives from various ranges of values of those characteristics. This design gives users the flexibility to find geographically distributed resources for applications that are sensitive to both node and network characteristics, and allows the system to rank acceptable configurations based on their quality for that application. We explore a variety of architectures to deliver SWORD's functionality in a scalable and highly-available manner. A 1000-node ModelNet evaluation using a workload of measurements collected from PlanetLab shows that an architecture based on 4-node server cluster sites at network peering facilities outperforms a decentralized DHT-based resource discovery infrastructure for all but the smallest number of sites. While such a centralized architecture shows significant promise, we find that our decentralized implementation, both in emulation and running continuously on over 200 PlanetLab nodes, performs well while benefiting from the DHT's self-healing properties."
2005,Combining Visualization and Statistical Analysis to Improve Operator Confidence and Efficiency for Failure Detection and Localization.,"Abstract:
Web applications suffer from software and configuration faults that lower their availability. Recovering from failure is dominated by the time interval between when these faults appear and when they are detected by site operators. We introduce a set of tools that augment the ability of operators to perceive the presence of failure: an automatic anomaly detector scours HTTP access logs to find changes in user behavior that are indicative of site failures, and a visualizer helps operators rapidly detect and diagnose problems. Visualization addresses a key question of autonomic computing of how to win operators' confidence so that new tools will be embraced. Evaluation performed using HTTP logs from Ebates.com demonstrates that these tools can enhance the detection of failure as well as shorten detection time. Our approach is application-generic and can be applied to any Web application without the need for instrumentation"
2005,Latency Lags Bandwidth.,"Abstract:
Summary form only given. As I review performance trends, I am struck by a consistent theme across many technologies over many years: bandwidth improves much more quickly than latency for four different technologies: disks, networks, memories and processors. A rule of thumb to quantify the imbalance is: bandwidth improves by more than the square of the improvement in latency. This paper lists a half-dozen performance milestones to document this observation, many reasons why it happens, a few ways to cope with it, and two small examples of how you might design systems differently if you kept this simple rule of thumb in mind."
2004,Hot links.,n/a
2004,Latency lags bandwith.,"As I review performance trends, I am struck by a consistent theme across many technologies: bandwidth improves much more quickly than latency. Here, I list a half-dozen performance milestones to document this observation, many reasons why it happens, a few ways to cope with it, a rule of thumb to quantify it, plus an example of how to design systems differently based on this observation.
"
2004,The health of research conferences and the dearth of big idea papers.,n/a
2004,Recovery-Oriented Computing: Building Multitier Dependability.,"Abstract:
Building systems to recover fast may be more productive than aiming for systems that never fail. Because recovery is not immune to failure either, the authors advocate multiple lines of defense in managing failures."
2004,Experience with Evaluating Human-Assisted Recovery Processes.,"Abstract:
We describe an approach to quantitatively evaluating human-assisted failure-recovery tools and processes in the environment of modern Internetand enterprise-class server systems. Our approach can quantify the dependability impact of a single recovery system, and also enables comparisons between different recovery approaches. The approach combines aspects of dependability benchmarking with human user studies, incorporating human participants in the system evaluations yet still producing typical dependability-related metrics as results. We illustrate our methodology via a case study of a system-wide undo/redo recovery tool for e-mail services; our approach is able to expose the dependability benefits of the tool as well as point out areas where its behavior could use improvement."
2004,Path-Based Failure and Evolution Management.,"We present a new approach to managing failures and evolution in large, complex distributed systems using runtime paths. We use the paths that requests follow as they move through the system as our core abstraction, and our ""macro"" approach focuses on component interactions rather than the details of the components themselves. Paths record component performance and interactions, are user- and request-centric, and occur in sufficient volume to enable statistical analysis, all in a way that is easily reusable across applications. Automated statistical analysis of multiple paths allows for the detection and diagnosis of complex failures and the assessment of evolution issues. In particular, our approach enables significantly stronger capabilities in failure detection, failure diagnosis, impact analysis, and understanding system evolution. We explore these capabilities with three real implementations, two of which service millions of requests per day. Our contributions include the approach; the maintainable, extensible, and reusable architecture; the various statistical analysis engines; and the discussion of our experience with a high-volume production service over several years"
2004,Distributed Resource Discovery on PlanetLab with SWORD.,"We describe SWORD, a decentralized service for resource discovery and service placement. SWORD finds the optimal embedding of a user's resource request (expressed as a topology of interconnected groups of nodes with per-node, inter-node, and inter-group constraints expressed as utility functions) in the topology of available nodes. SWORD consists of two parts: a multi-attribute distributed range query engine built on top of a distributed hashtable, and an ""optimizer"" whose work is parallelized on a per-query basis. In this paper we focus on SWORD's PlanetLab deployment and the lessons we have learned from that deployment. We find generally acceptable performance for our decentralized implementation, although we find that even a very small (two-node) ``centralized'' solution offers superior performance, in terms of median distributed query latency, for a reporting and querying node population the size of PlanetLab. Our deployment has led to qualitative observations on the usefulness of a DHT as a service building block, the benefits and dangers of automatic application restart, and the benefits of exporting a simple external interface that is semantically close to the service's internal representation of user queries.
"
2004,Combining statistical monitoring and predictable recovery for self-management.,"Complex distributed Internet services form the basis not only of e-commerce but increasingly of mission-critical network-based applications. What is new is that the workload and internal architecture of three-tier enterprise applications presents the opportunity for a new approach to keeping them running in the face of many common recoverable failures. The core of the approach is anomaly detection and localization based on statistical machine learning techniques. Unlike previous approaches, we propose anomaly detection and pattern mining not only for operational statistics such as mean response time, but also for structural behaviors of the system---what parts of the system, in what combinations, are being exercised in response to different kinds of external stimuli. In addition, rather than building baseline models a priori, we extract them by observing the behavior of the system over a short period of time during normal operation. We explain the necessary underlying assumptions and why they can be realized by systems research, report on some early successes using the approach, describe benefits of the approach that make it competitive as a path toward self-managing systems, and outline some research challenges. Our hope is that this approach will enable ""new science"" in the design of self-managing systems by allowing the rapid and widespread application of statistical learning theory techniques (SLT) to problems of system dependability.
"
2003,A Linear Programming Approach to Discriminant Analysis with a Reserved-Judgment Region.,"A linear-programming model is proposed for deriving discriminant rules that allow allocation of entities to a reserved-judgment region. The size of the reserved-judgment region, which can be controlled by varying parameters within the model, dictates the level of aggressiveness (cautiousness) of allocating (misallocating) entities to groups. Results of simulation experiments for various configurations of normal and contaminated normal three-group populations are reported for a variety of parameter selections. Results of cross-validation experiments using real data sets are also reported. Both the simulation and cross-validation experiments include comparison with other discriminant analysis techniques. The results demonstrate that the proposed model is useful for deriving discriminant rules that reduce the chances of misclassification, while maintaining a reasonable level of correct classification.
"
2003,Scalable Vector Processors for Embedded Systems.,"Abstract:
For embedded applications with data-level parallelism, a vector processor offers high performance at low power consumption and low design complexity. Unlike superscalar and VLIW designs, a vector processor is scalable and can optimally match specific application requirements.To demonstrate that vector architectures meet the requirements of embedded media processing, we evaluate the Vector IRAM, or VIRAM (pronounced ""V-IRAM""), architecture developed at UC Berkeley, using benchmarks from the Embedded Microprocessor Benchmark Consortium (EEMBC). Our evaluation covers all three components of the VIRAM architecture: the instruction set, the vectorizing compiler, and the processor microarchitecture. We show that a compiler can vectorize embedded tasks automatically without compromising code density. We also describe a prototype vector processor that outperforms high-end superscalar and VLIW designs by 1.5x to 100x for media tasks, without compromising power consumption. Finally, we demonstrate that clustering and modular design techniques let a vector processor scale to tens of arithmetic data paths before wide instruction-issue capabilities become necessary."
2003,Overcoming the Limitations of Conventional Vector Processors.,"Despite their superior performance for multimedia applications, vector processors have three limitations that hinder their widespread acceptance. First, the complexity and size of the centralized vector register file limits the number of functional units. Second, precise exceptions for vector instructions are difficult to implement. Third, vector processors require an expensive on-chip memory system that supports high bandwidth at low access latency. This paper introduces CODE, a scalable vector microarchitecture that addresses these three shortcomings. It is designed around a clustered vector register file and uses a separate network for operand transfers across functional units. With extensive use of decoupling, it can hide the latency of communication across functional units and provides 26% performance improvement over a centralized organization. CODE scales efficiently to 8 functional units without requiring wide instruction issue capabilities. A renaming table makes the clustered register file transparent at the instruction set level. Renaming also enables precise exceptions for vector instructions at a performance loss of less than 5%. Finally, decoupling allows CODE to tolerate large increases in memory latency at sub-linear performance degradation without using on-chip caches. Thus, CODE can use economical, off-chip, memory systems."
2003,Undo for Operators: Building an Undoable E-mail Store.,"System operators play a critical role in maintaining server dependability yet lack powerful tools to help them do so. To help address this unfulfilled need, we describe Operator Undo, a tool that provides a forgiving operations environment by allowing operators to recover from their own mistakes, from unanticipated software problems, and from intentional or accidental data corruption. Operator Undo starts by intercepting and logging user interactions with a network service before they enter the system, creating a record of user intent. During an undo cycle, all system hard state is physically rewound, allowing the operator to perform arbitrary repairs; after repairs are complete, lost user data is reintegrated into the repaired system by replaying the logged user interactions while tracking and compensating for any resulting externally-visible inconsistencies. We describe the design and implementation of an application-neutral framework for Operator Undo, and detail the process by which we instantiated the framework in the form of an undo-capable e-mail store supporting SMTP mail delivery and IMAP mail retrieval. Our proof-of-concept e-mail implementation imposes only a small performance overhead, and can store days or weeks of recovery log on a single disk."
2003,"Why Do Internet Services Fail, and What Can Be Done About It?","In 1986 Jim Gray published his landmark study of the causes of failures of Tandem systems and the techniques Tandem used to prevent such failures See J. Gray. Why do computers stop and what can be done about it? Symposium on Reliability in Distributed Software and Database Systems, 1986... Seventeen years later, Internet services have replaced fault-tolerant servers as the new kid on the 24x7-availability block. Using data from three large-scale Internet services, we analyzed the causes of their failures and the (potential) effectiveness of various techniques for preventing and mitigating service failure. We find that (1) operator error is the largest single cause of failures in two of the three services, (2) operator errors often take a long time to repair, (3) configuration errors are the largest category of operator errors, (4) failures in custom-written front-end software are significant, and (5) more extensive online testing and more thoroughly exposing and detecting component failures would reduce failure rates in at least one service. Qualitatively we find that improvement in the maintenance tools and systems used by service operations staff would decrease time to diagnose and repair problems."
2002,Architecture and Dependability of Large-Scale Internet Services.,"Abstract:
The popularity of large-scale Internet infrastructure services such as AOL, Google, and Hotmail has grown enormously. The scalability and availability requirements of these services have led to system architectures that diverge significantly from those of traditional systems like desktops, enterprise servers, or databases. Given the need for thousands of nodes, cost necessitates the use of inexpensive personal computers wherever possible, and efficiency often requires customized service software. Likewise, addressing the goal of zero downtime requires human operator involvement and pervasive redundancy within clusters and between globally distributed data centers. Despite these services' success, their architectures-hardware, software, and operational-have developed in an ad hoc manner that few have surveyed or analyzed. Moreover, the public knows little about why these services fail or about the operational practices used in an attempt to keep them running 24/7. As a first step toward formalizing the principles for building highly available and maintainable large-scale Internet services, we are surveying existing services' architectures and dependability. This article describes our observations to date."
2002,ROC-1: Hardware Support for Recovery-Oriented Computing.,"We introduce the ROC-1 hardware platform, a large-scale cluster system designed to provide high availability for Internet service applications. The ROC-1 prototype embodies our philosophy of recovery-oriented computing (ROC) by emphasizing detection and recovery from the failures that inevitably occur in Internet service environments, rather than simple avoidance of such failures. ROC-1 promises greater availability than existing server systems by incorporating four techniques applied from the ground up to both hardware and software: redundancy and isolation, online self-testing and verification, support for problem diagnosis and concern for human interaction with the system."
2002,An Introduction to Dependability.,n/a
2002,SIGARCH Conference Guidelines.,n/a
2002,Recovery Oriented Computing: A New Research Agenda for a New Century.,"Summary form only given, as follows. After 15 years of successfully improving cost-performance, it's time for new challenges for the systems research community. As a result of the focus on cost-performance, the fabled five 9s of availability (99.999% uptime) looks to be much easier to achieve in advertising than in computers, and the cost of managing systems can be five times the cost of the hardware. In a Post-PC Era of wireless gadgets using services on the Internet, one new challenge is building services that really are dependable and much less expensive to maintain. Traditional Fault-Tolerant Computing concentrates on tolerating hardware and operating system faults, ignoring faults by human operators and even applications. Recovery Oriented Computing (ROC) aims at improving Mean Time To Recover to both lower the cost of management and improve at the availability of whole system, including the people who operate it. We look to civil engineering and diplomacy to inspire principles for ROC design. This talk outlines motivation for and proposed principles of ROC design, plus some concrete results in the area of benchmarking of availability."
2002,A Simple Way to Estimate the Cost of Downtime.,"Systems that are more dependable and less expensive to maintain may be more expensive to purchase. If ordinary customers cannot calculate the costs of downtime, such systems may not succeed because it will be difficult to justify a higher price. Hence, we propose an easy-to-calculate estimate of downtime.

As one reviewer commented, the cost estimate we propose ``is simply a symbolic translation of the most obvious, common sense approach to the problem.'' We take this remark as a complement, noting that prior work has ignored pieces of this obvious formula.

We introduce this formula, argue why it will be important to have a formula that can easily be calculated, suggest why it will be hard to get a more accurate estimate, and give some examples.

Widespread use of this obvious formula can lay a foundation for systems that reduce downtime."
2002,Vector vs. superscalar and VLIW architectures for embedded multimedia benchmarks.,"Multimedia processing on embedded devices requires an architecture that leads to high performance, low power consumption, reduced design complexity, and small code size. In this paper, we use EEMBC, an industrial benchmark suite, to compare the VIRAM vector architecture to superscalar and VLIW processors for embedded multimedia applications. The comparison covers the VIRAM instruction set, vectorizing compiler and the prototype chip that integrates a vector processor with DRAM main memory. We demonstrate that executable code for VIRAM is up to 10 times smaller than VLIW code and comparable to /spl times/86 CISC code. The simple, cache-less VIRAM chip is 2 times faster than a 4-way superscalar RISC processor that uses a 5 times faster clock frequency and consumes 10 times more power VIRAM is also 10 times faster than cache-based VLIW processors. Even after manual optimization of the VLIW code and insertion of SIMD and DSP instructions, the single-issue VIRAM processor is 60% faster than 5-way to 8-way VLIW designs."
2002,"Rewind, repair, replay: three R's to dependability.","Motivated by the growth of web and infrastructure services and their susceptibility to human operator-related failures, we introduce system-level undo as a recovery mechanism designed to improve service dependability. Undo enables system operators to recover from their inevitable mistakes and furthermore enables retroactive repair of problems that were not fixed quickly enough to prevent detrimental effects. We present the ""three R's"", a model of undo that matches the needs of human error recovery and retroactive repair; discuss several of the issues raised by this undo model; and introduce an initial architectural framework for undoable systems using the example of an undoable e-mail service system.
"
2002,Studying and using failure data from large-scale internet services.,"Large-scale Internet services are the newest and arguably the most commercially important class of systems requiring 24x7 availability. As a result, very little information has been published about their causes of failure. In an attempt to address this deficiency, we have analyzed detailed failure reports from three large-scale Internet services. Our goals are to (1) identify the major factors contributing to user-visible failures, (2) evaluate the (potential) effectiveness of various techniques for preventing and mitigating service failure, and (3) build a fault model for service-level dependability and recovery benchmarks. Our initial results indicate that operator error and network problems are the leading contributors to user-visible failures, that failures in custom-written front-end software are significant, and that online testing and more thoroughly exposing and handling component failures would reduce failure rates in at least one service.
"
2000,The Art of Massive Storage: A Web Image Archive.,"Museums can make their entire collections available to the world via the Internet. The Thinker ImageBase, the San Francisco Fine Arts Museums' online art image database, demonstrates issues involved in managing large storage systems and delivering their contents to users."
2000,Ideal bootstrap estimation of expected prediction error for k-nearest neighbor classifiers: Applications for classification and error assessment.,"Euclidean distance k-nearest neighbor (k-NN) classifiers are simple nonparametric classification rules. Bootstrap methods, widely used for estimating the expected prediction error of classification rules, are motivated by the objective of calculating the ideal bootstrap estimate of expected prediction error. In practice, bootstrap methods use Monte Carlo resampling to estimate the ideal bootstrap estimate because exact calculation is generally intractable. In this article, we present analytical formulae for exact calculation of the ideal bootstrap estimate of expected prediction error for k-NN classifiers and propose a new weighted k-NN classifier based on resampling ideas. The resampling-weighted k-NN classifier replaces the k-NN posterior probability estimates by their expectations under resampling and predicts an unclassified covariate as belonging to the group with the largest resampling expectation. A simulation study and an application involving remotely sensed data show that the resampling-weighted k-NN classifier compares favorably to unweighted and distance-weighted k-NN classifiers.
"
2000,Exploiting On-Chip Memory Bandwidth in the VIRAM Compiler.,"Many architectural ideas that appear to be useful from a hardware standpoint fail to achieve wide acceptance due to lack of compiler support. In this paper we explore the design of the VIRAM architecture from the perspective of compiler writers, describing some of the code generation problems that arise in VIRAM and their solutions in the VIRAM compiler. VIRAM is a single chip system designed primarily for multimedia. It combines vector processing with mixed logic and DRAM to achieve high performance with relatively low energy, area, and design complexity. The paper focuses on two aspects of the VIRAM compiler and architecture. The first problem is to take advantage of the on-chip bandwidth for memory-intensive applications, including those with non-contiguous or unpredictable memory access patterns. The second problem is to support that kinds of narrow data types that arise in media processing, including processing of 8 and 16-bit data.
"
2000,Towards Availability Benchmarks: A Case Study of Software RAID Systems.,"Benchmarks have historically played a key role in guiding the progress of computer science systems research and development, but have traditionally neglected the areas of availability, maintainability, and evolutionary growth, areas that have recently become critically important in high-end system design. As a first step in addressing this deficiency, we introduce a general methodology for benchmarking the availability of computer systems. Our methodology uses fault injection to provoke situations where availability may be compromised, leverages existing performance benchmarks for workload generation and data collection, and can produce results in both detail-rich graphical presentations or in distilled numerical summaries. We apply the methodology to measure the availability of the software RAID systems shipped with Linux, Solaris 7 Server, and Windows 2000 Server, and find that the methodology is powerful enough not only to quantify the impact of various failure conditions on the availability of these systems, but also to unearth their design philosophies with respect to transient errors and recovery policy."
1999,ISTORE: Introspective Storage for Data-Intensive Network Services.,"Today's fast-growing data-intensive network services place heavy demands on the back-end servers that support them. This paper introduces ISTORE, a novel server architecture that couples LEGO-like plug-and-play hardware with a generic framework for constructing adaptive software that leverages continuous self-monitoring. ISTORE exploits introspection to provide high availability, performance, and scalability while drastically reducing the cost and complexity of administration. An ISTORE-based server monitors and adapts to changes in the imposed workload and to unexpected system events such as hardware failure. This adaptability is enabled by a combination of intelligent self-monitoring hardware components and an extensible software framework that allows the target application to specify monitoring and adaptation policies to the system."
1999,Cluster I/O with River: Making the Fast Case Common.,"We introduce River, a data-flow programming environment and I/O substrate for clusters of computers. River is designed to provide maximum performance in the common case ó even in the face of nonuniformities in hardware, software, and workload. River is based on two simple design features: a high-performance distributed queue, and a storage redundancy mechanism called graduated declustering. We have implemented a number of data-intensive applications on River, which validate our design with near-ideal performance in a variety of non-uniform performance scenarios."
1999,A Retrospective on Twelve Years of LISA Proceedings.,"We examine two models for categorizing tasks performed by system administrators. The first model is the traditional task based model. The second model breaks tasks down by the source of the problem. We then look at the historical trends of the last 12 years of LISA proceedings based on these models. Finally, we analyze some of the more important tasks done by system administrators and propose future research in those areas. Our hope is that some of the academic rigor in analyzing research can be brought to systems administration without losing the practicality that makes the research valuable.s"
1999,Usage Patterns of a Web-Based Image Collection.,"This paper presents a study of user access patterns to a large, Web-based, image collection. The images are the entire collection of the Fine Arts Museums of San Francisco, the largest on-line collection of high resolution art images in the world. The images are served using a tile-based solution that allows a user to zoom-in and navigate within an image. We studied five months of web log data for this collection. Our analysis revealed the following: less than 10% of all available documents on the site were accessed in the five month period and document popularity appears to follow a Zipf distribution. Also, images have interesting areas which are viewed more than others, some image resolutions are viewed far more than others, and user navigation patterns vary between resolutions and are sensitive to download time. The paper discusses these results and their implications for the design of caches and archival storage systems to support this type of workload."
1999,Designing a Self-Maintaining Storage System.,"This paper shows the suitability of a ""self-maintaining"" approach to Tertiary Disk, a large-scale disk array system built from commodity components. Instead of incurring the cost of custom hardware, we attempt to solve various problems by design and software. We have built a cluster of storage nodes connected by switched Ethernet. Each storage node is a PC hosting a few dozen SCSI disks, running the FreeBSD operating system. The system is used as a web-based image server for the Zoom Project in cooperation with the Fine Arts Museums of San Francisco (http://www.thinker.org/). We are designing a self-maintenance extension to the OS to run on this cluster to mitigate the system administrator's burden. There are several components required for building a self-maintaining system. One is decoupling the time of failure from the time of hardware replacement. This implies the system must have some amount of redundancy, and has no single point of failure. Our system is fully redundant, and everything is constructed to avoid a single point of failure. Another is correctly identifying failures and their dependencies. The paper also outlines several approaches to lower the human cost of system administration of such a system and making the system as autonomous as possible."
1999,Virtual Log Based File Systems for a Programmable Disk.,"In this paper, we study how to minimize the latency of small transactional writes to disk. The basic approach is to write to free sectors that are near the current disk head location by leveraging the embedded processor core inside the disk. We develop a number of analytical models to demonstrate the performance potential of this approach. We then present the design of a variation of a log-structured file system based on the concept of a virtual log, which supports fast small transactional writes without extra hardware support. We compare our approach against traditional update-in-place and logging systems by modifying the Solaris kernel to serve as a simulation engine. Our evaluations show that random synchronous updates on an unmodified UFS execute up to an order of magnitude faster on a virtual log than on a conventional disk. The virtual log can also significantly improve LFS in cases where delaying small writes is not an option or on-line cleaning would degrade performance. If the current trends of disk technology continue, we expect the performance advantage of this approach to become even more pronounced in the future."
1998,A New Direction for Computer Architecture Research.,"In the past few years, two important trends have evolved that could change the shape of computing: multimedia applications and portable electronics. Together, these trends will lead to a personal mobile-computing environment, a small device carried all the time that incorporates the functions of the pager, cellular phone, laptop computer, PDA, digital camera, and video game. The microprocessor needed for these devices is actually a merged general-purpose processor and digital-signal processor, with the power budget of the latter. Yet for almost two decades, architecture research has focused on desktop or server machines. We are designing processors of the future with a heavy bias toward the past. To design successful processor architectures for the future, we first need to explore future applications and match their requirements in a scalable, cost-effective way. The authors describe Vector IRAM, an initial approach in this direction, and challenge others in the very successful computer architecture community to investigate architectures with a heavy bias for the future."
1998,A Case for Intelligent Disks (IDISKs).,"Decision support systems (DSS) and data warehousing workloads comprise an increasing fraction of the database market today. I/O capacity and associated processing requirements for DSS workloads are increasing at a rapid rate, doubling roughly every nine to twelve months [38]. In response to this increasing storage and computational demand, we present a computer architecture for decision support database servers that utilizes ìintelligentî disks (IDISKs). IDISKs utilize low-cost embedded general-purpose processing, main memory, and high-speed serial communication links on each disk. IDISKs are connected to each other via these serial links and high-speed crossbar switches, overcoming the I/O bus bottleneck of conventional systems. By off-loading computation from expensive desktop processors, IDISK systems may improve cost-performance. More importantly, the IDISK architecture allows the processing of the system to scale with increasing storage demand.
"
1998,"The Architectural Costs of Streaming I/O: A Comparison of Workstations, Clusters, and SMPs.","We investigate resource usage while performing streaming I/O by contrasting three architectures, a single workstation, a cluster, and an SMP, under various I/O benchmarks. We derive analytical and empirically-based models of resource usage during data transfer, examining the I/O bus, memory bus, network, and processor of each system. By investigating each resource in detail, we assess what comprises a well-balanced system for these workloads. We find that the architectures we study are not well balanced for streaming I/O applications. Across the platforms, the main limitation to attaining peak performance is the CPU, due to lack of data locality. Increasing processor performance (especially with improved block operation performance) will be of great aid for these workloads in the future. For a cluster workstation, the I/O bus is a major system bottleneck, because of the increased load placed on it from network communication. A well-balanced cluster workstation should have copious I/O bus bandwidth, perhaps via multiple I/O busses. The SMP suffers from poor memory-system performance; even when there is true parallelism in the benchmark, contention in the shared-memory system leads to reduced performance. As a result, the clustered workstations provide higher absolute performance for streaming I/O workloads."
1998,Retrospective: A Retrospective on High-Level Language Computer Architecture.,n/a
1998,Performance Characterization of a Quad Pentium Pro SMP using OLTP Workloads.,"Commercial applications are an important, yet often overlooked, workload with significantly different characteristics from technical workloads. The potential impact of these differences is that computers optimized for technical workloads may not provide good performance for commercial applications, and these applications may not fully exploit advances in processor design. To evaluate these issues, we use hardware counters to measure architectural features of a four-processor Pentium Pro-based server running a TPC-C-like workload on an Informix database. We examine the effectiveness of out-of-order execution, branch prediction, speculative execution, superscalar issue and retire, caching and multiprocessor scaling. We find that out-of-order execution, superscalar issue and retire, and branch prediction are not as effective for database workloads as they are for technical workloads, such as SPEC. We find that caches are effective at reducing processor traffic to memory; even larger caches would be helpful to satisfy more data requests. Multiprocessor scaling of this workload is good, but even modest bus utilization degrades application memory latency, limiting database throughput."
1998,Retrospective: RISC I: A Reduced Instruction Set Computer.,n/a
1998,Retrospective on High-Level Language Computer Architecture.,"High-level language computers (HLLC) have attracted interest in the architectural and programming community during the last 15 years; proposals have been made for machines directed towards the execution of various languages such as ALGOL, 1,2 APL, 3,4,5 BASIC, 6,7 COBOL, 8,9 FORTRAN, 10,ll LISP, 12,13 PASCAL, 14 PL/I, 15,16,17 SNOBOL, 18,19 and a host of specialized languages. Though numerous designs have been proposed, only a handful of high-level language computers have actually been implemented. 4,7,9,20,21 In examining the goals and successes of high-level language computers, the authors have found that most designs suffer from fundamental problems stemming from a misunderstanding of the issues involved in the design, use, and implementation of cost-effective computer systems. It is the intent of this paper to identify and discuss several issues applicable to high-level language computer architecture, to provide a more concrete definition of high-level language computers, and to suggest a direction for high-level language computer architectures of the future."
1998,RISC I: A Reduced Instruction Set VLSI Computer.,"The Reduced Instruction Set Computer (RISC) Project investigates an alternative to the general trend toward computers with increasingly complex instruction sets: With a proper set of instructions and a corresponding architectural design, a machine with a high effective throughput can be achieved. The simplicity of the instruction set and addressing modes allows most instructions to execute in a single machine cycle, and the simplicity of each instruction guarantees a short cycle time. In addition, such a machine should have a much shorter design time. This paper presents the architecture of RISC I and its novel hardware support scheme for procedure call/return. Overlapping sets of register banks that can pass parameters directly to subroutines are largely responsible for the excellent performance of RISC I. Static and dynamic comparisons between this new architecture and more traditional machines are given. Although instructions are simpler, the average length of programs was found not to exceed programs for DEC VAX 11 by more than a factor of 2. Preliminary benchmarks demonstrate the performance advantages of RISC. It appears possible to build a single chip computer faster than VAX 11/780."
1997,Constrained discriminant analysis via 0/1 mixed integer programming.,"A nonlinear 0/1 mixed integer programming model is presented for a constrained discriminant analysis problem. The model places restrictions on the numbers of misclassifications allowed among the training entities, and incorporates a ""reserved judgment"" region to which entities whose classifications are difficult to determine may be allocated. Two linearizations of the model are given one heuristic and one exact. Numerical results from real-world machine-learning datasets are presented.
"
1997,Scalable Processors in the Billion-Transistor Era: IRAM.,"Members of the University of California, Berkeley, argue that the memory system will be the greatest inhibitor of performance gains in future architectures. Thus, they propose the intelligent RAM or IRAM. This approach greatly increases the on-chip memory capacity by using DRAM technology instead of much less dense SRAM memory cells. The resultant on-chip memory capacity coupled with the high bandwidths available on chip should allow cost-effective vector processors to reach performance levels much higher than those of traditional architectures. Although vector processors require explicit compilation, the authors claim that vector compilation technology is mature (having been used for decades in supercomputers), and furthermore, that future workloads will contain more heavily vectorizable components."
1997,A case for intelligent RAM.,"Two trends call into question the current practice of fabricating microprocessors and DRAMs as different chips on different fabrication lines. The gap between processor and DRAM speed is growing at 50% per year; and the size and organization of memory on a single DRAM chip is becoming awkward to use, yet size is growing at 60% per year. Intelligent RAM, or IRAM, merges processing and memory into a single chip to lower memory latency, increase memory bandwidth, and improve energy efficiency. It also allows more flexible selection of memory size and organization, and promises savings in board area. This article reviews the state of microprocessors and DRAMs today, explores some of the opportunities and challenges for IRAMs, and finally estimates performance and energy efficiency of three IRAM designs."
1997,A new voting based hardware data prefetch scheme.,"The dramatic increase in the processor memory gap in recent years has led to the development of techniques like data prefetching that hide the latency of cache misses. Two such hardware techniques are the stream buffer and the stride predictor. They have dissimilar architectures, are effective for different kinds of memory access patterns and require different amounts of extra memory bandwidth. We compare the performance of these two techniques and propose a scheme that unifies them. Simulation studies on six benchmark programs confirm that the combined scheme is more effective in reducing the average memory access time (AMAT) than either of the two individually."
1997,"Intelligent RAM (IRAM): The Industrial Setting, Applications and Architectures.","The goal of intelligent RAM (IRAM) is to design a cost-effective computer by designing a processor in a memory fabrication process, instead of in a conventional logic fabrication process, and include memory on-chip. To design a processor in a DRAM process one must learn about the business and culture of the DRAMs, which is quite different from microprocessors. The authors describe some of those differences and their current vision of IRAM applications, architectures, and implementations."
1997,The Energy Efficiency of IRAM Architectures.,"Portable systems demand energy efficiency in order to maximize battery life. IRAM architectures, which combine DRAM and a processor on the same chip in a DRAM process, are more energy efficient than conventional systems. The high density of DRAM permits a much larger amount of memory on-chip than a traditional SRAM cache design in a logic process. This allows most or all IRAM memory accesses to be satisfied on-chip. Thus there is much less need to drive high-capacitance off-chip buses, which contribute significantly to the energy consumption of a system. To quantify this advantage we apply models of energy consumption in DRAM and SRAM memories to results from cache simulations of applications reflective of personal productivity tasks on low power systems. We find that IRAM memory hierarchies consume as little as 22% of the energy consumed by a conventional memory hierarchy for memory-intensive applications, while delivering comparable performance. Furthermore, the energy consumed by a system consisting of an IRAM memory hierarchy combined with an energy efficient CPU core is as little as 40% of that of the same CPU core with a traditional memory hierarchy.
"
1997,"Extensible, Scalable Monitoring for Clusters of Computers.","We describe the CARD (Cluster Administration using Relational Databases) system for monitoring large clusters of cooperating computers. CARD scales both in capacity and in visualization to at least 150 machines, and can in principle scale far beyond that. The architecture is easily extensible to monitor new cluster software and hardware. CARD detects and automatically recovers from common faults. CARD uses a Java applet as its primary interface allowing users anywhere in the world to monitor the cluster through their browser.
"
1997,High-Performance Sorting on Networks of Workstations.,"We report the performance of NOW-Sort, a collection of sorting implementations on a Network of Workstations (NOW). We find that parallel sorting on a NOW is competitive to sorting on the large-scale SMPs that have traditionally held the performance records. On a 64-node cluster, we sort 6.0 GB in just under one minute, while a 32-node cluster finishes the Datamation benchmark in 2.41 seconds.

Our implementations can be applied to a variety of disk, memory, and processor configurations; we highlight salient issues for tuning each component of the system. We evaluate the use of commodity operating systems and hardware for parallel sorting. We find existing OS primitives for memory management and file access adequate. Due to aggregate communication and disk bandwidth requirements, the bottleneck of our system is the workstation I/O bus."
1996,LogP: A Practical Model of Parallel Computation.,n/a
1996,A Microcelebration.,n/a
1996,Serverless Network File Systems.,"We propose a new paradigm for network file system design: serverless network file systems . While traditional network file systems rely on a central server machine, a serverless system utilizes workstations cooperating as peers to provide all file system services. Any machine in the system can store, cache, or control any block of data. Our approach uses this location independence, in combination with fast local area networks, to provide better performance and scalability than traditional file systems. Furthermore, because any machine in the system can assume the responsibilities of a failed component, our serverless design also provides high availability via redundatn data storage. To demonstrate our approach, we have implemented a prototype serverless network file system called xFS. Preliminary performance measurements suggest that our architecture achieves its goal of scalability. For instance, in a 32-node xFS system with 32 active clients, each client receives nearly as much read or write throughput as it would see if it were the only active client."
1995,A Case for NOW (Networks Of Workstations).,"Networks of workstations are poised to become the primary computing infrastructure for science and engineering. NOWs may dramatically improve virtual memory and file system performance; achieve cheap, highly available, and scalable file storage: and provide multiple CPUs for parallel computing. Hurdles that remain include efficient communication hardware and software, global coordination of multiple workstation operating systems, and enterprise-scale network file systems. Our 100-node NOW prototype aims to demonstrate practical solutions to these challenges.< >"
1995,Truth in SPEC benchmarks.,"The System Performance Evaluation Cooperative (SPEC) benchmarks are a set of integer and floating-point programs that are intended to be ìeffective and fair in comparing the performance of high performance computing systemsî. SPEC ratings are often quoted in company advertising and have been trusted as the de facto measure of comparison for computer systems. Recently, there has been some concern regarding the fairness and the value of these benchmarks for comparing computer systems.

In this paper we investigate the following two questions regarding the SPEC92 benchmark suite: 1) How sensitive are the SPEC ratings to various tunings? 2) How reproducible are the published results? For six vendors, we compare the published SPECpeak and SPECbase ratings, and observe an 11% average improvement in the SPECpeak ratings due to changes in the compiler flags alone. In our own attempt to reproduce the published SPEC ratings, we came across various ìexplicitî and ìhiddenî tuning parameters that we consider unrealistic. We suggest a new unit called SPECsimple that requires using only the -O compiler optimization flag, shared libraries, and standard system configuration. SPECsimple is designed to better match the performance experienced by a typical user. Our measured SPECsimples are 65-86% of the advertised SPECpeak performance. We conclude this paper by citing cases compiler optimizations specifically designed for SPEC programs, in which performance decreases drastically or the computed results are incorrect if the compiled program does not exactly match the SPEC benchmark program. These findings show that the fairness and value of the popular SPEC benchmarks are questionable."
1995,The Berkeley Networks of Workstations (NOW) Project.,"The paper describes the Berkeley Network of Workstations (NOW) Project. This project leverages the high bandwidth, switch based local area networks with a custom low latency network interface and global layer operating system; this makes a building full of desktop computers act as a single large computer: ""the building is the computer""."
1995,Choosing the Best Storage System for Video Service.,n/a
1995,Storage Systems for Movies-on-Demand Video Servers.,"We evaluate storage system alternatives for movies-on-demand video servers. We begin by characterizing the movies-on-demand workload. We briefly discuss performance in disk arrays. First, we study disk farms in which one movie is stored per disk. This is a simple scheme, but it wastes substantial disk bandwidth, because disks holding less popular movies are underutilized; also, good performance requires that movies be replicated to reflect the user request pattern. Next, we examine disk farms in which movies are striped across disks, and find that striped video servers offer nearly full utilization of the disks by achieving better load balancing. For the remainder of the paper, we concentrate on tertiary storage systems. We evaluate the use of storage hierarchies for video service. These hierarchies include a tertiary library along with a disk farm. We examine both magnetic tape libraries and optical disk jukeboxes. We show that, unfortunately, the performance of neither tertiary system performs adequately as part of a storage hierarchy to service the predicted distribution of movie accesses. We suggest changes to tertiary libraries that would make them better-suited to these applications."
1995,A Case for NOW (Networks of Workstations) - Abstract.,"An apparatus is disclosed for achieving interference-free fluid velocity distributions in the plane of the walls of a subsonic wind tunnel. Adjustable slats having longitudinal baffles therebetween make up the test section walls, excluding the ground plane, and extend upstream and downstream of the vehicle to be tested. Static pressure measurements along the centerline of each slat provide information that enables calculation of the slat contour required to achieve close matching of the streamlines within the tunnel to those occurring on the road. "
1995,The Interaction of Parallel and Sequential Workloads on a Network of Workstations.,"This paper examines the plausibility of using a network of workstations (NOW) for a mixture of parallel and sequential jobs. Through simulations, our study examines issues that arise when combining these two workloads on a single platform. Starting from a dedicated NOW just for parallel programs, we incrementally relax uniprogramming restrictions until we have a multi-programmed, multi-user NOW for both interactive sequential users and parallel programs. We show that a number of issues associated with the distributed NOW environment (e.g., daemon activity, coscheduling skew) can have a small but noticeable effect on parallel program performance. We also find that efficient migration to idle workstations is necessary to maintain acceptable parallel application performance. Furthermore, we present a methodology for deriving an optimal delay time for recruiting idle machines for use by parallel programs; this recruitment threshold was just 3 minutes for the research cluster we measured. Finally, we quantify the effects of the additional parallel load upon interactive users by keeping track of the potential number of user delays in our simulations. When we limit the maximum number of delays per user, we can still maintain acceptable parallel program performance. In summary, we find that for our workloads a 2:1 rule applies: a NOW cluster of approximately 60 machines can sustain a 32-node parallel workload in addition to the sequential load placed upon it by interactive users."
1995,Serverless Network File Systems.,"We propose a new paradigm for network file system design: serverless network file systems . While traditional network file systems rely on a central server machine, a serverless system utilizes workstations cooperating as peers to provide all file system services. Any machine in the system can store, cache, or control any block of data. Our approach uses this location independence, in combination with fast local area networks, to provide better performance and scalability than traditional file systems. Furthermore, because any machine in the system can assume the responsibilities of a failed component, our serverless design also provides high availability via redundatn data storage. To demonstrate our approach, we have implemented a prototype serverless network file system called xFS. Preliminary performance measurements suggest that our architecture achieves its goal of scalability. For instance, in a 32-node xFS system with 32 active clients, each client receives nearly as much read or write throughput as it would see if it were the only active client."
1994,Coding Techniques for Handling Failures in Large Disk Arrays.,"A crucial issue in the design of very large disk arrays is the protection of data against catastrophic disk failures. Although today single disks are highly reliable, when a disk array consists of 100 or 1000 disks, the probability that at least one disk will fail within a day or a week is high. In this paper we address the problem of designing erasure-correcting binary linear codes that protect against the loss of data caused by disk failures in large disk arrays. We describe how such codes can be used to encode data in disk arrays, and give a simple method for data reconstruction. We discuss important reliability and performance constraints of these codes, and show how these constraints relate to properties of the parity check matrices of the codes. In so doing, we transform code design problems into combinatorial problems. Using this combinatorial framework, we present codes and prove they are optimal with respect to various reliability and performance constraints.
"
1994,"RAID: High-Performance, Reliable Secondary Storage.","Disk arrays were proposed in the 1980s as a way to use parallelism between multiple disks to improve aggregate I/O performance. Today they appear in the product lines of most major computer manufacturers. This article gives a comprehensive overview of disk arrays and provides a framework in which to organize current and future work. First, the article introduces disk technology and reviews the driving forces that have popularized disk arrays: performance and reliability. It discusses the two architectural techniques used in disk arrays: striping across multiple disks to improve performance and redundancy to improve reliability. Next, the article describes seven disk array architectures, called RAID (Redundant Arrays of Inexpensive Disks) levels 0ñ6 and compares their performance, cost, and reliability. It goes on to discuss advanced research and implementation topics such as refining the basic RAID levels to improve performance and designing algorithms to maintain data consistency. Last, the article describes six disk array prototypes of products and discusses future opportunities for research, with an annotated bibliography disk array-related literature."
1994,Performance and Design Evaluation of the RAID-II Storage Server.,"RAID-II is a high-bandwidth, network-attached storage server designed and implemented at the University of California at Berkeley. In this paper, we measure the performance of RAID-II and evaluate various architectural decisions made during the design process. We first measure the end-to-end performance of the system to be approximately 20 MB/s for both disk array reads and writes. We then perform a bottleneck analysis by examining the performance of each individual subsystem and conclude that the disk subsystem limits performance. By adding a custom interconnect board with a high-speed memory and bus system and parity engine, we are able to achieve a performance speedup of 8 to 15 over a comparative system using only off-the-shelf hardware.
"
1994,"A New Approach to I/O Performance Evaluation - Self-Scaling I/O Benchmarks, Predicted I/O Performance.","Current I/O benchmarks suffer from several chronic problems: they quickly become obsolete; they do not stress the I/O system; and they do not help much in understanding I/O system performance. We propose a new approach to I/O performance analysis. First, we propose a self-scaling benchmark that dynamically adjusts aspects of its workload according to the performance characteristic of the system being measured. By doing so, the benchmark automatically scales across current and future systems. The evaluation aids in understanding system performance by reporting how performance varies according to each of five workload parameters. Second, we propose predicted performance, a technique for using the results from the self-scaling evaluation to estimate quickly the performance for workloads that have not been measured. We show that this technique yields reasonably accurate performance estimates and argue that this method gives a far more accurate comparative performance evaluation than traditional single-point benchmarks. We apply our new evaluation technique by measuring a SPARCstation 1+ with one SCSI disk, an HP 730 with one SCSI-II disk, a DECstation 5000/200 running the Sprite LFS operating system with a three-disk disk array, a Convex C240 minisupercomputer with a four-disk disk array, and a Solbourne 5E/905 fileserver with a two-disk disk array.
"
1994,Terabytes >> Teraflops or Why Work on Processors When I/O is Where the Action is? (Abstract).,n/a
1994,A case for NOW (networks-of-workstations).,n/a
1994,RAID-II: A High-Bandwidth Network File Server.,"In 1989, the RAID (Redundant Arrays of Inexpensive Disks) group at U.C. Berkeley built a prototype disk array called RAID-I. The bandwidth delivered to clients by RAID-I was severely limited by the memory system bandwidth of the disk array's host workstation. They designed their second prototype, RAID-II, to deliver more of the disk array bandwidth to file server clients. A custom-built crossbar memory system called the XBUS board connects the disks directly to the high-speed network, allowing data for large requests to bypass the server workstation. RAID-II runs Log-Structured File System (LFS) software to optimize performance for bandwidth-intensive applications. The RAID-II hardware with a single XBUS controller board delivers 20 megabytes/second for large, random read operations and up to 31 megabytes/second for sequential read operations. A preliminary implementation of LFS on RAID-II delivers 21 megabytes/second on large read requests and 15 megabytes/second on large write operations.< >"
1994,Storage alternatives for video service.,"In the next decade, video-on-demand (VOD) services will be widely available to customers and potentially highly profitable to service providers. In order to provide access to many movies, video storage servers may contain not only magnetic disks but also high-capacity tertiary storage devices. In this paper, we study two storage device alternatives: an array of magnetic disks and an array of magnetic tapes. Magnetic disk arrays provide high-bandwidth, low-latency retrieval and moderate storage capacity at high cost. Magnetic tape arrays provide low-bandwidth, high-latency retrieval and high storage capacity at low cost. We evaluate these two storage system alternatives to determine the number of users that each can support. Our simulations show that magnetic disk arrays can support considerably more users, at a lower cost per user, than magnetic tape arrays.< >"
1994,Cooperative Caching: Using Remote Client Memory to Improve File System Performance.,"Emerging high-speed networks will allow machines to access remote data nearly as quickly as they can access local data. This trend motivates the use of cooperative caching: coordinating the file caches of many machines distributed on a LAN to form a more effective overall file cache. In this paper we examine four cooperative caching algorithms using a trace-driven simulation study. These simulations indicate that for the systems studied cooperative caching can halve the number of disk accesses, improving file system read response time by as much as 73%. Based on these simulations we conclude that cooperative caching can significantly improve file system read response time and that relatively simple cooperative caching algorithms are sufficient to realize most of the potential performance gain."
1994,A Quantitative Analysis of Cache Policies for Scalable Network File Systems.,"Current network file system protocols rely heavily on a central server to coordinate file activity among client workstations. This central server can become a bottleneck that limits scalability for environments with large numbers of clients. In central server systems such as NFS and AFS, all client writes, cache misses, and coherence messages are handled by the server. To keep up with this workload, expensive server machines are needed, configured with high-performance CPUs, memory systems, and I/O channels. Since the server stores all data, it must be physically capable of connecting to many disks. This reliance on a central server also makes current systems inappropriate for wide area network use where the network bandwidth to the server may be limited.

In this paper, we investigate the quantitative performance effect of moving as many of the server responsibilities as possible to client workstations to reduce the need for high-performance server machines. We have devised a cache protocol in which all data reside on clients and all data transfers proceed directly from client to client. The server is used only to coordinate these data transfers. This protocol is being incorporated as part of our experimental file system, xFS. We present results from a trace-driven simulation study of the protocol using traces from a 237 client NFS installation. We find that the xFS protocol reduces server load by more than a factor of six compared to AFS without significantly affecting response time or file availability."
1994,Toward Workload Characterization of Video Server and Digital Library Applications.,"An energy-efficient automatic sluice gate for sustaining a fluid level, separating an upstream pool from a downstream pool in an irrigation system and enabling the level of water in one of the pools to be kept constant at a settable value. A sluice gate separates an upstream pool (1) from a downstream pool (2) and enables the level of one of the pools to be kept constant at a settable value, said gate comprising a baffle (3) movable about a horizontal rotational shaft (4) whereby the rotation of the baffle about its shaft determines the flow of water downstream, said baffle constituting a segment of a cylinder having the said rotational shaft as its axis, and further comprising a box member (7A) and a second box member (7B), both moving with the baffle and dipping respectively in the upstream pool and in the downstream pool, means (9, 13, 15) for at least partially filling the box members, means (20) for keeping constant the level in the first or the second box member according to whether the level in the upstream pool or in the downstream pool is to be kept constant and means (50) of setting said constant level value."
1993,Designing Disk Arrays for High Data Reliability.,"Redundancy based on a parity encoding has been proposed for ensuring that disk arrays provide highly reliable data. Parity-based redundancy tolerates many independent and dependent disk failures (shared support hardware) without on-line spare disks and many more such failures with on-line spare disks. This paper explores the design of reliable, redundant disk arrays. In the context of a 70-disk strawman array, it presents and applies analytic and simulation models for the time until data is lost. It shows how to balance requirements for high data reliability against the overhead cost of redundant data, on-line spares, and on-site repair personnel in terms of an array?s architecture, its component reliabilities, and its repair policies.
"
1993,Massive Parallelism and Massive Storage: Trends and Predictions for 1995 to 2000.,The mainline technologies driving the computer industry are microprocessors and DRAMs. The graphs below predict DRAM capacity and microprocessor performance by extrapolating from the past. We can expect gigabit DRAMs and microprocessors 1000 times faster than the VAX-11/780 by the end of the decade. These will be the building blocks for networks of computers containing hundreds to thousands of processor-memory modules. This talks describes characteristics of such machines and makes similar predictions for secondary and tertiary memory. 
1993,LogP: Towards a Realistic Model of Parallel Computation.,"A vast body of theoretical research has focused either on overly simplistic models of parallel computation, notably the PRAM, or overly specific models that have few representatives in the real world. Both kinds of models encourage exploitation of formal loopholes, rather than rewarding development of techniques that yield performance across a range of current and future parallel machines. This paper offers a new parallel machine model, called LogP, that reflects the critical technology trends underlying parallel computers. it is intended to serve as a basis for developing fast, portable parallel algorithms and to offer guidelines to machine designers. Such a model must strike a balance between detail and simplicity in order to reveal important bottlenecks without making analysis of interesting problems intractable. The model is based on four parameters that specify abstractly the computing bandwidth, the communication bandwidth, the communication delay, and the efficiency of coupling communication and computation. Portable parallel algorithms typically adapt to the machine configuration, in terms of these parameters. The utility of the model is demonstrated through examples that are implemented on the CM-5.
"
1993,"A New Approach to I/O Performance Evaluation - Self-Scaling I/O Benchmarks, Predicted I/O Performance.","Current I/O benchmarks suffer from several chronic problems: they quickly become obsolete, they do not stress the I/O system, and they do not help in understanding I/O system performance. We propose a new approach to I/O performance analysis. First, we propose a self-scaling benchmark that dynamically adjusts aspects of its workload according to the performance characteristic of the system being measured. By doing so, the benchmark automatically scales across current and future systems. The evaluation aids in understanding system performance by reporting how performance varies according to each of fie workload parameters. Second, we propose predicted performance, a technique for using the results from the self-scaling evaluation to quickly estimate the performance for workloads that have not been measured. We show that this technique yields reasonably accurate performance estimates and argue that this method gives a far more accurate comparative performance evaluation than traditional single point benchmarks. We apply our new evaluation technique by measuring a SPARCstation 1+ with one SCSI disk, an HP 730 with one SCSI-II disk, a Sprite LFS DECstation 5000/200 with a three-disk disk array, a Convex C240 minisupercomputer with a four-disk disk array, and a Solbourne 5E/905 fileserver with a two-disk disk array.
"
1992,Tradeoffs in Supporting Two Page Sizes.,"As computer system main memories get larger and processor cycles-per-instruction (CPIs) get smaller, the time spent in handling translation lookaside buffer (TLB) misses could become a performance bottleneck. We explore relieving this bottleneck by (a) increasing the page size and (b) supporting two page sizes.

We discuss how to build a TLB to support two page sizes and examine both alternatives experimentally with a dozen uniprogrammed, user-mode traces for the SPARC architecture. Our results show that increasing the page size to 32KB causes both a significant increase in average working set size (e.g., 60%) and a significant reduction in the TLB's contribution to CPI, CPITLB, (namely a factor of eight) compared to using 4KB pages. Results for using two page sizes, 4KB and 32KB pages, on the other hand, show a small increase in working set size (about 10%) and variable decrease in CPITLB, (from negligible to as good as found with the 32KB page size). CPITLB when using two page sizes is consistently better for fully associative TLBs than for set-associative ones.

Our results are preliminary, however, since (a) our traces do not include multiprogramming or operating system behavior, and (b) our page-size assignment policy may not reflect a real operating system's policy."
1991,Towards guidelines for SIGARCH sponsored conferences.,"After almost 20 years of having each program chair invent the policy for each offering of a conference, it seems time to coalesce that experience and set forth guidelines for conferences sponsored by SIGARCH. At the last business meeting it was proposed that SIGARCH adopt an official set of guidelines to aid program chairs on matters of policy at SIGARCH sponsored conferences (e.g., ISCA, ASPLOS, and so on). The attendees at the meeting voted for this proposal. A draft of the proposed guidelines was distributed at the meeting. It was decided that the proposed guidelines should appear in CAN for comments by the community. If you have comments, please send them to CAN. A final vote on the contents of the guidelines will occur at the next SIGARCH business meeting, to be held at the 19th ISCA on May 20, 1992 in Queensland, Australia. "
1990,Maximizing Performance in a Striped Disk Array.,"Improvements in disk speeds have not kept up with improvements in processor and memory speeds. One way to correct the resulting speed mismatch is to stripe data across many disks. In this paper, we address how to stripe data to get maximum performance from the disks. Specifically, we examine how to choose the striping unit, i.e. the amount of logically contiguous data on each disk. We synthesize rules for determining the best striping unit for a given range of workloads.

We show how the choice of striping unit depends on only two parameters: 1) the number of outstanding requests in the disk system at any given time, and 2) the average positioning time ◊ data transfer rate of the disks. We derive an equation for the optimal striping unit as a function of these two parameters; we also show how to choose the striping unit without prior knowledge about the workload."
1990,An Evaluation of Redundant Arrays of Disks Using an Amdahl 5890.,"Recently we presented several disk array architectures designed to increase the data rate and I/O rate of supercomputing applications, transaction processing, and file systems [Patterson 88]. In this paper we present a hardware performance measurement of two of these architectures, mirroring and rotated parity. We see how throughput for these two architectures is affected by response time requirements, request sizes, and read to write ratios. We find that for applications with large accesses, such as many supercomputing applications, a rotated parity disk array far outperforms traditional mirroring architecture. For applications dominated by small accesses, such as transaction processing, mirroring architectures have higher performance per disk than rotated parity architectures.
"
1989,Failure Correction Techniques for Large Disk Arrays.,"The ever increasing need for II0 bandwidth will be met with ever larger arrays of disks. These arrays require redundancy to protect against data loss, This paper examines alternative choices for encodings, or codes, that reliably store information in disk arrays. Codes are selected to maximize mean time to data loss or minimize disks containing redundant data, but are all constrained to minimize performance penalties associated with updating information or recovering from catastrophic disk failures. We show codes that give highly reliable data storage with low redundant data overhead for arrays of 1000 information disks. With the proliferating processing power provided by advanced VLSI technology and parallel architectures comes an inflating demand for Input/Output (UO) performance. The mainstay of online secondary storage, the magnetic disk, is providing neither the data rates required for applications that process large amounts of sequential data nor the access rates required for applications that process large numbers of random accesses [Bora183]. This widening gap has led to I/O systems that achieve performance through disk parallelism, using such techniques as disk striping (also known as disk interleaving) for higher data rates and data distributing for greater access rates [Kim85, Kim87, Klietz88, Livny87, Park86, Salem861. Even though disk performance has not kept pace with processor performance, magnetic disk technology has not been idle; densities have been growing exponentially and physical packaging has achieved amazing volume reductions. The 5.25 inch form-factor drives may currently have the best cost per megabyte, and the 3.5-inch form-factor is expected to overtake it soon [Vasudeva88]. This trend has led some to explore the replacement of individual large form-factor drives with many smaller form-factor drives [Jilke86, Maginnis87, Patterson881, giving yet another reason to expect that future I/O systems will contain large numbers of disks. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/ or specific permission. 0 1989 ACM O-8979 1-300-O/89/0004/0 123 $1 SO.,, While performance improves with increasing numbers of disks, the catch is that the chance of data loss also increases. A simple model for device lifetime, used by the disk industry [Muman87] and for electronics in general [Siewiorek82], is an exponential random variable. In this model the rate of failures in an I/O system is directly proportional to the number of disks; even with disks 10 times as reliable as the best on the market today, the first unrecoverable failure in a non-redundant array of a thousand disks can be expected in less than two weeks. Such high rates of data loss impel the inclusion of data redundancy to allow information to survive hardware failures. Today's magnetic disk drives suffer from three primary types of failures. Transient or noise-related errors are corrected by repeating the offending operation or by applying per sector error-cone & on facilities."
1989,Introduction to redundant arrays of inexpensive disks (RAID).,"The authors discuss various types of RAIDs (redundant arrays of inexpensive disks), a cost-effective option to meet the challenge of exponential growth in the processor and memory speeds. They argue that the size reduction of personal-computer (PC) disks is the key to the success of disk arrays. While large arrays of mainframe processors are possible, it is certainly easier to construct an array from the same number of microprocessors (or PC drives). With advantages in cost-performance, reliability, power consumption, and floor space, the authors expect RAIDs to replace large drives in future I/O systems.< >"
1989,How reliable is a RAID?,"Disk arrays offer greatly increased transfer bandwidth at low cost, but without additional data redundancy they can suffer from significantly degraded reliability. The authors examine the reliability of RAID (redundant arrays of inexpensive disks) systems and find that although extremely reliable disk arrays cannot be attained with data redundancy alone, RAID system reliability can be made better than conventional large disks with little extra hardware. They show that the rest of the system components, such as the host bus adapters, power supplies, and fans cannot be ignored. The authors present a scheme in which the system support components are organized into groups orthogonal to the data redundancy groups, thus guaranteeing that no single disk or component failure will permanently lose data. This approach yields disk arrays with reliability about 50% greater than conventional disk subsystems. If further reliability improvements are sought, various parts of the support hardware can be made redundant. The authors have shown how these affect the design of a 49-data-disk RAID.< >"
1988,A Project on High Performance I/O Subsystems.,n/a
1988,The Scalable Processor Architecture (SPARC).,"An introduction is given to the SPARC architecture and its more interesting features. The discussion covers the registers (both window and floating-point), and instructions, including formats, load/store, integer computation, control transfer, floating-point computation, and coprocessor. A brief comparison with Berkeley RISC (reduced-instruction-set-computer) and SOAR is provided.< >"
1988,A Case for Redundant Arrays of Inexpensive Disks (RAID).,"Increasing performance of CPUs and memories will be squandered if not matched by a similar performance increase in I/O. While the capacity of Single Large Expensive Disks (SLED) has grown rapidly, the performance improvement of SLED has been modest. Redundant Arrays of Inexpensive Disks (RAID), based on the magnetic disk technology developed for personal computers, offers an attractive alternative to SLED, promising improvements of an order of magnitude in performance, reliability, power consumption, and scalability. This paper introduces five levels of RAIDs, giving their relative cost/performance, and compares RAID to an IBM 3380 and a Fujitsu Super Eagle.
"
1988,The Design of XPRS.,"This paper presents an overview of the techniques we are using to build a DBMS at Berkeley that will simultaneously provide high performance and high availability in transaction processing environments, in applications with complex ad-hoc queries and in applications with large objects such as images or CAD layouts. We plan to achieve these goals using a general purpose DBMS and operating system and a shared memory multiprocessor. The hardware and software tactics which we are using to accomplish these goals are described in this paper and include a novel ""fast path"" feature, a special purpose concurrency control scheme, a two-dimensional file system, exploitation of parallelism and a novel method to efficiently mirror disks."
1987,What Price Smalltalk?,"In anticipation of the promise of tremendous hardware advances, software researchers have fashioned expansive programming environments to improve programmer productivity. Even with the march of technology, exploratory programming environments such as Smalltalk-80 require such expensive computers that few programmers can afford them. With the hope of increasing that community, a research project at the University of California created a reduced instruction set computer for Smalltalk called SOAR, which stands for Smalltalk on a RISC. The authors are now able to estimate the performance implications of the Smalltalk-80 programming environment. In the next section the authors list the demands that Smalltalk-80 places on traditional computer systems, then present software and hardware ideas that answer those demands."
1987,Fast multiply and divide for a VLSI floating-point unit.,"This paper presents the design of a fast and area-efficient multiply-divide unit used in building a VLSI floating-point processor (FPU), conforming to the IEEE standard 754. Details of the algorithms, implementation techniques and design tradeoffs are presented, The multiplier and divider are implemented in 2 micron CMOS technology with two layers of metal, and occupy 23 square mm (23% of the entire FPU). We expect to perform extended-precision multiplication and division in 1.1 and 2.8 microseconds, respectively."
1986,An In-Cache Address Translation Mechanism.,"In the design of SPUR, a high-performance multiprocessor workstation, the use of large caches and hardware-supported cache consistency suggests a new approach to virtual address translation. By performing translation in each processor's virtually-tagged cache, the need for separate translation lookaside buffers (TLBs) is eliminated. Eliminating the TLB substantially reduces the hardware cost and complexity of the translation mechanism and eliminates the translation consistency problem. Trace-driven simulations show that normal cache behavior is only minimally affected by caching page table entries, and that in many cases, using a separate device would actually reduce system performance.
"
1986,Evaluation of the SPUR Lisp Architecture.,"The SPUR microprocessor has a 40-bit tagged architecture designed to improve its performance for Lisp programs. Although SPUR includes just a small set of enhancements to the Berkeley RISC-II architecture, simulation results show that with a 150-ns cycle time SPUR will run Common Lisp programs at least as fast as a Symbolies 3600 or a DEC VAX 8600. This paper explains SPUR's instruction set architecture and provides measurements of how certain components of the architecture perform. "
1985,Reduced Instruction Set Computers.,"Reduced instruction set computers aim for both simplicity in hardware and synergy between architectures and compilers. Optimizing compilers are used to compile programming languages down to instructions that are as unencumbered as microinstructions in a large virtual address space, and to make the instruction cycle time as fast as possible. "
1984,Architecture of SOAR: Smalltalk on a RISC.,"Smalltalk on a RISC (SOAR) is a simple, Von Neumann computer that is designed to execute the Smalltalk-80 system much faster than existing VLSI microcomputers. The Smalltalk-80 system is a highly productive programming environment but poses tough challenges for implementors: dynamic data typing, a high level instruction set, frequent and expensive procedure calls, and object-oriented storage management. SOAR compiles programs to a low level, efficient instruction set. Parallel tag checks permit high performance for the simple common cases and cause traps to software routines for the complex cases. Parallel register initialization and multiple on-chip register windows speed procedure calls. Sophisticated software techniques relieve the hardware of the burden of managing objects. We have initial evaluations of the effectiveness of the SOAR architecture by compiling and simulating benchmarks, and will prove SOAR's feasibility by fabricating a 35,000-transistor SOAR chip. These early results suggest that a Reduced Instruction Set Computer can provide high performance in an exploratory programming environment."
1983,Architecture of a VLSI Instruction Cache for a RISC.,"A cache was first used in a commercial computer in 1968,1 and researchers have spent the last 15 years analyzing caches and suggesting improvements. In designing a VLSI instruction cache for a RISC microprocessor we have uncovered four ideas potentially applicable to other VLSI machines. These ideas provide expansible cache memory, increased cache speed, reduced program code size, and decreased manufacturing costs. These improvements blur the habitual distinction between an instruction cache and an instruction fetch unit.

The next four sections present the four architectural ideas, followed by a section on performance evaluation of each idea. We then describe the implementation of the cache and finally summarize the results."
1982,A VLSI RISC.,n/a
1982,Assessing RJSCs In High-Level Language Support.,"A reduced instruction set computer, RISC I, was compared to five traditional machines. It provided the highest performance with the smallest penalty for using high-level language."
1982,A RISCy Approach to Computer Design.,n/a
1982,RISC assessment: A high-level language experiment.,"We present the result of an informal experiment comparing the performance of one Reduced Instruction Set Computer, RISC I, to five traditional computers, VAX-11/780, PDP-11/70, BBN C/70, MC68000, and Z8000, in a high-level language environment. Measuring either absolute performance or the penalty for using high-level languages, the best computer is RISC I.
"
1981,An Experiment in High Level Language Microprogramming and Verification.,"The STRUM system was created to apply software engineering techniques to microprogramming. It provides the tools that allow the microprogrammer to use high level language, structured programming, and formal program verification to create emulations for a horizontally microprogrammed computer. This system is evaluated in two parts: (1) High level microprogramming language design and its use in structured microprogramming; and (2) Verification of a large microprogram. Both parts of this evaluation include experimental results. Part I includes a comparison of an emulation created using traditional techniques to the same emulation created using the STRUM system. Part II describes the formal verification of a 1700 line program that was immediately subjected to extensive testing. This work provides new results on the efficiency of high level microprogramming languages, the effectiveness of peephole optimization for microcode and the practicality of formal microprogram verification. "
1981,V-Compiler: a next-generation tool for microprogramming.,"Microprogramming has always been a difficult task. Related to hardware and software, it seems to have inherited difficulties from both. Microprogramming has the classic reliability and maintainability problems of software and from hardware it has inherited size and speed efficiency as practically the only measure of success. This legacy has made microprogramming a very difficult task. Falk summarized the state of microprogramming today:
At present, microprogramming is an elite activity, performed effectively only by a small number of expert practitioners. The work is detailed, precise, time-consuming, and considerably more expensive than present-day software programming."
1981,VAX hardware for the proposed IEEE floating-point standard.,"The proposed IEEE floating-point standard has been implemented in a substitute floating-point accelerator for the VAX ?? 11/780. We explain how features of the proposed standard influenced the design of the new processor. By comparing it with the original VAX accelerator, we illustrate the differences between hardware for the proposed standard and hardware for a more traditional floating-point architecture."
1981,RISC I: A Reduced Instruction Set VLSI Computer.,"The Reduced Instruction Set Computer (RISC) Project investigates an alternative to the general trend toward computers with increasingly complex instruction sets: With a proper set of instructions and a corresponding architectural design, a machine with a high effective throughput can be achieved. The simplicity of the instruction set and addressing modes allows most instructions to execute in a single machine cycle, and the simplicity of each instruction guarantees a short cycle time. In addition, such a machine should have a much shorter design time.

This paper presents the architecture of RISC I and its novel hardware support scheme for procedure call/return. Overlapping sets of register banks that can pass parameters directly to subroutines are largely responsible for the excellent performance of RISC I. Static and dynamic comparisons between this new architecture and more traditional machines are given. Although instructions are simpler, the average length of programs was found not to exceed programs for DEC VAX 11 by more than a factor of 2. Preliminary benchmarks demonstrate the performance advantages of RISC. It appears possible to build a single chip computer faster than VAX 11/780."
1980,Design Considerations for Single-Chip Computers of the Future.,"In the mid 1980's it will be possible to put a million devices (transistors or active MOS gate electrodes) onto a single silicon chip. General trends in the evolution of silicon integrated circuits are reviewed and design constraints for emerging VLSI circuits are analyzed. Desirable architectural features in modern computers are then discussed and consequences for an implementation with large-scale integrated circuits are investigated. The resulting recommended processor design includes features such as an on-chip memory hierarchy, multiple homogeneous caches for enhanced execution parallelism, support for complex data structures and high-level languages, a flexible instruction set, and communication hardware. It is concluded that a viable modular building block for the next generation of computing systems will be a self-contained computer on a single chip. A tentative allocation of the one milion transistors to the various functional blocks is given, and the result is a memory intensive design."
1980,Retrospective on High-Level Language Computer Architecture.,"High-level language computers (HLLC) have attracted interest in the architectural and programming community during the last 15 years; proposals have been made for machines directed towards the execution of various languages such as ALGOL,1,2 APL,3,4,5 BASIC,6,7 COBOL,8,9 FORTRAN,10,ll LISP,12,13 PASCAL,14 PL/I,15,16,17 SNOBOL,18,19 and a host of specialized languages. Though numerous designs have been proposed, only a handful of high-level language computers have actually been implemented.4,7,9,20,21 In examining the goals and successes of high-level language computers, the authors have found that most designs suffer from fundamental problems stemming from a misunderstanding of the issues involved in the design, use, and implementation of cost-effective computer systems. It is the intent of this paper to identify and discuss several issues applicable to high-level language computer architecture, to provide a more concrete definition of high-level language computers, and to suggest a direction for high-level language computer architectures of the future.
"
1979,Design Considerations for the VLSI Processor of X-tree.,"X-NODE is a single-chip VLSI processor to be realized in the mid 1980's and to be used as a building block for a tree-structured multiprocessor system (X-TREE). Three major trends influence the design of this processor: the continuing evolution of VLSI technology, the requirements for parallelism and communication in a multiprocessor system, and the need for better support of software and high level language constructs. The influence of these trends on the processor architecture are discussed and the current state of the design of X-NODE is outlined. X-NODE will introduce several new features exploiting the full potential of VLSI technology. The processor and hierarchical memory of multiple device types will be combined on a single chip to provide a powerful processor. With basically a memory-to-memory architecture, an on-chip caching scheme provides the performance of a register based architecture. This on-chip memory hierarchy contains program and data, as well as microcode. The instruction set of any processor can thus be dynamically changed and tailored to the specific problem being executed. It is planned to support high level language constructs directly in hardware through mechanisms such as bounds checking.
"
1979,"Towards an efficient, machine-independent language for microprogramming.","A machine independent low level language YALLL is presented. This language produces microcode for two very different machines: Hewlett Packard HP 300 and Digital Equipment Corporation VAX 11/780. The efficiency of this language is tested by comparing two examples on both machines to microassembly coded versions. To our best knowledge, this is the first time programs have been compiled and executed on two different microarchitectures. These examples also let us compare the efficiency of the microarchitectures and macroarchitectures of these machines and re-examine the benefits of microprogramming versus macroprogramming. We conclude this paper with comments upon transportability of high level microprogramming languages.
"
1978,"Communication In X-TREE, A Modular Multiprocessor System.","A communication network for a tree-structured assembly (X-TREE) of single-chip processors is described, and considerations for selecting this particular approach are discussed. The communication links between the processors are high-speed, byte-parallel connections with an asynchronous handshaking protocol. Each node of X-TREE consists of a powerful processor, a switching network and a dedicated communications controller. The latter checks the availability of the links terminating in this node, supervises the creation and elimination of message channels and controls the routing and time multiplexing of concurrent messages over the same link. The switching network inside each X-NODE connects the external links with the internal processor via a fast multiplexed bus which is interfaced to each input/output port through fifo message buffers. Network topology, routing algorithm, addressing scheme, message format and communication hardware are discussed."
1978,An approach to firmware engineering.,n/a
1978,X-Tree: A Tree Structured Multi-Processor Computer Architecture.,"The problem of organizing multiple, monolithic microprocessors into an effective general purpose computer structure is examined. A tree structure with extra interconnections was found to be especially attractive. It provides a structured hierarchy for control, addressing and message routing. More important, it appears to provide a mechanism to automatically migrate data abstractions and processes over the network of processors. The network can be expanded to any desired size and no global control or routine mechanisms are needed.

The potential advantages and disadvantages of the X-Tree structure are discussed and the results of some static simulations are presented."
1976,Strum: Structured Microprogram Development System for Correct Firmware.,"An approach to the development of correct microprograms is to use the methodologies that have been beneficial in the generation of correct user programs, i. e., structured programming, high-level languages (HLL's), and formal program verification using Floyd's inductive assertion method. This paper presents a system that combines these techniques to simplify the design and implementation of correct microprograms for a real microprogrammable computer. It gives some statistics which support our emphasis on generation as well as correctness and some preliminary results on the use of our system."
