2000,The evolution of RISC technology at IBM.,"Abstract:
This paper traces the evolution of IBM RISC architecture from its origins in the 1970s at the IBM Thomas J. Watson Research Center to the present-day IBM RISC System/6000* computer. The acronym RISC, for Reduced Instruction-Set Computer, is used in this paper to describe the 801 and subsequent architectures. However, RISC in this context does not strictly imply a reduced number of instructions, but rather a set of primitives carefully chosen to exploit the fastest component of the storage hierarchy and provide instructions that can be generated easily by compilers. We describe how these goals were embodied in the 801 architecture and how they have since evolved on the basis of experience and new technologies. The effect of this evolution is illustrated with the results of several benchmark tests of CPU performance."
1991,Computer Architecture in the 1990s.,"Abstract:
Some of the technology that will drive the advances of the 1990s are explored. A brief tutorial is given to explain the fundamental speed limits of metal interconnections. The advantages and disadvantages of optical interconnections and where they may be used are discussed in some detail. Trends in speeding up performance by increasing data-path width and by increasing the number of operations performed are reviewed, and questions of efficiency are examined. The advent of super reliable machines produced at very low cost by replicating entire processors is examined.< >"
1990,A Statistical Approach to Machine Translation.,"In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results. "
1990,The Evolution of RISC Technology at IBM.,"Abstract:
This paper traces the evolution of IBM RISC architecture from its origins in the 1970s at the IBM Thomas J. Watson Research Center to the present-day IBM RISC System/6000* computer. The acronym RISC, for Reduced Instruction-Set Computer, is used in this paper to describe the 801 and subsequent architectures. However, RISC in this context does not strictly imply a reduced number of instructions, but rather a set of primitives carefully chosen to exploit the fastest component of the storage hierarchy and provide instructions that can be generated easily by compilers. We describe how these goals were embodied in the 801 architecture and how they have since evolved on the basis of experience and new technologies. The effect of this evolution is illustrated with the results of several benchmark tests of CPU performance."
1989,Probabilistic Parsing Method for Sentence Disambiguation.,n/a
1988,Estimating Interlock and Improving Balance for Pipelined Architectures.,"Abstract
Pipelining is now a standard technique for increasing the speed of computers, particularly for floating-point arithmetic. Single-chip, pipelined floating-point functional units are available as ‚Äúoff the shelf‚Äù components. Addressing arithmetic can be done concurrently with floating-point operations to construct a fast processor that can exploit fine-grain parallelism. This paper describes a metric to estimate the optimal execution time of DO loops on particular processors. This metric is parameterized by the memory bandwidth and peak floating-point rate of the processor, as well as the length of the pipelines used in the functional units. Data dependence analysis provides information about the execution order constraints of the operations in the DO loop and is used to estimate the amount of pipeline interlock required by a loop. Several transformations are investigated to determine their impact on loops under this metric."
1988,A statistical approach to language translation.,"An approach to automatic translation is outlined that utilizes techniques of statistical information extraction from large databases. The method is based on the availability of pairs of large corresponding texts that are translations of each other. In our case, the texts are in English and French. Fundamental to the technique is a complex glossary of correspondence of fixed locations.The steps of the proposed translation process are: (1) Partition the source text into a set of fixed locations. (2) Use the glossary plus contextual information to select the corresponding set of fixed locations into a sequence forming the target sentence. (3) Arrange the words of the target fixed locations into a sequence forming the target sentence. We have developed statistical techniques facilitating both the automatic creation of the glossary, and the performance of the three translation steps, all on the basis of an alignment of corresponding sentences in the two texts. While we are not yet able to provide examples of French/English translation, we present some encouraging intermediate results concerning glossary creation and the arrangement of target word sequences."
1988,A Statistical Approach to French/English Translation.,"In this paper we will outline an approach to automatic translation that utilizes techniques of statistical information extraction from large data bases. These self-organizing techniques have proven successful in the field of automatic speech recognition [1,2,3]. Statistical approaches have also been used recently in lexicography [4] and natural language processing [3,5,6]. The idea of automatic translation by statistical (information theoretic) methods was proposed many years ago by Warren Weaver [7]."
1987,Estimating Interlock and Improving Balance for Pipelined Architectures.," Pipelining is now a standard technique for increasing the speed of computers, particularly for floating-point arithmetic. Single-chip, pipelined floating-point functional units are available as ìoff the shelfî components. Addressing arithmetic can be done concurrently with floating-point operations to construct a fast processor that can exploit fine-grain parallelism. This paper describes a metric to estimate the optimal execution time of DO loops on particular processors. This metric is parameterized by the memory bandwidth and peak floating-point rate of the processor, as well as the length of the pipelines used in the functional units. Data dependence analysis provides information about the execution order constraints of the operations in the DO loop and is used to estimate the amount of pipeline interlock required by a loop. Several transformations are investigated to determine their impact on loops under this metric."
1982,Optimization of range checking (with retrospective).,"An analysis is given for optimizing run-time range checks in regions of high execution frequency. These optimizations are accomplished using strength reduction, code motion and common subexpression elimination. Test programs, using the above optimizations, are used to illustrate run-time improvements."
1982,Optimization of Range Checking.,"An analysis is given for optimizing run-time range checks in regions of high execution frequency. These optimizations are accomplished using strength reduction, code motion and common subexpression elimination. Test programs, using the above optimizations, are used to illustrate run-time improvements."
1981,Register Allocation Via Coloring.,"Abstract
Register allocation may be viewed as a graph coloring problem. Each node in the graph stands for a computed quantity that resides in a machine register, and two nodes are connected by an edge if the quantities interfere with each other, that is, if they are simultaneously live at some point in the object program. This approach, though mentioned in the literature, was never implemented before. Preliminary results of an experimental implementation in a PL/I optimizing compiler suggest that global register allocation approaching that of hand-coded assembly language may be attainable."
1980,Communication: Strenght Reduction for Division and Modulo with Application to Accessing a Multilevel Store.,"Abstract:
A method for replacing certain division and modulo operations by additions and subtractions is presented. This optimization allows efficient and easy use of partitioned arrays to access a multilevel store."
1980,Measurement of Programming Improvement Algorithms.,n/a
1977,An Algorithm for Reduction of Operator Strength.,"A simple algorithm which uses an indexed temporary table to perform reduction of operator strength in strongly connected regions is presented. Several extensions, including linear function test replacement, are discussed. These algorithms should fit well into an integrated package of local optimization algorithms."
1976,A Program Data Flow Analysis Procedure.,The global data relationships in a program can be exposed and codified by the static analysis methods described in this paper. A procedure is given which determines all the definitions which can possibly ‚Äúreach‚Äù each node of the control flow graph of the program and all the definitions that are ‚Äúlive‚Äù on each edge of the graph. The procedure uses an ‚Äúinterval‚Äù ordered edge listing data structure and handles reducible and irreducible graphs indistinguishably.
1974,Optimal decoding of linear codes for minimizing symbol error rate (Corresp.).,"Abstract:
The general problem of estimating the a posteriori probabilities of the states and transitions of a Markov source observed through a discrete memoryless channel is considered. The decoding of linear block and convolutional codes to minimize symbol error probability is shown to be a special case of this problem. An optimal decoding algorithm is derived."
1972,Configurable computers: a new class of general purpose machines.,n/a
1971,Bootstrap Hybrid Decoding for Symmetrical Binary Input Channels.,A new method of decoding is presented that utilizes algebraic constraints across streams of convolutionally encoded information sequences. The scheme works for all channels that are symmetrical from the input. Theoretical performance curves are presented showing the improvement over ordinary sequential decoding and over the older hybrid scheme developed by Falconer. Some simulation results are also reported on.
1964,Universality of Tag Systems with P=2.,"By a simple direct construction it is shown that computations done by Turing machines can be duplicated by a very simple symbol manipulation process. The process is described by a simple form of Post canonical system with some very strong restrictions. This system is monogenic: each formula (string of symbols) of the system can be affected by one and only one production (rule of inference) to yield a unique result. Accordingly, if we begin with a single axiom (initial string) the system generates a simply ordered sequence of formulas, and this operation of a monogenic system brings to mind the idea of a machine. The Post canonical system is further restricted to the ‚ÄúTag‚Äù variety, described briefly below. It was shown in [1] that Tag systems are equivalent to Turing machines. The proof in [1] is very complicated and uses lemmas concerned with a variety of two-tape nonwriting Turing machines. The proof here avoids these otherwise interesting machines and strengthens the main result; obtaining the theorem with a best possible deletion number P = 2. Also, the representation of the Turing machine in the present system has a lower degree of exponentiation, which may be of significance in applications. These systems seem to be of value in establishing unsolvability of combinatorial problems."
1959,Lossless symbol coding with nonprimes (Corresp.).,n/a
