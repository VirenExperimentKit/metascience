2014,A historical perspective of speech recognition.,What do we know now that we did not know 40 years ago?
2013,The Convergence of Machine and Biological Intelligence.,"Abstract:
To explore the exciting new domain of brain informatics, we invited several well-known experts to discuss the state of the art, the challenges, the opportunities, and the trends. In ""Creating Human-Level AI by Educating a Child Machine,"" Raj Reddy proposes an architecture for a ""child machine"" that can learn and is teachable. In ""Cyborg Intelligence,"" Zhaohui Wu, Gang Pan, and Nenggan Zheng describe a biological-machine system consisting of both an organic and a computing part. In ""Formal Minds and Biological Brains II: From the Mirage of Intelligence to a Science and Engineering of Consciousness,"" Paul F.M.J. Verschure discusses human-like cognitive architectures and describes the Distributed Adaptive Control (DAC) architecture for perception, cognition, and action. In ""The Challenges of Closed-Loop Invasive Brain-Machine Interfaces,"" Qiaosheng Zhang and Xiaoxiang Zheng discuss the challenges and trends in closed-loop brain-machine interfaces. In ""Neural Signal Processing in Brain-Machine Interfaces,"" Jose C. Principe takes a critical look at the challenges and opportunities of performing computation with pulses, as neurons do. In ""Neuroprosthesis Control via a Noninvasive Hybrid Brain-Computer Interface,"" Alex Kreilinger, Martin Rohm, Vera Kaiser, Robert Leeb, Rüdiger Rupp, and Gernot R. Müller-Putz describe an example of the convergence of biological intelligence and machine intelligence in a hand-elbow neuroprosthesis control unit."
2012,Human and Machine Intelligence.,"In his 1950 Mind paper, Alan Turing reframed the question of whether machines could think as an operational or behavioral question: Could a computer be built that was indistinguishable from people in playing the ""imitation game,"" now known as ""the Turing Test""? He conjectured that by the end of the 20th century ""one [would] be able to speak of machines thinking without expecting to be contradicted"" and that computers would succeed in the Turing Test.
Turing's first conjecture proved right. Although his second has not yet been realized, research in Artifi cial Intelligence (AI) has generated a variety of algorithms and techniques regularly deployed in systems enabling them to behave in ways that are broadly considered to be intelligent. The performances of Watson, Siri, and driverless cars are but a few examples in the public eye. This session's panelists will highlight some of the major accomplishments of research in AI and its infl uential role in the development of computer science and computer systems more broadly, considering not only progress in individual subfi elds, but also designs for integrating these into well-functioning systems. They will also consider the ways in which AI theories and methods have infl uenced research on human cognition in behavioral sciences and neuroscience as well as scientifi c research more generally, and they will discuss major challenges and opportunities for the decades ahead."
2008,Transmembrane helix prediction using amino acid property features and latent semantic analysis.,"Background
Prediction of transmembrane (TM) helices by statistical methods suffers from lack of sufficient training data. Current best methods use hundreds or even thousands of free parameters in their models which are tuned to fit the little data available for training. Further, they are often restricted to the generally accepted topology ""cytoplasmic-transmembrane-extracellular"" and cannot adapt to membrane proteins that do not conform to this topology. Recent crystal structures of channel proteins have revealed novel architectures showing that the above topology may not be as universal as previously believed. Thus, there is a need for methods that can better predict TM helices even in novel topologies and families.

Results
Here, we describe a new method ""TMpro"" to predict TM helices with high accuracy. To avoid overfitting to existing topologies, we have collapsed cytoplasmic and extracellular labels to a single state, non-TM. TMpro is a binary classifier which predicts TM or non-TM using multiple amino acid properties (charge, polarity, aromaticity, size and electronic properties) as features. The features are extracted from sequence information by applying the framework used for latent semantic analysis of text documents and are input to neural networks that learn the distinction between TM and non-TM segments. The model uses only 25 free parameters. In benchmark analysis TMpro achieves 95% segment F-score corresponding to 50% reduction in error rate compared to the best methods not requiring an evolutionary profile of a protein to be known. Performance is also improved when applied to more recent and larger high resolution datasets PDBTM and MPtopo. TMpro predictions in membrane proteins with unusual or disputed TM structure (K+ channel, aquaporin and HIV envelope glycoprotein) are discussed.

Conclusion
TMpro uses very few free parameters in modeling TM segments as opposed to the very large number of free parameters used in state-of-the-art membrane prediction methods, yet achieves very high segment accuracies. This is highly advantageous considering that high resolution transmembrane information is available only for very few proteins. The greatest impact of TMpro is therefore expected in the prediction of TM segments in proteins with novel topologies. Further, the paper introduces a novel method of extracting features from protein sequence, namely that of latent semantic analysis model. The success of this approach in the current context suggests that it can find potential applications in other sequence-based analysis problems."
2007,"Improving Pronunciation Inference using N-Best List, Acoustics and Orthography.","Abstract:
In this paper, we tackle the problem of pronunciation inference and out-of-vocabulary (OOV) enrollment in automatic speech recognition (ASR) applications. We combine linguistic and acoustic information of the OOV word using its spelling and a single instance of its utterance to derive an appropriate phonetic baseform. The novelty of the approach is in its employment of an orthography-driven n-best hypothesis and rescoring strategy of the pronunciation alternatives. We make use of decision trees and heuristic tree search to construct and score the n-best hypotheses space. We use acoustic alignment likelihood and phone transition cost to leverage the empirical evidence and phonotactic priors to rescore the hypotheses and refine the baseforms."
2006,Robotics and Intelligent Systems in Support of Society.,"Abstract:
Over the last 50 years, there has been extensive research into robotics and intelligent systems. Although much of the research has targeted specific technical problems, advances in these areas have led to systems and solutions that will profoundly impact society. Underlying most of the advances is the unprecedented exponential improvement of information technology. Several applications of robotics and intelligent systems could profoundly impact the well being of our society, transforming how we live, learn, and work"
2006,Interviews: The Challenges of Emerging Economies.,"Abstract:
Reports on computer technologies and communications technologies that can be provided to emerging economies that are affordable and will enable the building of efficient communications infrastructures."
2006,Digital Library of India: A Testbed for Indian Language Research.,"This paper describes the goal of the Universal Digital Library Project (UDL) and presents the approach taken by-and the technological challenges associated with-the Million Books to the Web Project (MBP). The Digital Library of India (DLI) initiative, which is the Indian part of the UDL and MBP, is discussed. DLI fosters a large number of research activities in areas such as text summarization, information retrieval, machine translation and transliteration, optical character recognition, handwriting recognition, and natural language parsing and morphological analyses. This paper provides an overview of the activities of DLI in these areas and shows how DLI serves as a multilingual resource. "
2005,The Origins of the American Association for Artificial Intelligence.,"Abstract
This article provides a historical background on how AAAI came into existence. It provides a rationale for why we needed our own society. It provides a list of the founding members of the community that came together to establish AAAI. Starting a new society comes with a whole range of issues and problems: What will it be called? How will it be financed? Who will run the society? What kind of activities will it engage in? and so on. This article provides a brief description of the considerations that went into making the final choices. It also provides a description of the historic first AAAI conference and the people that made it happen."
2004,Computational Biology and Language.,"Abstract
Current scientific research is characterized by increasing specialization, accumulating knowledge at a high speed due to parallel advances in a multitude of sub-disciplines. Recent estimates suggest that human knowledge doubles every two to three years – and with the advances in information and communication technologies, this wide body of scientific knowledge is available to anyone, anywhere, anytime. This may also be referred to as ambient intelligence – an environment characterized by plentiful and available knowledge. The bottleneck in utilizing this knowledge for specific applications is not accessing but assimilating the information and transforming it to suit the needs for a specific application. The increasingly specialized areas of scientific research often have the common goal of converting data into insight allowing the identification of solutions to scientific problems. Due to this common goal, there are strong parallels between different areas of applications that can be exploited and used to cross-fertilize different disciplines. For example, the same fundamental statistical methods are used extensively in speech and language processing, in materials science applications, in visual processing and in biomedicine. Each sub-discipline has found its own specialized methodologies making these statistical methods successful to the given application. The unification of specialized areas is possible because many different problems can share strong analogies, making the theories developed for one problem applicable to other areas of research. It is the goal of this paper to demonstrate the utility of merging two disparate areas of applications to advance scientific research. The merging process requires cross-disciplinary collaboration to allow maximal exploitation of advances in one sub-discipline for that of another. We will demonstrate this general concept with the specific example of merging language technologies and computational biology."
2003,Three open problems in AI.,n/a
1998,Innovation and Obstacles: The Future of Computing.,n/a
1997,An Integral Approach to Free-Form Object Modeling.,"Abstract:
Presents an approach to free-form object modeling from multiple range images. In most conventional approaches, successive views are registered sequentially. In contrast to the sequential approaches, we propose an integral approach which reconstructs statistically optimal object models by simultaneously aggregating all data from multiple views into a weighted least-squares (WLS) formulation. The integral approach has two components. First, a global resampling algorithm constructs partial representations of the object from individual views, so that correspondence can be established among different views. Second, a weighted least-squares algorithm integrates resampled partial representations of multiple views, using the techniques of principal component analysis with missing data (PCAMD). Experiments show that our approach is robust against noise and mismatch."
1996,The Challenge of Artificial Intelligence.,"Abstract:
Artificial intelligence (AI) is a relatively young discipline, yet it has already led to general-purpose problem-solving methods and novel applications. Ultimately, AI's goals of creating models and mechanisms of intelligent action can be realized only in the broader context of computer science. Creating mechanisms for sharing of knowledge, knowhow, and literacy is the challenge. The great Chinese philosopher Kuan-Tzu once said: ""If you give a fish to a man, you will feed him for a day. If you give him a fishing rod, you will feed him for life."" We can go one step further: If we can provide him with the knowledge and the know-how for making that fishing rod, we can feed the whole village. Therein lies the promise-and the challenge-of AI."
1995,Informedia Digital Video Library.,"The Informedia Digital Video Library Project is developing new technologies for creating full-content search and retrieval digital video libraries. Working in collaboration with WQED Pittsburgh, the project is creating a testbed that will enable K-12 students to access, explore, and retrieve science and mathematics materials from the digital video library. The library will initially contain 1,000 hours of video from the archives of project partners: WQED, Fairfax Co. VA Schools' Electronic BBC-produced video courses. (Industrial partners include Digital Equipment Corp., Bell Atlantic, Intel Corp., and Microsoft, Inc.) This library will be installed at Winchester Thurston School, an independent K-12 school in Pittsburgh.
"
1995,Grand Challenges in AI.,n/a
1995,Principal Component Analysis with Missing Data and Its Application to Polyhedral Object Modeling.,"Abstract:
Observation-based object modeling often requires integration of shape descriptions from different views. To overcome the problems of errors and their accumulation, we have developed a weighted least-squares (WLS) approach which simultaneously recovers object shape and transformation among different views without recovering interframe motion. We show that object modeling from a range image sequence is a problem of principal component analysis with missing data (PCAMD), which can be generalized as a WLS minimization problem. An efficient algorithm is devised. After we have segmented planar surface regions in each view and tracked them over the image sequence, we construct a normal measurement matrix of surface normals, and a distance measurement matrix of normal distances to the origin for all visible regions over the whole sequence of views, respectively. These two matrices, which have many missing elements due to noise, occlusion, and mismatching, enable us to formulate multiple view merging as a combination of two WLS problems. A two-step algorithm is presented. After surface equations are extracted, spatial connectivity among the surfaces is established to enable the polyhedral object model to be constructed. Experiments using synthetic data and real range images show that our approach is robust against noise and mismatching and generates accurate polyhedral object models.< >"
1995,The Universal Library: Intelligent Agents and Information on Demand.,"Abstract
The vision of The Universal Library represents a powerful idea. It is not a mere electronic equivalent of a conventional library. The scope of this library would involve a combination of sources which will provide video, movies and music on demand, as well as books, newpapers and magazines. Because of the astronomical cost of $5 to $20 billion that is required to create the The Universal Library is beyond the reach of any one country, the most effective way to attain this goal is likely to be through a truly international collaboration. The projected world wide annual revenues of $20 to $200 billion is likely to make the idea of The Universal Library into a reality.
The technical community has the opportunity to expand on the idea of the conventional library by creating what is now only a vision. There are a number of difficult technical problems we can begin to solve. But to solve the various problems and bring the vision to life will require the enlightened leadership and cooperation from the national leadership."
1995,An Integral Approach to Free-Formed Object Modeling.,"Abstract:
Presents a new approach to free-formed object modeling from multiple range images. In most conventional approaches, successive views are registered sequentially. In contrast to the sequential approaches, we propose an integral approach which reconstructs statistically optimal object models by simultaneously aggregating all data from multiple views into a weighted least-squares (WLS) formulation. The integral approach has two components. First, a global resampling algorithm constructs partial representations of the object from individual views so that correspondences can be established among different views. The global resampling algorithm is based on the spherical attribute image (SAI) previously introduced in the context of object representation and recognition. Second, a weighted least-squares algorithm integrates resampled partial representations of multiple views, using the technique of principal component analysis with missing data (PCAMD). Experiments using real range images show that our approach is robust against noise and mismatches, and generates accurate object models.< >"
1994,Principal component analysis with missing data and its application to object modeling.,"Abstract:
Observation-based modeling can reduce the cost and effort of model constructions for tasks such as virtual reality environment. Object modeling from a sequence of range images has been formulated as a problem of principal component analysis with missing data (PCAMD), which can be generalized as a weighted least square (WLS) minimization problem. After all visible regions appeared over the whole sequence are segmented and tracked, a normal measurement matrix of surface normals and a distance measurement matrix of normal distances to the origin are constructed respectively. These two measurement matrices, with possibly many missing elements due to occlusion and mismatching, enable us to formulate multiple view merging as a combination of two WLS problems. The solution to the first WLS problem, which employs the quaternion representation of the rotation matrix, yields surface normals and rotation matrices. Subsequently the normal distances and translation vectors are computed by solving the second WLS problem. Experiments using synthetic data and real range images show that our approach is robust against noise and mismatch because it produces a statistically optimal object model by making use of redundancy from multiple views. A toy house model from a sequence of real range images is presented.< >"
1994,Virtual reality modeling from a sequence of range images.,"Abstract:
Virtual reality object modeling from a sequence of range images has been formulated as a problem of principal component analysis with missing data (PCAMD), which can be generalized as a weighted least square (WLS) minimization problem. An efficient algorithm has been devised to solve the problem of PCAMD. After all visible P regions appeared over the whole sequence of F views are segmented and tracked, a 3F/spl times/P normal measurement matrix of surface normals and an F/spl times/P distance measurement matrix of normal distances to the origin are constructed respectively. These two measurement matrices, with possibly many missing elements due to occlusion and mismatching, enable us to formulate multiple view merging as a combination of two WLS problems. By combining information at both the signal level and the algebraic level, a modified Jarvis' march algorithm is proposed to recover the spatial connectivity among all the reconstructed surface patches. Experiments using synthetic data and real range images show that our approach is robust against noise and mismatch. A toy house model from a sequence of real range images is presented.< >"
1994,Spoken-Language Research at Carnegie Mellon.,n/a
1993,Spoken-Language Research At Carnegie Mellon.,n/a
1992,Spoken-Language Research At Carnegie Mellon.,n/a
1991,Spoken-Language Research at Carnegie Mellon.,n/a
1990,An overview of the SPHINX speech recognition system.,"Abstract:
A description is given of SPHINX, a system that demonstrates the feasibility of accurate, large-vocabulary, speaker-independent, continuous speech recognition. SPHINX is based on discrete hidden Markov models (HMMs) with LPC- (linear-predictive-coding) derived parameters. To provide speaker independence, knowledge was added to these HMMs in several ways: multiple codebooks of fixed-width parameters, and an enhanced recognizer with carefully designed models and word-duration modeling. To deal with coarticulation in continuous speech, yet still adequately represent a large vocabulary, two new subword speech units are introduced: function-word-dependent phone models and generalized triphone models. With grammars of perplexity 997, 60, and 20, SPHINX attained word accuracies of 71, 94, and 96%, respectively, on a 997-word task.< >"
1990,Translating telephone: problems and prospects.,"Abstract:
Summary form only given. Japan recently initiated a seven year $120 million project as the first phase toward developing a phone system in which a Japanese speaker can converse with say, an English speaker in real time. This requires solutions to a number of currently unsolved problems: (1) developing a speech recognition system capable of recognizing a large (possibly unlimited) vocabulary of spontaneous, unrehearsed, continuous speech, (2) developing a natural language translation system capable of dealing with ambiguity, nongrammaticality, and incomplete phrases, and (3) achieving natural sounding speech synthesis that preserves speaker characteristics.< >"
1990,Speech Research at Carnegie Mellon.,n/a
1989,Speech Research at Carnegie Mellon.,n/a
1989,Speech Research at Carnegie Mellon.,n/a
1988,Foundations and Grand Challenges of Artificial Intelligence: AAAI Presidential Address.,"Abstract
AAAI is a society devoted to supporting the progress in science, technology and applications of AI. I thought I would use this occasion to share with you some of my thoughts on the recent advances in AI, the insights and theoretical foundations that have emerged out of the past thirty years of stable, sustained, systematic explorations in our field, and the grand challenges motivating the research in our field."
1983,Steps Toward Graceful Interaction in Spoken and Written Man-Machine Communication.,"Natural language processing is often seen as a way to provide easy-to-use and flexible interfaces to interactive computer systems. White natural language interfaces typically perform well in response to straightforward requests and questions within their domain of discourse, they often fail to interact gracefully with their users in less predictable circumstances. Most current systems cannot, for instance: respond reasonably to input not conforming to a rigid grammar; ask for and understand clarification if their user's input is unclear; offer clarification of their own output if the user asks for it; or interact to resolve any ambiguities that may arise when the user attempts to describe things to the system.
We believe that graceful interaction in these and the many other contingencies that can arise in human conversation is essential if interfaces are ever to appear co-operative and helpful, and hence be suitable for the casual or naive user, and more habitable for the experienced user. In this paper, we attempt to outline key components of graceful interaction, to identify major problems involved in realizing them, and in some cases to suggest the shape of solutions.
To this end we propose a decomposition of graceful interaction into a number of relatively independent skills: skills involved in parsing elliptical, fragmented, and otherwise ungrammatical input; in ensuring robust communication; in explaining abilities and limitations, actions and the motives behind them; in keeping track of the focus of attention of a dialogue; in identifying things from descriptions, even if ambiguous or unsatisfiable; and in describing things in terms appropriate for the context. We suggest these skills are necessary for graceful interaction in general and form a good working basis for graceful interaction in a certain large class of application domains, which we define. None of these components appear individually much beyond the current state of the art, at least for suitably restricted domains of discourse. Thus, we advocate research into the construction of gracefully interacting systems as an activity likely to pay major dividends in improved man-machine communication in a relatively short time."
1981,Breaking the Man-Machine Communication Barrier.,"Abstract:
How can interactive systems be made more helpful and responsive? This tool-independent system cuts down frustration by incorporating many of the communications shortcuts people use naturally."
1980,The Hearsay-II Speech-Understanding System: Integrating Knowledge to Resolve Uncertainty.,"The Hearsay-II system, developed during the DARPA-sponsored five-year speech understanding research program, represents both a specific solution to the speech-understanding problem and a general framework for coordinating independent processes to achieve cooperative problem-solving behaviour. As a computational problem, speech understanding reflects a large number of intrinsically interesting issues. Spoken sounds are achieved by a long chain of successive transformations, from intentions, through semantic and syntactic structuring, to the eventually resulting audible acoustic waves. As a consequence, interpreting speech means effectively inverting these transformations to recover the speaker's intention from the sound. At each step in the interpretive process, ambiguity and uncertainty arise. The Hearsay-II problem-solving framework reconstructs an intention from hypothetical interpretations formulated at various levels of abstraction. In addition, it allocates limited processing resources first to the most promising incremental actions. The final configuration of the Hearsay-II system comprises problem-solving components to generate and evaluate speech hypotheses, and a focus-of-control mechanism to identify potential actions of greatest value. Many of these specific procedures reveal novel approaches to speech problems. Most important, the system successfully integrates and coordinates all of these independent activities to resolve uncertainty and control combinatorics. Several adaptations of the Hearsay-II framework have already been undertaken in other problem domains, and it is anticipated that this trend will continue; many future systems necessarily will integrate diverse sources of knowledge to solve complex problems cooperatively. Discussed in this paper are the characteristics of the speech problem in particular, the special kinds of problem-solving uncertainty in that domain, the structure of the Hearsay-II system developed to cope with that uncertainty, and the relationship between Hearsay-II's structure and those of other speech-understanding systems. The paper is intended for the general computer science audience and presupposes no speech or artificial intelligence background."
1979,Matching Segments of Images.,"Abstract:
This correspondence describes research in the development of symbolic registration techniques directed toward the comparison of pairs of images of the same scene to ultimately generate descriptions of the changes in the scene. Unlike most earlier work in image registration, all the matching and analysis will be performed at a symbolic level rather than a signal level. We have applied this registration procedure on several different types of scenes and the system appears to work well both on pairs of images which may be analyzed in part by signal based systems and those which cannot be so analyzed."
1979,Graceful Interaction in Man-Machine Communication.,"Compared to humans, current natural language dialogue systems often behave in a rigid and fragile manner when their conversations deviate from a narrowly conceived mainstream, e.g. when faced with ungrammatical, unclear, or unrecognizable input, ambiguous descriptions, or requests for clarification of their own output. We believe that the time is now ripe to construct systems which can interact gracefully with their users when such contingencies arise. Graceful interaction is not a single skill, but a combination of several diverse abilities. We list these components, and describe one of them - the ability to communicate robustly. Detailed descriptions of all the components appear in [4], along with details of a system architecture for their integrated Implementation."
1977,Speech Understanding Systems.,"Abstract
A five-year interdisciplinary effort by speech scientists and computer scientists has demonstrated the feasibility of programming a computer system to “understand” connected speech, i.e., translate it into operational form and respond accordingly. An operational system (HARPY) accepts speech from five speakers, interprets a 1000-word vocabulary, and attains 91 percent sentence accuracy. This Steering Committee summary report describes the project history, problem, goals, and results."
1977,Knowledge Guided Learning of Structural Descriptions.,n/a
1977,The LOCUS Model of Search and its Use in Image Interpretation.,"The Locus model of search is a non-backtracking, deterministic search technique in which a beam of near-miss alternatives around the best path are extended in parallel for graph searching problems. In this paper we formulate image interpretation as a graph searching problem and show how the Locus model provides a near-optimal minimal effort solution. The structure of the model is illustrated using a detailed example. The relationship of the present approach to earlier attempts at image interpretation are discussed."
1977,Change Detection and Analysis in Multispectral Images.,"This paper describes work on the development of symbolic registration and change analysis techniques applied to the problem of the comparison of pairs of images of a scene to generate descriptions of the changes in the scene. Unlike earlier work in change analysis, all the matching and later analysis is performed symbolically. We also discuss techniques for the generation of symbolic descriptions of images, both the segmentation and feature extraction problems. These techniques have been applied to several different scenes and in this paper we present the results for two of these scenes."
1976,System Organizations for Speech Understanding: Implications of Network and Multiprocessor Computer Architectures for AI.,"Abstract:
This paper considers various factors affecting system em organization for speech understanding research. The structure of the Hearsay system based on a set of cooperating, independent processes using the hypothesize-and-test paradigm is presented. Design considerations for the effective use of multiprocessor and network achitectures in speech understanding systems ems are presented: control of processes, interprocess communication and data sharing, resource allocation, and debugging are discussed. 1"
1976,The Hearsay- I Speech Understanding System: An Example of the Recognition Process.,"Abstract:
This paper describes the structure and operation of the Hearsay-I 1 speech understanding system by the use of a specific example illustrating the various stages of recognition. The system consists of a set of cooperating independent processes, each representing a source of knowledge. The knowledge is used either to predict what may appear in a given context or to verify hypotheses resulting from a prediction. The structure of the system is illustrated by considering its operation in a particular task situation: Voice-Chess. The representation and use of various sources of knowledge are outlined. Preliminary results of the reduction in search resulting from the use of various sources of knowledge are given."
1973,The Hearsay Speech Understanding System: An Example of the Recognition Process.,"This paper describes the structure and operation of the Hearsay-I 1 speech understanding system by the use of a specific example illustrating the various stages of recognition. The system consists of a set of cooperating independent processes, each representing a source of knowledge. The knowledge is used either to predict what may appear in a given context or to verify hypotheses resulting from a prediction. The structure of the system is illustrated by considering its operation in a particular task situation: Voice-Chess. The representation and use of various sources of knowledge are outlined. Preliminary results of the reduction in search resulting from the use of various sources of knowledge are given."
1973,System Organizations for Speech Understanding: Implications of Network and Multiprocessor Computer Architecture for AI.,"This paper considers various factors affecting system em organization for speech understanding research. The structure of the Hearsay system based on a set of cooperating, independent processes using the hypothesize-and-test paradigm is presented. Design considerations for the effective use of multiprocessor and network achitectures in speech understanding systems ems are presented: control of processes, interprocess communication and data sharing, resource allocation, and debugging are discussed."
1972,XCRIBL - A Hardcopy Scan Line Graphics System for Document Generation.,n/a
1971,Speech Recognition: Prospects of the Seventies.,n/a
1968,"A computer with hands, eyes, and ears.","The anthropomorphic terms of the title may suggest an interest in machines that look or act like men. To this extent it is misleading. Our interest is in extending the range of tasks to which machines can be applied to include those that, when performed by a human, require coordination between perceptual and motor processes. We attempt to suppress the egocentric idea that man performs these tasks in the best of all possible ways."
1967,Pitch period determination of speech sounds.,
