2016,Floating-point precision tuning using blame analysis.,"While tremendously useful, automated techniques for tuning the precision of floating-point programs face important scalability challenges. We present Blame Analysis, a novel dynamic approach that speeds up precision tuning. Blame Analysis performs floating-point instructions using different levels of accuracy for their operands. The analysis determines the precision of all operands such that a given precision is achieved in the final result of the program. Our evaluation on ten scientific programs shows that Blame Analysis is successful in lowering operand precision. As it executes the program only once, the analysis is particularly useful when targeting reductions in execution time. In such case, the analysis needs to be combined with search-based tools such as Precimonious. Our experiments show that combining Blame Analysis with Precimonious leads to obtaining better results with significant reduction in analysis time: the optimized programs execute faster (in three cases, we observe as high as 39.9% program speedup) and the combined analysis time is 9x faster on average, and up to 38x faster than Precimonious alone."
2013,Precimonious: tuning assistant for floating-point precision.,"Given the variety of numerical errors that can occur, floating-point programs are difficult to write, test and debug. One common practice employed by developers without an advanced background in numerical analysis is using the highest available precision. While more robust, this can degrade program performance significantly. In this paper we present Precimonious, a dynamic program analysis tool to assist developers in tuning the precision of floating-point programs. Precimonious performs a search on the types of the floating-point program variables trying to lower their precision subject to accuracy constraints and performance goals. Our tool recommends a type instantiation that uses lower precision while producing an accurate enough answer without causing exceptions. We evaluate Precimonious on several widely used functions from the GNU Scientific Library, two NAS Parallel Benchmarks, and three other numerical programs. For most of the programs analyzed, Precimonious reduces precision, which results in performance improvements as high as 41%."
2012,A family of Anadromic numerical methods for matrix Riccati differential equations.,"Matrix Riccati Differential Equations (MRDEs) X? = A21 ?XA11 +A22X ?XA12X, X(0) = X0, where Aij ? Aij(t), appear frequently throughout applied mathematics, science, and engineering. Naturally, the existing conventional Runge-Kutta methods and linear multi-step methods can be adapted to solve MRDEs numerically. Indeed, they have been adapted. There are a few unconventional numerical methods, too, but they are suited more for time-invariant MRDEs than time-varying ones. For stiff MRDEs, existing implicit methods which are preferred to explicit ones require solving nonlinear systems of equations (of possibly much higher dimensions than the original problem itself of, for example, implicit Runge-Kutta methods), and thus they can pose implementation difficulties and also be expensive. In the past, the property of an MRDE which has been most preserved is the symmetry property for a symmetric MRDE; many other crucial properties have been discarded. Besides the symmetry property, our proposed methods also preserve two other important properties — Bilinear Rational Dependence on the initial value, and a Generalized Inverse Relation between an MRDE and its complementary MRDE. By preserving the generalized inverse relation, our methods are accurately able to integrate an MRDE whose solution has singularities. By preserving the property of bilinear dependence on the initial value, our methods also conserve the rank of change to the initial value and a solution’s monotonicity property. Our methods are anadromic,1meaning if an MRDE is integrated by one of our methods from t=? to ?+? and then integrated backward from t=?+? to ? using the same method, the value at t=? is recovered in the absence of rounding errors. This implies that our methods are necessarily of even order of convergence. For time-invariant MRDEs, methods of any even order of convergence are established, while for time-varying MRDEs, methods of order as high as 10 are established; but only methods of order up to 6 are stated in detail. Our methods are semi-implicit, in the sense that there are no nonlinear systems of matrix equations to solve, only linear ones, unlike any pre-existing implicit method. Given the availability of high quality codes for linear matrix equations, our methods can easily be implemented and embedded into any application software package that needs a robust MRDE solver. Numerical examples are presented to support our claims. 1We coined “anadromic” from the Greek roots “???-” (back up) and “??o?o?” (act of running) of “anadromous” (which describes fish that must return from the ocean to spawn in the same streams whence they hatched) after we had tried to describe our numerical methods with words such as “reflexive” [27, 28], “symmetric” (which is still used today in the literature, e.g., [22]), and “reversible” which turned out to be overused."
2012,The Turing Computational Model.,"The panel presentations will discuss the beauty and simplicity of the Turing machine formulation of the previously elusive concept of computability and the intuitively satisfying explanation of the power and limitations of computability. They will also review how the Turing machine model provided simple proofs of deep results in logic, including GÃ¶delâ€™s incompleteness theorem. The panel will also examine specifi c results in computer science infl uenced by the Turing machine model as well as how it shaped the development of computational complexity theory. Quantum computing will be discussed and its relationship to the classic Turing machine model. The panel will also discuss what Alan Turing might say about the Inevitable Fallibility of Software."
2006,Error bounds from extra-precise iterative refinement.,"We present the design and testing of an algorithm for iterative refinement of the solution of linear equations where the residual is computed with extra precision. This algorithm was originally proposed in 1948 and analyzed in the 1960s as a means to compute very accurate solutions to all but the most ill-conditioned linear systems. However, two obstacles have until now prevented its adoption in standard subroutine libraries like LAPACK: (1) There was no standard way to access the higher precision arithmetic needed to compute residuals, and (2) it was unclear how to compute a reliable error bound for the computed solution. The completion of the new BLAS Technical Forum Standard has essentially removed the first obstacle. To overcome the second obstacle, we show how the application of iterative refinement can be used to compute an error bound in any norm at small cost and use this to compute both an error bound in the usual infinity norm, and a componentwise relative error bound.We report extensive test results on over 6.2 million matrices of dimensions 5, 10, 100, and 1000. As long as a normwise (componentwise) condition number computed by the algorithm is less than 1/max{10,âˆšn}Îµw, the computed normwise (componentwise) error bound is at most 2 max{10, âˆšn} Â· Îµw, and indeed bounds the true error. Here, n is the matrix dimension and Îµw = 2-24 is the working precision. Residuals were computed in double precision (53 bits of precision). In other words, the algorithm always computed a tiny error at negligible extra cost for most linear systems. For worse conditioned problems (which we can detect using condition estimation), we obtained small correct error bounds in over 90% of cases."
2006,Prospectus for the Next LAPACK and ScaLAPACK Libraries.,"Abstract
New releases of the widely used LAPACK and ScaLAPACK numerical linear algebra libraries are planned. Based on an on-going user survey (www.netlib.org/lapack-dev) and research by many people, we are proposing the following improvements: Faster algorithms, including better numerical methods, memory hierarchy optimizations, parallelism, and automatic performance tuning to accommodate new architectures; More accurate algorithms, including better numerical methods, and use of extra precision; Expanded functionality, including updating and downdating, new eigenproblems, etc. and putting more of LAPACK into ScaLAPACK; Improved ease of use, e.g., via friendlier interfaces in multiple languages. To accomplish these goals we are also relying on better software engineering techniques and contributions from collaborators at many institutions."
2005,An Open Question to Developers of Numerical Software.,"Abstract:
IEEE 754 a standard for binary floating-point arithmetic has revolutionized the portability and reliability of programs that use binary floating-point arithmetic. Floating point is almost universally implemented with special-purpose hardware that tucks into a small corner of the CPU chip and runs in the hundreds of Mflops to Gflops range. Single-stepping through today's floating-point software to debug it often turns out to be futile. The concept of a NaN, standing for ""not a number"", evolved from an ""indefinite"" in Seymour Cray's CDC 6600. IEEE 754, by default, requires an untrapped ""invalid operation"", to signal itself by raising a flag and to deliver a NaN just when any other result, be it finite or infinite, would cause worse confusion. The NaN lets a program retain control unless the program or programmer directs its cancellation upon an invalid operation. Thus, a program conducting a search can return to the realm being searched after an accidental foray beyond a boundary whose existence and location were previously unknown. A sNaN differs from the other quiet NaNs by traooing any attempt to perform arithmetic upon it; then a trap-handler must interpret this sNaN."
2002,"Design, implementation and testing of extended and mixed precision BLAS.","This article describes the design rationale, a C implementation, and conformance testing of a subset of the new Standard for the BLAS (Basic Linear Algebra Subroutines): Extended and Mixed Precision BLAS. Permitting higher internal precision and mixed input/output types and precisions allows us to implement some algorithms that are simpler, more accurate, and sometimes faster than possible without these features. The new BLAS are challenging to implement and test because there are many more subroutines than in the existing Standard, and because we must be able to assess whether a higher precision is used for internal computations than is used for either input or output variables. We have therefore developed an automated process of generating and systematically testing these routines. Our methodology is applicable to languages besides C. In particular, our algorithms used in the testing code will be valuable to all other BLAS implementors. Our extra precision routines achieve excellent performance---close to half of the machine peak Megaflop rate even for the Level 2 BLAS, when the data access is stride one."
2002,On computing givens rotations reliably and efficiently.,"We consider the efficient and accurate computation of Givens rotations. When f and g are positive real numbers, this simply amounts to computing the values of c = f/âˆšf2 + g2, s = g/âˆšf2 + g2, and r = âˆšf2 + g2. This apparently trivial computation merits closer consideration for the following three reasons. First, while the definitions of c, s and r seem obvious in the case of two nonnegative arguments f and g, there is enough freedom of choice when one or more of f and g are negative, zero or complex that LAPACK auxiliary routines SLARTG, CLARTG, SLARGV and CLARGV can compute rather different values of c, s and r for mathematically identical values of f and g. To eliminate this unnecessary ambiguity, the BLAS Technical Forum chose a single consistent definition of Givens rotations that we will justify here. Second, computing accurate values of c, s and r as efficiently as possible and reliably despite over/underflow is surprisingly complicated. For complex Givens rotations, the most efficient formulas require only one real square root and one real divide (as well as several much cheaper additions and multiplications), but a reliable implementation using only working precision has a number of cases. On a Sun Ultra-10, the new implementation is slightly faster than the previous LAPACK implementation in the most common case, and 2.7 to 4.6 times faster than the corresponding vendor, reference or ATLAS routines. It is also more reliable; all previous codes occasionally suffer from large inaccuracies due to over/underflow. For real Givens rotations, there are also improvements in speed and accuracy, though not as striking. Third, the design process that led to this reliable implementation is quite systematic, and could be applied to the design of similarly reliable subroutines."
1999,Symbolic computation of divided differences.,"Divided differences are enormously useful in developing stable and accurate numerical formulas. For example, programs to compute f(x)-f(y) as might occur in integration, can be notoriously inaccurate. Such problems can be cured by approaching these computations through divided difference formulations. This paper provides a guide to divided difference theory and practice, with a special eye toward the needs of computer algebra systems that should be programmed to deal with these often-messy formulas."
1997,Composition constants for raising the orders of unconventional schemes for ordinary differential equations.,"Many models of physical and chemical processes give rise to ordinary differential equations with special structural properties that go unexploited by general-purpose software designed to solve numerically a wide range of differential equations. If those properties are to be exploited fully for the sake of better numerical stability, accuracy and/or speed, the differential equations may have to be solved by unconventional methods. This short paper is to publish composition constants obtained by the authors to increase efficiency of a family of mostly unconventional methods, called reflexive."
1992,Analysis and refutation of the LCAS.,"A Language Compatible Arithmetic Standard (LCAS) has been proposed as International Standard ISO/IEC 10967:1991 for Language Compatible Arithmetic, Project JTC. 22.28, Version 3.1 (1 March 1991), by Drs. Mary Payne and Brian Wichmann. An earlier version appeared in both ACM SIGNUM and ACM SIGPLAN 25 1 (Jan. 1990). The following remonstrance has been sent to ANSI X3-T2, the American committee in charge of the proposal:This proposed Language Compatible Arithmetic Standard is so severely flawed that the computing world must reject it."
1990,Accurate Singular Values of Bidiagonal Matrices.,"Computing the singular values of a bidiagonal matrix is the final phase of the standard algorithm for the singular value decomposition of a general matrix. A new algorithm that computes all the singular values of a bidiagonal matrix to high relative accuracy independent of their magnitudes is presented. In contrast, the standard algorithm for bidiagonal matrices may compute small singular values with no relative accuracy at all. Numerical experiments show that the new algorithm is comparable in speed to the standard algorithm, and frequently faster."
1985,Anomalies in the IBM ACRITH package.,"Abstract:
The IBM ACRITH package of numerical software is advertised as reliable and easy to use; but sometimes its results must astonish or confuse a naive user. This report exhibits a few of the surprises. For instance, a finite continued fraction, easy to evaluate in two dozen keystrokes on a handheld calculator, causes ACRITH to overflow either exponent range or 15 Megabytes of virtual memory. Lacking access to source code, we must speculate to explain the anomalies. Some seem attributable to small bugs in the code; some to optimistic claims or oversimplifications in the code's documentation; some to flaws in the doctrine underlying the code. We conclude that different techniques than used by ACRITH might have been about as accurate and yet more economical, robust and perspicuous."
1984,A Proposed Radix- and Word-length-independent Standard for Floating-point Arithmetic.,"Abstract:
Besides making the proposed IEEE 854 standard available for comment, this article explains how to overcome some of its implementation problems."
1975,Problem #9: an ellipse problem.,"The problem concerns four variables a,b,c,d to be interpreted as centre (c,d) and principal semiaxes a,b of an ellipse [EQUATION] We wish to know when E lies inside the unit disk D: x2 + y2 â‰¤ 1. More precisely, we seek a set of polynomials {Pj(a2, b2, c2, d2,)} for j=1,2,...,n with the property that Eâ‰¤Dif and only if all Pj(a2, b2, c2, d2,) â‰¤0. It is known that n> 1, but n should be minimal."
1974,When are pivotal interchanges not necessary?,"Solving a linear system Ax = b by Gaussian Elimination usually entails pivotal inter-changes designed to inhibit that explosive growth of intermediate results which would otherwise, through roundoff, vitiate the calculation. But these interchanges, motivated by numerical desiderata, frequently conflict with combinatorial desiderata like ""Sparsity"". We shall show that two special cases in which interchanges are well known not to be needed for stability, namely, when A is positive definite or diagonally dominant, are examples of a more frequent situation; A's field of values lies in a half-plane not containing zero. This situation, which is associated with certain electric networks and some boundary value problems, allows at least in principle for an estimate of the number of extra guard digits that need be carried to prevent explosive growth from blighting results obtained without interchanges."
1971,A Survey of Error Analysis.,"Rounding error is just one kind of error, and an easier kind to analyze than some others. Error and uncertainty in data is a more important kind, and not so easy to estimate nor analyze; here is where error analysts are currently busiest. The most refractory kind of error is attributable to flaws in the design of computer systems, both hardware and software, caused primarily by misconceptions about the other kinds of error."
1968,On the convergence of a practical QR algorithm.,n/a
1965,Pracniques: further remarks on reducing truncation errors.,n/a
1964,A fortran post-mortem procedure.,n/a
1963,Algorithm 167: calculation of confluent divided differences.,n/a
1963,Algorithm 169: Newton interpolation with forward divided differences.,n/a
1963,Algorithm 168: Newton interpolation with backward divided differences.,n/a
