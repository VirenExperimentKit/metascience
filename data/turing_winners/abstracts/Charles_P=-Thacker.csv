2013,Reconfigurable computing in the era of post-silicon scaling [panel discussion].,"Abstract:
Summary form only given, as follows. Although transistor densities continue to scale exponentially, the failure of Dennard Scaling prevents us from maximally utilizing die area in future power-constrained multicore processors-a phenomenon referred to as ""Dark Silicon"". Alternative energy-efficient architectures based on FPGAs, GPGPUs, ASICs, MPPAs, etc. are likely to continue on an exponential scaling trajectory while outperforming conventional architectures by an order-of-magnitude or more. With the impending threat of dark silicon, there is a critical window of opportunity for reconfigurable computing to become a mainstream ingredient and driver of future, scalable computer architectures. Before this can happen, major challenges and opportunities must be addressed: (1) how to gracefully integrate reconfigurable computing into existing software and hardware ecosystems, (2) how to build tools, languages, and compilers for agile application development and debugging, (3) how to identify and exploit emerging applications in datacenters and in energy-constrained form factors, (4) how to train and educate students and practitioners to use these systems in sustainable ways, and (5) how to define new and stable boundaries between software and hardware that make it easier to exploit reconfigurable computing. This panel brings together pioneers and experts in computer architecture and reconfigurable computing to discuss opportunities and challenges in the wake of dark silicon."
2012,Computer Architecture.,"Sixty-five years ago, Alan Turing produced a proposal for the construction of a general-purpose computer, the Automatic Computing Engine, or ACE. Subsequently built at the U.K. National Physical Laboratory, it was briefly the fastest computer in the world. Although its architecture was quite different from the arrangement proposed by Von Neumann and others that eventually came to dominate the computing landscape, examining it gives us a chance to understand some of the tradeoffs that early computer architects explored.
The panel will examine the ACE to provide a setting for the discussions that follow, in which they will explore some of the architectural tradeoffs that have been made in the past, are still being made today, and which will shape the direction of computing in the future. What would Alan Turing have thought about the impact that computers have had on society? What would he have thought about the warehouse-scale computing that makes possible a realization of Vannevar Bush's 1945 Memex vision? What about the possibility of quantum computing? The panelists will discuss these topics as well as the progress and future of academic computer architecture research."
2010,Improving the future by examining the past.,"During the last fifty years, the technology underlying computer systems has improved dramatically. As technology has evolved, designers have made a series of choices in the way it was applied in computers. In some cases, decisions that were made in the twentieth century make less sense in the twenty-first. Conversely, paths not taken might now be more attractive given the state of technology today, particularly in light of the limits the field is facing, such as the increasing gap between processor speed and storage access times and the difficulty of cooling today's computers.
In this talk, I'll discuss some of these choices and suggest some possible changes that might make computing better in the twenty-first century."
2007,A design for high-performance flash disks.,Most commodity flash disks exhibit very poor performance when presented with writes that are not sequentially ordered. We argue that performance can be significantly improved through the addition of sufficient RAM to hold data structures describing a fine-grain mapping between disk logical blocks and physical flash addresses. We present a design that accomplishes this.
1993,The Alpha Demonstration Unit: A High-Performance Multiprocessor.,n/a
1993,High Speed Switch Scheduling for Local Area Networks.,"Current technology trends make it possible to build communication networks that can support high-performance distributed computing. This paper describes issues in the design of a prototype switch for an arbitrary topology point-to-point network with link speeds of up to 1 Gbit/s. The switch deals in fixed-length ATM-style cells, which it can process at a rate of 37 million cells per second. It provides high bandwidth and low latency for datagram traffic. In addition, it supports real-time traffic by providing bandwidth reservations with guaranteed latency bounds. The key to the switch's operation is a technique called parallel iterative matching, which can quickly identify a set of conflict-free cells for transmission in a time slot. Bandwidth reservations are accommodated in the switch by building a fixed schedule for transporting cells from reserved flows across the switch; parallel iterative matching can fill unused slots with datagram traffic. Finally, we note that parallel iterative matching may not allocate bandwidth fairly among flows of datagram traffic. We describe a technique called statistical matching, which can be used to ensure fairness at the switch and to support applications with rapidly changing needs for guaranteed bandwidth."
1992,The Alpha Demonstration Unit: A High-performance Multiprocessor for Software and Chip Development.,n/a
1992,High Speed Switch Scheduling for Local Area Networks.,"Current technology trends make it possible to build communication networks that can support high-performance distributed computing. This paper describes issues in the design of a prototype switch for an arbitrary topology point-to-point network with link speeds of up to 1 Gbit/s. The switch deals in fixed-length ATM-style cells, which it can process at a rate of 37 million cells per second. It provides high bandwidth and low latency for datagram traffic. In addition, it supports real-time traffic by providing bandwidth reservations with guaranteed latency bounds. The key to the switch's operation is a technique called parallel iterative matching, which can quickly identify a set of conflict-free cells for transmission in a time slot. Bandwidth reservations are accommodated in the switch by building a fixed schedule for transporting cells from reserved flows across the switch; parallel iterative matching can fill unused slots with datagram traffic. Finally, we note that parallel iterative matching may not allocate bandwidth fairly among flows of datagram traffic. We describe a technique called statistical matching, which can be used to ensure fairness at the switch and to support applications with rapidly changing needs for guaranteed bandwidth."
1991,"Autonet: A High-Speed, Self-Configuring Local Area Network Using Point-to-Point Links.","Abstract:
Autonet is a self-configuring local area network composed of switches interconnected by 100 Mb/s, full-duplex, point-to-point links. The switches contain 12 ports that are internally connected by a full crossbar. Switches use cut-through to achieve a packet forwarding latency as low as 2 ms/switch. Any switch port can be cabled to any other switch port or to a host network controller. A processor in each switch monitors the network's physical configuration. A distributed algorithm running on the switch processor computes the routes packets are to follow and fills in the packet forwarding table in each switch. With Autonet, distinct paths through the set of network links can carry packets in parallel, allowing many pairs of hosts to communicate simultaneously at full link bandwidth. A 30-switch network with more than 100 hosts has been the service network for Digital's Systems Research Center since February 1990.< >"
1988,Firefly: A Multiprocessor Workstation.,"Abstract:
The Firefly shared-memory multiprocessor workstation system consists of from one to nine VLSI VAX processors, each with a floating-point accelerator and a cache. The caches are coherent, so that all processors see a consistent view of main memory. The Firefly runs a software system (Topaz) that emulates the Ultrix system call interface, and in addition provides support for multiprocessing through multiple threads of control in a single address space. Communication is provided uniformly through the use of remote procedure call. The authors describe the goals, hardware, software system, and performance of the Firefly, and discuss the extent to which the development has been successful in providing software to take advantage of multiprocessing.< >"
1987,Firefly: A Multiprocessor Workstation.,"Firefly is a shared-memory multiprocessor workstation that contains from one to seven MicroVAX 78032 processors, each with a floating point unit and a sixteen kilobyte cache. The caches are coherent, so that all processors see a consistent view of main memory. A system may contain from four to sixteen megabytes of storage. Input-output is done via a standard DEC QBus. Input-output devices are an Ethernet controller, fixed disks, and a monochrome 1024 x 768 display with keyboard and mouse. Optional hardware includes a high resolution color display and a controller for high capacity disks. Figure 1 is a system block diagram.The Firefly runs a software system that emulates the Ultrix system call interface. It also supports medium- and coarse-grained multiprocessing through multiple threads of control in a single address space. Communications are implemented uniformly through the use of remote procedure calls.This paper describes the goals, architecture, implementation and performance analysis of the Firefly. It then presents some measurements of hardware performance, and discusses the degree to which SRC has been successful in producing software to take advantage of multiprocessing."
