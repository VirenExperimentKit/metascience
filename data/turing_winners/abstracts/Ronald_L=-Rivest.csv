2018,From Battlefields to Elections: Winning Strategies of Blotto and Auditing Games.,"Mixed strategies are often evaluated based on the expected payoff that they guarantee. This is not always desirable. In this paper, we consider games for which maximizing the expected payoff deviates from the actual goal of the players. To address this issue, we introduce the notion of a (u,p)-maxmin strategy which ensures receiving a minimum utility of u with probability at least p. We then give approximation algorithms for the problem of finding a (u, p)-maxmin strategy for these games.

The first game that we consider is Colonel Blotto, a well-studied game that was introduced in 1921. In the Colonel Blotto game, two colonels divide their troops among a set of battlefields. Each battlefield is won by the colonel that puts more troops in it. The payoff of each colonel is the weighted number of battlefields that she wins. We show that maximizing the expected payoff of a player does not necessarily maximize her winning probability for certain applications of Colonel Blotto. For example, in presidential elections, the playersí goal is to maximize the probability of winning more than half of the votes, rather than maximizing the expected number of votes that they get. We give an exact algorithm for a natural variant of continuous version of this game. More generally, we provide constant and logarithmic approximation algorithms for finding (u, p)-maxmin strategies.

We also introduce a security game version of Colonel Blotto which we call auditing game. It is played between two players, a defender and an attacker. The goal of the defender is to prevent the attacker from changing the outcome of an instance of Colonel Blotto. Again, maximizing the expected payoff of the defender is not necessarily optimal. Therefore we give a constant approximation for (u, p)-maxmin strategies.
"
2017,When Is an Election Verifiable?,"Abstract:
Verifiable elections currently require voter-verifiable paper ballots, demonstrably adequate custody of those ballots, and well-designed audits of the results based on manual inspection of those ballots."
2017,BatchVote: Voting Rules Designed for Auditability.,"Abstract
We propose a family of novel social choice functions. Our goal is to explore social choice functions for which ease of auditing is a primary design goal, instead of being ignored or left as a puzzle to solve later.
Our proposal, ‚ÄúBatchVote,‚Äù creates a social choice function f from an arbitrary ‚Äúinner‚Äù social choice function g, such as instant-runoff voting (IRV), and an integer B, the number of batches.
We aim to preserve flexibility by allowing g to be arbitrary, while providing the ease of auditing of a plurality election.
To compute the winner of an election of n votes, the social choice function f partitions the votes into B batches of roughly the same size, pseudorandomly. The social choice function g is applied to each batch. The election winner, according to f, is the weighted plurality winner for the B outcomes, where the weight of each batch is the number of votes it contains. The social choice function f may be viewed as an ‚Äúinterpolation‚Äù between plurality (which is easily auditable) and g (which need not be).
Auditing is simple by design: we can view f as being a (weighted) plurality election by B ‚Äúsupervoters,‚Äù where the bth supervoter‚Äôs vote is determined by applying g to the votes in batch b, and the weight of her vote is the number of votes in her batch. Since plurality elections are easy to audit, the election output can be audited by checking a random sample of ‚Äúsupervotes‚Äù against the corresponding paper records."
2017,Marked Mix-Nets.,"Abstract
We propose a variant mix-net method, which we call a ‚Äúmarked mix-net‚Äù. Marked mix-nets avoid the extra cost associated with verifiability (producing a proof of correct mixing operation), while offering additional assurances about the privacy of the messages, compared to a non-verifiable mix-net.
With a marked mix-net, each mix-server adds an extra secret mark in each ciphertext, and the input ciphertexts are made non-malleable but still re-randomizable (RCCA).
Marked mix-nets appear to be a good fit for the mix-net requirements of voting systems that need a mix-net for anonymity but where correctness is guaranteed through independent mechanisms. Our work investigates applications to STAR-Vote, but other applications could be explored, e.g., in Pr√™t-√†-Voter, Selene or Wombat."
2017,Time-Space Trade-offs in Population Protocols.,"Population protocols are a popular model of distributed computing, in which randomly-interacting agents with little computational power cooperate to jointly perform computational tasks. Inspired by developments in molecular computation, and in particular DNA computing, recent algorithmic work has focused on the complexity of solving simple yet fundamental tasks in the population model, such as leader election (which requires convergence to a single agent in a special ìleaderî state), and majority (in which agents must converge to a decision as to which of two possible initial states had higher initial count). Known results point towards an inherent trade-off between the time complexity of such algorithms, and the space complexity, i.e. size of the memory available to each agent.

In this paper, we explore this trade-off and provide new upper and lower bounds for majority and leader election. First, we prove a unified lower bound, which relates the space available per node with the time complexity achievable by a protocol: for instance, our result implies that any protocol solving either of these tasks for n agents using O(log log n) states must take ?(n/polylogn) expected time. This is the first result to characterize time complexity for protocols which employ super-constant number of states per node, and proves that fast, poly-logarithmic running times require protocols to have relatively large space costs.

On the positive side, we give algorithms showing that fast, poly-logarithmic convergence time can be achieved using O (log2 n) space per node, in the case of both tasks. Overall, our results highlight a time complexity separation between O (log log n) and ?(log2 n) state space size for both majority and leader election in population protocols, and introduce new techniques, which should be applicable more broadly.
"
2017,Public Evidence from Secret Ballots.,"Abstract
Elections seem simple‚Äîaren‚Äôt they just about counting? But they have a unique, challenging combination of security and privacy requirements. The stakes are high; the context is adversarial; the electorate needs to be convinced that the results are correct; and the secrecy of the ballot must be ensured. They also have practical constraints: time is of the essence, and voting systems need to be affordable and maintainable, as well as usable by voters, election officials, and pollworkers. It is thus not surprising that voting is a rich research area spanning theory, applied cryptography, practical systems analysis, usable security, and statistics. Election integrity involves two key concepts: convincing evidence that outcomes are correct and privacy, which amounts to convincing assurance that there is no evidence about how any given person voted. These are obviously in tension. We examine how current systems walk this tightrope."
2015,Keys under doormats.,Mandating insecurity by requiring government access to all data and communications.
2015,Keys under doormats: mandating insecurity by requiring government access to all data and communications.,"Twenty years ago, law enforcement organizations lobbied to require data and communication services to engineer their products to guarantee law enforcement access to all data. After lengthy debate and vigorous predictions of enforcement channels ìgoing dark,î these attempts to regulate security technologies on the emerging Internet were abandoned. In the intervening years, innovation on the Internet flourished, and law enforcement agencies found new and more effective means of accessing vastly larger quantities of data. Today, there are again calls for regulation to mandate the provision of exceptional access mechanisms. In this article, a group of computer scientists and security experts, many of whom participated in a 1997 study of these same topics, has convened to explore the likely effects of imposing extraordinary access mandates.

We have found that the damage that could be caused by law enforcement exceptional access requirements would be even greater today than it would have been 20 years ago. In the wake of the growing economic and social cost of the fundamental insecurity of todayís Internet environment, any proposals that alter the security dynamics online should be approached with caution. Exceptional access would force Internet system developers to reverse ìforward secrecyî design practices that seek to minimize the impact on user privacy when systems are breached. The complexity of todayís Internet environment, with millions of apps and globally connected services, means that new law enforcement requirements are likely to introduce unanticipated, hard to detect security flaws. Beyond these and other technical vulnerabilities, the prospect of globally deployed exceptional access systems raises difficult problems about how such an environment would be governed and how to ensure that such systems would respect human rights and the rule of law."
2014,Picture-Hanging Puzzles.,"Abstract
We show how to hang a picture by wrapping rope around n nails, making a polynomial number of twists, such that the picture falls whenever any k out of the n nails get removed, and the picture remains hanging when fewer than k nails get removed. This construction makes for some fun mathematical magic performances. More generally, we characterize the possible Boolean functions characterizing when the picture falls in terms of which nails get removed as all monotone Boolean functions. This construction requires an exponential number of twists in the worst case, but exponential complexity is almost always necessary for general functions."
2013,"FlipIt: The Game of ""Stealthy Takeover"".","Recent targeted attacks have increased significantly in sophistication, undermining the fundamental assumptions on which most cryptographic primitives rely for security. For instance, attackers launching an Advanced Persistent Threat (APT) can steal full cryptographic keys, violating the very secrecy of ìsecretî keys that cryptographers assume in designing secure protocols. In this article, we introduce a game-theoretic framework for modeling various computer security scenarios prevalent today, including targeted attacks. We are particularly interested in situations in which an attacker periodically compromises a system or critical resource completely, learns all its secret information and is not immediately detected by the system owner or defender. We propose a two-player game between an attacker and defender called FlipIt or The Game of ìStealthy Takeover.î In FlipIt, players compete to control a shared resource. Unlike most existing games, FlipIt allows players to move at any given time, taking control of the resource. The identity of the player controlling the resource, however, is not revealed until a player actually moves. To move, a player pays a certain move cost. The objective of each player is to control the resource a large fraction of time, while minimizing his total move cost. FlipIt provides a simple and elegant framework in which we can formally reason about the interaction between attackers and defenders in practical scenarios. In this article, we restrict ourselves to games in which one of the players (the defender) plays with a renewal strategy, one in which the intervals between consecutive moves are chosen independently and uniformly at random from a fixed probability distribution. We consider attacker strategies ranging in increasing sophistication from simple periodic strategies (with moves spaced at equal time intervals) to more complex adaptive strategies, in which moves are determined based on feedback received during the game. For different classes of strategies employed by the attacker, we determine strongly dominant strategies for both players (when they exist), strategies that achieve higher benefit than all other strategies in a particular class. When strongly dominant strategies do not exist, our goal is to characterize the residual game consisting of strategies that are not strongly dominated by other strategies. We also prove equivalence or strict inclusion of certain classes of strategies under different conditions. Our analysis of different FlipIt variants teaches cryptographers, system designers, and the community at large some valuable lessons:
1.
Systems should be designed under the assumption of repeated total compromise, including theft of cryptographic keys. FlipIt provides guidance on how to implement a cost-effective defensive strategy.

 
2.
Aggressive play by one player can motivate the opponent to drop out of the game (essentially not to play at all). Therefore, moving fast is a good defensive strategy, but it can only be implemented if move costs are low. We believe that virtualization has a huge potential in this respect.

 
3.
Close monitoring of oneís resources is beneficial in detecting potential attacks faster, gaining insight into attackerís strategies, and scheduling defensive moves more effectively.

 
Interestingly, FlipIt finds applications in other security realms besides modeling of targeted attacks. Examples include cryptographic key rotation, password changing policies, refreshing virtual machines, and cloud auditing."
2013,Honeywords: making password-cracking detectable.,"We propose a simple method for improving the security of hashed passwords: the maintenance of additional ``honeywords'' (false passwords) associated with each user's account. An adversary who steals a file of hashed passwords and inverts the hash function cannot tell if he has found the password or a honeyword. The attempted use of a honeyword for login sets off an alarm. An auxiliary server (the ``honeychecker'') can distinguish the user password from honeywords for the login routine, and will set off an alarm if a honeyword is submitted."
2013,Drifting Keys: Impersonation detection for constrained devices.,"Abstract:
We introduce Drifting Keys (DKs), a simple new approach to detecting device impersonation. DKs enable detection of complete compromise by an attacker of the device and its secret state, e.g., cryptographic keys. A DK evolves within a device randomly over time. Thus an attacker will create DKs that randomly diverge from those in the original, valid device over time, alerting a trusted verifier to the attack. DKs may be transmitted unidirectionally from a device, eliminating interaction between the device and verifier. Device emissions of DK values can be quite compact - even just a single bit - and DK evolution and emission require minimal computation. Thus DKs are well suited for highly constrained devices, such as sensors and hardware authentication tokens. We offer a formal adversarial model for DKs, and present a simple scheme that we prove essentially optimal (undominated) for a natural class of attack timelines. We explore application of this scheme to one-time passcode authentication tokens. Using the logs of a large enterprise, we experimentally study the effectiveness of DKs in detecting the compromise of such tokens."
2012,"Information, Data, Security in a Networked Future.","The digital information revolution begins as giants such as Alan Turing, Claude Shannon and John von neumann, among many others, recognize the power of digital representations and programmable computers. Although rooted in the technology of his time, Vannevar Bush's portrait of the information revolution has emerged and fl ourished especially in the form of the World Wide Web resting atop the global Internet.
The panelists will explore some specifi cs of the digital information revolution, notably theory and practice in securing, authenticating and maintaining the integrity of information (Cerf); and roots of modern cryptography and current topics in this area (rivest and Shamir). They will also gain insight into the long-term problem of identifying, fi nding, and assuring the integrity of digital objects in the most general sense of that term (Kahn). Finally, they look at how our understanding of computer science is changing (Hopcroft) and how that evolution will affect the digital world in which are we spending an increasing fraction of our daily lives."
2012,Hourglass schemes: how to prove that cloud files are encrypted.,"We consider the following challenge: How can a cloud storage provider prove to a tenant that it's encrypting files at rest, when the provider itself holds the corresponding encryption keys? Such proofs demonstrate sound encryption policies and file confidentiality. (Cheating, cost-cutting, or misconfigured providers may bypass the computation/management burdens of encryption and store plaintext only.)
To address this problem, we propose hourglass schemes, protocols that prove correct encryption of files at rest by imposing a resource requirement (e.g., time, storage or computation) on the process of translating files from one encoding domain (i.e., plaintext) to a different, target domain (i.e., ciphertext). Our more practical hourglass schemes exploit common cloud infrastructure characteristics, such as limited file-system parallelism and the use of rotational hard drives for at-rest files. For files of modest size, we describe an hourglass scheme that exploits trapdoor one-way permutations to prove correct file encryption whatever the underlying storage medium.
We also experimentally validate the practicality of our proposed schemes, the fastest of which incurs minimal overhead beyond the cost of encryption. As we show, hourglass schemes can be used to verify properties other than correct encryption, e.g., embedding of ""provenance tags"" in files for tracing the source of leaked files. Of course, even if a provider is correctly storing a file as ciphertext, it could also store a plaintext copy to service tenant requests more efficiently. Hourglass schemes cannot guarantee ciphertext-only storage, a problem inherent when the cloud manages keys. By means of experiments in Amazon EC2, however, we demonstrate that hourglass schemes provide strong incentives for economically rational cloud providers against storage of extra plaintext file copies."
2012,Picture-Hanging Puzzles.,"Abstract
We show how to hang a picture by wrapping rope around n nails, making a polynomial number of twists, such that the picture falls whenever any k out of the n nails get removed, and the picture remains hanging when fewer than k nails get removed. This construction makes for some fun mathematical magic performances. More generally, we characterize the possible Boolean functions characterizing when the picture falls in terms of which nails get removed as all monotone Boolean functions. This construction requires an exponential number of twists in the worst case, but exponential complexity is almost always necessary for general functions."
2012,Defending against the Unknown Enemy: Applying FlipIt to System Security.,"Abstract
Most cryptographic systems carry the basic assumption that entities are able to preserve the secrecy of their keys. With attacks today showing ever increasing sophistication, however, this tenet is eroding. ‚ÄúAdvanced Persistent Threats‚Äù (APTs), for instance, leverage zero-day exploits and extensive system knowledge to achieve full compromise of cryptographic keys and other secrets. Such compromise is often silent, with defenders failing to detect the loss of private keys critical to protection of their systems. The growing virulence of today‚Äôs threats clearly calls for new models of defenders‚Äô goals and abilities.
In this paper, we explore applications of FlipIt, a novel game-theoretic model of system defense introduced in [14]. In FlipIt, an attacker periodically gains complete control of a system, with the unique feature that system compromises are stealthy, i.e., not immediately detected by the system owner, called the defender. We distill out several lessons from our study of FlipIt and demonstrate their application to several real-world problems, including password reset policies, key rotation, VM refresh and cloud auditing."
2012,A Bayesian Method for Auditing Elections.,"We propose an approach to post-election auditing based on Bayesian principles, and give experimental evidence for its efficiency and effectiveness. We call such an audit a ìBayes auditî. It aims to control the probability of miscertification (certifying a wrong election outcome). The miscertification probability is computed using a Bayesian model based on information gathered by the audit so far.

A Bayes audit is a single-ballot audit method applicable to any voting system (e.g. plurality, approval, IRV, Borda, Schulze, etc.) as long as the number of ballot types is not too large. The method requires only the ability to randomly sample single ballots and the ability to compute the election outcome for a profile of ballots. A Bayes audit does not require the computation of a ìmargin of victoryî in order to get started.

Bayes audits are applicable both to ballot-polling audits, which work just from the paper ballots, and to comparison audits, which work by comparing the paper ballots to their electronic representations. The procedure is quite simple and can be described on a single page. The Bayes audit uses an efficient method (which may be based on the use of gamma variates or on PÛlya's Urn) for simulating a Bayesian posterior distribution on the tally of a profile of ballots.

A Bayes audit is very similar to single-ballot risk-limiting audits. However, since Bayes audits are based on different principles, the precise relationship between risk-limiting audits and Bayes audits remains open. We provide some initial experimental results indicating that Bayes audits are quite efficient, requiring few ballots to be examined, and that the miscertification rate is indeed kept small, even for very close elections.

We provide some initial experimental results indicating that Bayes audits are quite efficient, requiring few ballots to be examined, and that the miscertification rate is indeed kept small, even for very close elections."
2011,The invertibility of the XOR of rotations of a binary word.,"We prove the following result regarding operations on a binary word whose length is a power of two: computing the exclusive-or of a number of rotated versions of the word is an invertible (one-to-one) operation if and only if the number of versions combined is odd. (This result is not new; there is at least one earlier proof, due to Thomsen [Cryptographic hash functions, PhD thesis, Technical University of Denmark, 28 November 2008]. Our proof may be new.)
"
2011,Tweakable Block Ciphers.,"Abstract
A common trend in applications of block ciphers over the past decades has been to employ block ciphers as one piece of a ‚Äúmode of operation‚Äù‚Äîpossibly, a way to make a secure symmetric-key cryptosystem, but more generally, any cryptographic application. Most of the time, these modes of operation use a wide variety of techniques to achieve a subgoal necessary for their main goal: instantiation of ‚Äúessentially different‚Äù instances of the block cipher.
We formalize a cryptographic primitive, the ‚Äútweakable block cipher.‚Äù Such a cipher has not only the usual inputs‚Äîmessage and cryptographic key‚Äîbut also a third input, the ‚Äútweak.‚Äù The tweak serves much the same purpose that an initialization vector does for CBC mode or that a nonce does for OCB mode. Our abstraction brings this feature down to the primitive block-cipher level, instead of incorporating it only at the higher modes-of-operation levels. We suggest that (1) tweakable block ciphers are easy to design, (2) the extra cost of making a block cipher ‚Äútweakable‚Äù is small, and (3) it is easier to design and prove the security of applications of block ciphers that need this variability using tweakable block ciphers."
2011,How to tell if your cloud files are vulnerable to drive crashes.,"This paper presents a new challenge--verifying that a remote server is storing a file in a fault-tolerant manner, i.e., such that it can survive hard-drive failures. We describe an approach called the Remote Assessment of Fault Tolerance (RAFT). The key technique in a RAFT is to measure the time taken for a server to respond to a read request for a collection of file blocks. The larger the number of hard drives across which a file is distributed, the faster the read-request response. Erasure codes also play an important role in our solution. We describe a theoretical framework for RAFTs and offer experimental evidence that RAFTs can work in practice in several settings of interest."
2010,Corrections to scantegrity II: end-to-end verifiability by voters of optical scan elections through confirmation codes.,"Abstract:
In the above titled paper (ibid., vol. 4, no. 4, pp. 611-627, Dec. 09), due to a production error, the affiliations of two of the authors were listed incorrectly. The correct affiliations are presented here. Also, the name of the last author in the affiliations footnote was printed incorrectly. The correct name is P. Y. A. Ryan."
2010,Scantegrity Mock Election at Takoma Park.,"We report on our experiences and lessons learned using Scantegrity II in a mock election held April 11, 2009, in Takoma Park, Maryland (USA). Ninetyfive members of the community participated in our test of this voting system proposed for the November 2009 municipal election. Results helped improve the system for the November binding election."
2010,Scantegrity II Municipal Election at Takoma Park: The First E2E Binding Governmental Election with Ballot Privacy.,"On November 3, 2009, voters in Takoma Park, Maryland, cast ballots for the mayor and city council members using the Scantegrity II voting system--the first time any end-to-end (E2E) voting system with ballot privacy has been used in a binding governmental election. This case study describes the various efforts that went into the election--including the improved design and implementation of the voting system, streamlined procedures, agreements with the city, and assessments of the experiences of voters and poll workers.

The election, with 1728 voters from six wards, involved paper ballots with invisible-ink confirmation codes, instant-runoff voting with write-ins, early and absentee (mail-in) voting, dual-language ballots, provisional ballots, privacy sleeves, any-which-way scanning with parallel conventional desktop scanners, end-to-end verifiability based on optional web-based voter verification of votes cast, a full hand recount, thresholded authorities, three independent outside auditors, fully-disclosed software, and exit surveys for voters and pollworkers.

Despite some glitches, the use of Scantegrity II was a success, demonstrating that E2E cryptographic voting systems can be effectively used and accepted by the general public."
2010,"A Modular Voting Architecture (""Frog Voting"").","Abstract
This paper presents a new framework‚Äìa reference architecture‚Äìfor voting that we feel has many attractive features. It is not a machine design, but rather a framework that will stimulate innovation and design. It is potentially the standard architecture for all future voting equipment. The ideas expressed here are subject to improvement and further research.
An early version of this paper appeared in [2]. This version of the paper is very similar, but contains a postscript (Section 8) providing commentary and discussion of perspectives on this proposal generated during the intervening years between 2001 and 2008."
2009,Guest editorial: special issue on electronic voting.,"Abstract:
The 13 papers in this special issue focus on electronic voting."
2009,Scantegrity II: end-to-end verifiability by voters of optical scan elections through confirmation codes.,"Abstract:
Scantegrity II is an enhancement for existing paper ballot systems. It allows voters to verify election integrity - from their selections on the ballot all the way to the final tally - by noting codes and checking for them online. Voters mark Scantegrity II ballots just as with conventional optical scan, but using a special ballot marking pen. Marking a selection with this pen makes legible an otherwise invisible preprinted confirmation code. Confirmation codes are independent and random for each potential selection on each ballot. To verify that their individual votes are recorded correctly, voters can look up their ballot serial numbers online and verify that their confirmation codes are posted correctly. The confirmation codes do not allow voters to prove how they voted. However, the confirmation codes constitute convincing evidence of error or malfeasance in the event that incorrect codes are posted online. Correctness of the final tally with respect to the published codes is proven by election officials in a manner that can be verified by any interested party. Thus, compromise of either ballot chain of custody or the software systems cannot undetectably affect election integrity. Scantegrity II has been implemented and tested in small elections in which ballots were scanned either at the polling place or centrally. Preparations for its use in a public sector election have commenced."
2009,"Indifferentiability of Permutation-Based Compression Functions and Tree-Based Modes of Operation, with Applications to MD6.","Abstract
MD6 [17] is one of the earliest announced SHA-3 candidates, presented by Rivest at CRYPTO‚Äô08 [16]. Since then, MD6 has received a fair share of attention and has resisted several initial cryptanalytic attempts [1,11].
Given the interest in MD6, it is important to formally verify the soundness of its design from a theoretical standpoint. In this paper, we do so in two ways: once for the MD6 compression function and once for the MD6 mode of operation. Both proofs are based on the indifferentiability framework of Maurer et al. [13](also see [9]).
The first proof demonstrates that the ‚Äúprepend/map/chop‚Äù manner in which the MD6 compression function is constructed yields a compression function that is indifferentiable from a fixed-input-length (FIL), fixed-output-length random oracle.
The second proof demonstrates that the tree-based manner in which the MD6 mode of operation is defined yields a hash function that is indifferentiable from a variable-input-length (VIL), fixed-output-length random oracle.
Both proofs are rather general and apply not only to MD6 but also to other sufficiently similar hash functions.
These results may be interpreted as saying that the MD6 design has no structural flaws that make its input/output behavior clearly distinguishable from that of a VIL random oracle, even for an adversary who has access to inner components of the hash function. It follows that, under plausible assumptions about those inner components, the MD6 hash function may be safely plugged into any application proven secure assuming a monolithic VIL random oracle."
2008,On Auditing Elections When Precincts Have Different Sizes.,"We address the problem of auditing an election when precincts may have different sizes. Prior work in this field has emphasized the simpler case when all precincts have the same size. Using auditing methods developed for use with equal-sized precincts can, however, be inefficient or result in loss of statistical confidence when applied to elections with variable-sized precincts.

We survey, evaluate, and compare a variety of approaches to the variable-sized precinct auditing problem, including the SAFE method [11] which is based on theory developed for equal-sized precincts. We introduce new methods such as the negative-exponential method ""NEGEXP"" that select precincts independently for auditing with predetermined probabilities, and the ""PPEBWR"" method that uses a sequence of rounds to select precincts with replacement according to some predetermined probability distribution that may depend on error bounds for each precinct (hence the name PPEBWR: probability proportional to error bounds, with replacement), where the error bounds may depend on the sizes of the precincts, or on how the votes were cast in each precinct.

We give experimental results showing that NEGEXP and PPEBWR can dramatically reduce (by a factor or two or three) the cost of auditing compared to methods such as SAFE that depend on the use of uniform sampling. Sampling so that larger precincts are audited with appropriately larger probability can yield large reductions in expected number of votes counted in an audit.

We also present the optimal auditing strategy, which is nicely representable as a linear programming problem but only efficiently computable for small elections (fewer than a dozen precincts). We conclude with some recommendations for practice. "
2008,Scantegrity II: End-to-End Verifiability for Optical Scan Election Systems using Invisible Ink Confirmation Codes.,"Scantegrity II is an enhancement for existing paper ballot systems. It allows voters to verify election integrity - from their selections on the ballot all the way to the final tally - by noting codes and checking for them online. Voters mark Scantegrity II ballots just as with conventional optical scan, but using a special ballot marking pen. Marking a selection with this pen makes legible an otherwise invisible preprinted confirmation code. Confirmation codes are independent and random for each potential selection on each ballot. To verify that their individual votes are recorded correctly, voters can look up their ballot serial numbers online and verify that their confirmation codes are posted correctly. The confirmation codes do not allow voters to prove how they voted. However, the confirmation codes constitute convincing evidence of error or malfeasance in the event that incorrect codes are posted online. Correctness of the final tally with respect to the published codes is proven by election officials in a manner that can be verified by any interested party. Thus, compromise of either ballot chain of custody or the software systems cannot undetectably affect election integrity. Scantegrity II has been implemented and tested in small elections in which ballots were scanned either at the polling place or centrally. Preparations for its use in a public sector election have commenced."
2007,Amplifying Collision Resistance: A Complexity-Theoretic Treatment.,"Abstract
We initiate a complexity-theoretic treatment of hardness amplification for collision-resistant hash functions, namely the transformation of weakly collision-resistant hash functions into strongly collision-resistant ones in the standard model of computation. We measure the level of collision resistance by the maximum probability, over the choice of the key, for which an efficient adversary can find a collision. The goal is to obtain constructions with short output, short keys, small loss in adversarial complexity tolerated, and a good trade-off between compression ratio and computational complexity. We provide an analysis of several simple constructions, and show that many of the parameters achieved by our constructions are almost optimal in some sense."
2007,Security of Voting Systems.,n/a
2007,On the Security of the EMV Secure Messaging API (Extended Abstract).,"Abstract
We present new attacks against the EMV financial transaction security system (known in Europe as ‚ÄúChip and PIN‚Äù), specifically on the back-end API support for sending secure messages to EMV smartcards."
2007,Robbing the Bank with a Theorem Prover - (Abstract).,"Abstract
In this work, we present the first automated analysis of security application programming interfaces (security APIs). In particular, we analyze the API of the IBM 4758 CCA, a hardware security module for banking networks. Adapting techniques from formal analyses of security protocols, we model the API purely according its specification and assuming ideal encryption primitives. We then use the automated theorem-prover Otter to analyze this model, combining its standard reasoning strategies with novel techniques of our own (also presented here). In this way, we derive not only all published API-level attacks against the 4758 CCA, but an extension to these attacks as well. Thus, this work represents the first step toward fully-automated, rigorous analyses of security APIs."
2007,On Estimating the Size and Confidence of a Statistical Audit.,"We consider the problem of statistical sampling for auditing elections, and we develop a remarkably simple and easily-calculated upper bound for the sample size necessary for determining with probability at least c if a given set of n objects contains fewer than b ""bad"" objects. While the size of the optimal sample drawn without replacement can be determined with a computer program, our goal is to derive a highly accurate and simple formula that can be used by election officials equipped with only a hand-held calculator. We actually develop several formulae, but the one we recommend for use in practice is: U3(n, b, c) = ?(n - (b - 1)/2) ? (1 - (1 - c)1/b)? = ?(n - (b - 1)/2) ? (1 - exp(ln(1 - c)/b))? As a practical matter, this formula is essentially exact: we prove that it is never too small, and empirical testing for many representative values of n ? 10,000, and b ? n/2, and c ? 0.99 never finds it more than one too large. Theoretically, we show that for all n and b this formula never exceeds the optimal sample size by more than 3 for c ? 0.9975, and by more than (-ln(1 - c))/2 for general c."
2006,How to Leak a Secret: Theory and Applications of Ring Signatures.,"Abstract
In this work we formalize the notion of a ring signature, which makes it possible to specify a set of possible signers without revealing which member actually produced the signature. Unlike group signatures, ring signatures have no group managers, no setup procedures, no revocation procedures, and no coordination: any user can choose any set of possible signers that includes himself, and sign any message by using his secret key and the others‚Äô public keys, without getting their approval or assistance. Ring signatures provide an elegant way to leak authoritative secrets in an anonymous way, to sign casual email in a way that can only be verified by its intended recipient, and to solve other problems in multiparty computations.
Our main contribution lies in the presentation of efficient constructions of ring signatures; the general concept itself (under different terminology) was first introduced by Cramer et al. [CDS94]. Our constructions of such signatures are unconditionally signer-ambiguous, secure in the random oracle model, and exceptionally efficient: adding each ring member increases the cost of signing or verifying by a single modular multiplication and a single symmetric encryption. We also describe a large number of extensions, modifications and applications of ring signatures which were published after the original version of this work (in Asiacrypt 2001)."
2006,Fourth-factor authentication: somebody you know.,"User authentication in computing systems traditionally depends on three factors: something you have (e.g., a hardware token), something you are (e.g., a fingerprint), and something you know (e.g., a password). In this paper, we explore a fourth factor, the social network of the user, that is, somebody you know.Human authentication through mutual acquaintance is an age-old practice. In the arena of computer security, it plays roles in privilege delegation, peer-level certification, help-desk assistance, and reputation networks. As a direct means of logical authentication, though, the reliance of human being on another has little supporting scientific literature or practice.In this paper, we explore the notion of vouching, that is, peer-level, human-intermediated authentication for access control. We explore its use in emergency authentication, when primary authenticators like passwords or hardware tokens become unavailable. We describe a practical, prototype vouching system based on SecurID, a popular hardware authentication token. We address traditional, cryptographic security requirements, but also consider questions of social engineering and user behavior."
2006,Lightweight Email Signatures (Extended Abstract).,"Abstract
We present Lightweight Email Signatures (LES), a simple cryptographic architecture for authenticating email. LES is an extension of DKIM, the recent IETF effort to standardize domain-based email signatures. LES shares DKIM‚Äôs ease of deployment: they both use the DNS to distribute a single public key for each domain. Importantly, LES supports common uses of email that DKIM jeopardizes: multiple email personalities, firewalled ISPs, incoming-only email forwarding services, and other common uses that often require sending email via a third-party SMTP server. In addition, LES does not require DKIM‚Äôs implied intra-domain mechanism for authenticating users when they send email.
LES provides these features using identity-based signatures. Each domain authority generates a master keypair, publishes the public component in the DNS, and stores the private component securely. Using this private component, the authority delivers to each of its users, via email, an individual secret key whose identity string corresponds to the user‚Äôs email address. A sender then signs messages using this individual secret key. A recipient verifies such a signature by querying the appropriate master public key from the DNS, computing the sender‚Äôs public key, and verifying the signature accordingly. As an added bonus, the widespread availability of user-level public keys enables deniable authentication, such as ring signatures. Thus, LES provides email authentication with optional repudiability.
We built a LES prototype to determine its practicality. Basic user tests show that the system is relatively easy to use, and that cryptographic performance, even when using deniable authentication, is well within acceptable range."
2006,Phish and Chips.,"Abstract
This paper surveys existing and new security issues affecting the EMV electronic payments protocol. We first introduce a new price/effort point for the cost of deploying eavesdropping and relay attacks ‚Äì a microcontroller-based interceptor costing less than $100. We look next at EMV protocol failures in the back-end security API, where we describe two new attacks based on chosen-plaintext CBC weaknesses, and on key separation failues. We then consider future modes of attack, specifically looking at combining the phenomenon of phishing (sending unsolicited messages by email, post or phone to trick users into divulging their account details) with chip card sabotage. Our proposed attacks exploit covert channels through the payments network to allow sabotaged cards to signal back their PINS. We hope these new recipes will enliven the debate about the pros and cons of Chip and PIN at both technical and commercial levels."
2006,Scratch & vote: self-contained paper-based cryptographic voting.,"We present Scratch & Vote; (S&V), a cryptographic voting system designed to minimize cost and complexity: (1) ballots are paper-based and can be printed using today's technology, (2) ballots are universally verifiable without electionofficial intervention, and (3) tallying requires only one trustee decryption per race, thanks to homomorphic aggregation. Scratch & Vote combines the multi-candidate election techniques of Baudron et al. with the ballot-casting simplicity of Chaum and Ryan's paper-based techniques. In addition, S&V allows each voter to participate directly in the audit process on election day, prior; to casting their own ballot."
2005,Lightweight Encryption for Email.,"Email encryption techniques have been available for more than a decade, yet none has been widely deployed. The problems of key generation, certification, and distribution have not been pragmatically addressed. We recently proposed a method for implementing a Lightweight Public Key Infrastructure (PKI) for email authentication using recent developments in identitybased cryptography and today's existing Internet infrastructure.

While this solution works well for email authentication, email encryption exhibits a different threat model that requires special treatment. In this work, we discuss how to achieve email encryption and present a realistic deployment and adoption process, while respecting the current functionality and expectations of email."
2004,Access-controlled resource discovery in pervasive networks.,"Networks of the future will be characterized by a variety of computational devices that display a level of dynamism not seen in traditional wired networks. Because of the dynamic nature of these networks, resource discovery is one of the fundamental problems that must be solved. While resource discovery systems are not a novel concept, securing these systems in an efficient and scalable way is challenging. This paper describes the design and implementation of an architecture for access?controlled resource discovery. This system achieves this goal by integrating access control with the Intentional Naming System (INS), a resource discovery and service location system. The integration is scalable, efficient, and fits well within a proxy?based security framework designed for dynamic networks. We provide performance experiments that show how our solution outperforms existing schemes. The result is a system that provides secure, access?controlled resource discovery that can scale to large numbers of resources and users. Copyright © 2004 John Wiley & Sons, Ltd.
"
2004,Peppercoin Micropayments.,"Abstract
We present the ‚ÄùPeppercoin‚Äù method for processing micropayments efficiently. With this method, a fraction of the micropayments received are determined, via a procedure known as ‚Äùcryptographic selection,‚Äù to qualify for upgrade to a macropayment. The merchant deposits the upgraded micropayments as macropayments, and merely logs locally the non-qualifying micropayments. In this manner, the merchant transforms a large collection of small micropayments into a smaller collection of macropayments, of the same total expected value. The merchant pays much less for processing the resulting macropayments, since there are fewer of them. Consumers are billed for exactly the amount they spend, based on auxiliary information recorded in each micropayment. The method is highly secure, and compatible with existing payment mechanisms such as credit cards."
2004,On Permutation Operations in Cipher Design.,"Abstract:
New and emerging applications can change the mix of operations commonly used within computer architectures. It is sometimes surprising when instruction-set architecture (ISA) innovations intended for one purpose are used for other (initially unintended) purposes. We consider recent proposals for the processor support of families of bit-level permutations. From a processor architecture point of view, the ability to support very fast bit-level permutations may be viewed as a further validation of the basic word-orientation of processors, and their ability to support next-generation secure multimedia processing. However, bitwise permutations are also fundamental operations in many cryptographic primitives and we discuss the suitability of these new operations for cryptographic purposes."
2004,On the Notion of Pseudo-Free Groups.,"Abstract
We explore the notion of a pseudo-free group, first introduced by Hohenberger [Hoh03], and provide an alternative stronger definition. We show that if Z
‚àó
n
is a pseudo-free abelian group (as we conjecture), then Z
‚àó
n
also satisfies the Strong RSA Assumption [FO97,CS00,BP97]. Being a ‚Äúpseudo-free abelian group‚Äù may be the strongest natural cryptographic assumption one can make about a group such as Z
‚àó
n
. More generally, we show that a pseudo-free group satisfies several standard cryptographic assumptions, such as the difficulty of computing discrete logarithms."
2003,The blocker tag: selective blocking of RFID tags for consumer privacy.,"We propose the use of ""selective blocking"" by ""blocker tags"" as a way of protecting consumers from unwanted scanning of RFID tags attached to items they may be carrying or wearing.While an ordinary RFID tag is a simple, cheap (e.g. five-cent) passive device intended as an ""electronic bar-code"" for use in supply-chain management, a blocker tag is a cheap passive RFID device that can simulate many ordinary RFID tags simultaneously. When carried by a consumer, a blocker tag thus ""blocks"" RFID readers. It can do so universally by simulating all possible RFID tags. Or a blocker tag can block selectively by simulating only selected subsets of ID codes, such as those by a particular manufacturer, or those in a designated ""privacy zone.We believe that this approach, when used with appropriate care, provides a very attractive alternative for addressing privacy concerns raised by the potential (and likely) widespread use of RFID tags in consumer products.We also discuss possible abuses arising from blocker tags, and means for detecting and dealing with them."
2003,Does Anyone Really Need MicroPayments?,"Abstract
Many cryptographers have tried to develop special technology for transferring tiny amounts of value; the theory being that the computational and/or administrative costs of other payment schemes render them unsuitable for small value transactions. In this panel we discussed two major questions: firstly are the existing systems really not useful for small values and secondly might other models such as flat rate or subscription systems be more suitable anyway, and be possible without the need for small payments?"
2003,Access-Controlled Resource Discovery for Pervasive Networks.,"Networks of the future will be characterized by a variety of computational devices that display a level of dynamism not seen in traditional wired networks. Because of the dynamic nature of these networks, resource discovery is one of the fundamental problems that must be faced. While resource discovery systems are not a novel concept, securing these systems in an efficient and scalable way is challenging. This paper describes the design and implementation of an architecture for access-controlled resource discovery. This system achieves this goal by integrating access control with the Intentional Naming System (INS), a resource discovery and service location system. The integration is scalable, efficient, and fits well within a proxy-based security framework designed for dynamic networks. We provide performance experiments that show how our solution outperforms existing schemes. The result is a system that provides secure, access-controlled resource discovery that can scale to large numbers of resources and users."
2003,Security and Privacy Aspects of Low-Cost Radio Frequency Identification Systems.,"Abstract
Like many technologies, low-cost Radio Frequency Identification (RFID) systems will become pervasive in our daily lives when affixed to everyday consumer items as ‚Äùsmart labels‚Äù. While yielding great productivity gains, RFID systems may create new threats to the security and privacy of individuals or organizations. This paper presents a brief description of RFID systems and their operation. We describe privacy and security risks and how they apply to the unique setting of low-cost RFID devices. We propose several security mechanisms and suggest areas for future research."
2002,Tweakable Block Ciphers.,"Abstract
We propose a new cryptographic primitive, the ‚Äútweakable block cipher.‚Äù Such a cipher has not only the usual inputs ‚Äî message and cryptographic key ‚Äî but also a third input, the ‚Äútweak.‚Äù The tweak serves much the same purpose that an initialization vector does for CBC mode or that a nonce does for OCB mode. Our proposal thus brings this feature down to the primitive block-cipher level, instead of incorporating it only at the higher modes-of-operation levels. We suggest that (1) tweakable block ciphers are easy to design, (2) the extra cost of making a block cipher ‚Äútweakable‚Äù is small, and (3) it is easier to design and prove modes of operation based on tweakable block ciphers."
2002,Micropayments Revisited.,"Abstract
We present new micropayment schemes that are more efficient and user friendly than previous ones.
These schemes reduce bank processing costs by several orders of magnitude, while preserving a simple interface for both users and merchants. The schemes utilize a probabilistic deposit protocol that, in some of the schemes, may be entirely hidden from the users."
2002,Transitive Signature Schemes.,"Abstract
We introduce and provide the first example of a transitive digital signature scheme. Informally, this is a way to digitally sign vertices and edges of a dynamically growing, transitively closed, graph G so as to guarantee the following properties:
Given the signatures of edges (u, v) and (v,w), anyone can easily derive the digital signature of the edge (u,w).
It is computationaly hard for any adversary to forge the digital signature of any new vertex or other edge of G, even if he can request the legitimate signer to digitally sign any number of G‚Äôs vertices and edges of his choice in an adaptive fashion (i.e., even if he can choose which vertices and edges the legitimate signer should sign next after he sees the legitimate signatures of the ones requested so far)."
2002,Privacy Tradeoffs: Myth or Reality? Panel.,"Abstract
We discuss tradeoffs between privacy and other attributes such as security, usability, and advances in technology. We discuss whether such tradeoffs are inherent, or if it is possible to ‚Äúhave it all.‚Äù"
2002,The Untrusted Computer Problem and Camera-Based Authentication.,"Abstract
The use of computers in public places is increasingly common in everyday life. In using one of these computers, a user is trusting it to correctly carry out her orders. For many transactions, particularly banking operations, blind trust in a public terminal will not satisfy most users. In this paper the aim is therefore to provide the user with authenticated communication between herself and a remote trusted computer, via the untrusted computer.
After defining the authentication problem that is to be solved, this paper reduces it to a simpler problem. Solutions to the simpler problem are explored in which the user carries a trusted device with her. Finally, a description is given of two camera-based devices that are being developed."
2002,Proxy-based security protocols in networked mobile devices.,"We describe a resource discovery and communication system designed for security and privacy. All objects in the system, e.g., appliances, wearable gadgets, software agents, and users have associated trusted software proxies that either run on the appliance hardware or on a trusted computer. We describe how security and privacy are enforced using two separate protocols: a protocol for secure device-to-proxy communication, and a protocol for secure proxy-to-proxy communication. Using two separate protocols allows us to run a computationally-inexpensive protocol on impoverished devices, and a sophisticated protocol for resource authentication and communication on more powerful devices.We detail the device-to-proxy protocol for lightweight wireless devices and the proxy-to-proxy protocol which is based on SPKI/SDSI (Simple Public Key Infrastructure / Simple Distributed Security Infrastructure). A prototype system has been constructed, which allows for secure, yet efficient, access to networked, mobile devices. We present a quantitative evaluation of this system using various metrics."
2002,Making Mix Nets Robust for Electronic Voting by Randomized Partial Checking.,"We propose a new technique for making mix nets robust, called randomized partial checking (RPC). The basic idea is that rather than providing a proof of completely correct operation, each server provides strong evidence of its correct operation by revealing a pseudo-randomly selected subset of its input/output relations."
2001,Certificate Chain Discovery in SPKI/SDSI.,"SPKI/SDSI is a novel public-key infrastructure emphasizing naming, groups, ease-of-use, and flexible authorization. To access a protected resource, a client must present to the server a proof that the client is authorized; this proof takes the form of a ìcertificate chainî proving that the client's public key is in one of the groups on the resource's ACL, or that the client's public key has been delegated authority (in one or more stages) from a key in one of the groups on the resource's ACL. While finding such a chain can be nontrivial, due to the flexible naming and delegation capabilities of SPKI/SDSI certificates, we present a practical and efficient algorithm for this problem of ìcertificate chain discoveryî. We also present a tight worst-case bound on its running time, which is polynomial in the length of its input. We also present an extension of our algorithm that is capable of handling ìthreshold subjectsî, where several principals are required to co-sign a request to access a protected resource."
2001,How to Leak a Secret.,"Abstract
In this paper we formalize the notion of a ring signature, which makes it possible to specify a set of possible signers without revealing which member actually produced the signature.Unlike group signatures, ring signatures have no group managers, no setup procedures, no revocation procedures, and no coordination:any user can choose any set of possible signers that includes himself,and sign any message by using his secret key and the others‚Äô public keys,without getting their approval or assistance. Ring signatures provide an elegant way to leak authoritativ secrets in an anonymous way, to sign casual email in a way which can only be verified by its intended recipient, and to solve other problems in multiparty computations. The main contribution of this paper is a new construction of such signatures which is unconditionally signer-ambiguous, provably secure in the random oracle model,and exceptionally efficient:adding each ring member increases the cost of signing or verifying by a single modular multiplication and a single symmetric encryption."
2001,The Business of Electronic Voting.,"Abstract
This work reports on a Financial Cryptography 2001 panel where we concentrated on the emerging business of electronic voting."
2000,RC6 as the AES.,"Each of the finalist algorithms appears to offer adequate security, and each offers a considerable number of advantages. Any of the finalists could serve admirably as the AES. However, each algorithm also has one or more areas where it does not fare quite as well as some other algorithm; none of the finalists is outstandingly superior to the rest. ñ Nechvatal, Barker, Bassham, Burr, Dworkin, Foti, Roback [12] "
1999,Piecemeal Graph Exploration by a Mobile Robot.,"Abstract
We study how a mobile robot can learn an unknown environment in a piecemeal manner. The robot's goal is to learn a complete map of its environment, while satisfying the constraint that it must return every so often to its starting position (for refueling, say). The environment is modeled as an arbitrary, undirected graph, which is initially unknown to the robot. We assume that the robot can distinguish vertices and edges that it has already explored. We present a surprisingly efficient algorithm for piecemeal learning an unknown undirected graph G=(V, E) in which the robot explores every vertex and edge in the graph by traversing at most O(E+V1+o(1)) edges. This nearly linear algorithm improves on the best previous algorithm, in which the robot traverses at most O(E+V2) edges. We also give an application of piecemeal learning to the problem of searching a graph for a ‚Äútreasure.‚Äù"
1999,"Translucent Cryptography - An Alternative to Key Escrow, and Its Implementation via Fractional Oblivious Transfer.","We present an alternative to the controversial ``key-escrow'' techniques for enabling law enforcement and national security access to encrypted communications. Our proposal allows such access with probability p for each message, for a parameter p between 0 and 1 to be chosen (say, by Congress) to provide an appropriate balance between concerns for individual privacy, on the one hand, and the need for such access by law enforcement and national security, on the other. (For example, with p=0.4 , a law-enforcement agency conducting an authorized wiretap which records 100 encrypted conversations would expect to be able to decrypt (approximately) 40 of these conversations; the agency would not be able to decrypt the remaining 60 conversations at all.) Our scheme is remarkably simple to implement, as it requires no prior escrowing of keys.

We implement translucent cryptography based on noninteractive oblivious transfer. Extending the schemes of Bellare and Micali [2], who showed how to transfer a message with probability Ω, we provide schemes for noninteractive fractional oblivious transfer, which allow a message to be transmitted with any given probability p . Our protocol is based on the DiffieóHellman assumption and uses just one El Gamal encryption (two exponentiations), regardless of the value of the transfer probability p . This makes the implementation of translucent cryptography competitive, in efficiency of encryption, with current suggestions for software key escrow."
1999,Improved Analysis of Some Simplified Variants of RC6.,"Abstract
RC6 has been submitted as a candidate for the Advanced Encryption Standard (AES). Two important features of RC6 that were absent from its predecessor RC5 are a quadratic function and a fixed rotation. By examining simplified variants that omit these features we clarify their essential contribution to the overall security of RC6."
1999,Pseudonym Systems.,"Abstract
Pseudonym systems allow users to interact with multiple organizations anonymously, using pseudonyms. The pseudonyms cannot be linked, but are formed in such a way that a user can prove to one organization a statement about his relationship with another. Such a statement is called a credential. Previous work in this area did not protect the system against dishonest users who collectively use their pseudonyms and credentials, i.e., share an identity. Previous practical schemes also relied very heavily on the involvement of a trusted center. In the present paper we give a formal definition of pseudonym systems where users are motivated not to share their identity, and in which the trusted center‚Äôs involvement is minimal. We give theoretical constructions for such systems based on any one-way function. We also suggest an efficient and easy-to-implement practical scheme."
1998,Self-Delegation with Controlled Propagation - or - What If You Lose Your Laptop.,"Abstract
We introduce delegation schemes wherein a user may delegate certain rights to himself, but may not safely delegate these rights to others. In our motivating application, a user has a primary (long-term) key that receives some personalized access rights, yet the user may reasonably wish to delegate these rights to new secondary (short-term) keys he creates to use on his laptop when traveling, to avoid having to store his primary secret key on the vulnerable laptop. We propose several cryptographic schemes, both generic ones under general assumptions and more specific practical ones, that fulfill these somewhat conflicting requirements, without relying on special-purpose (e.g., tamper-proof) hardware.
This is an extended abstract of our work [19]."
1998,Can We Eliminate Certificate Revocations Lists?,"Abstract
We briefly consider certificate revocation lists (CRLs), and ask whether they could, and should, be eliminated, in favor of other mechanisms. In most cases, the answer seems to be ‚Äúyes.‚Äù We suggest some possible replacement mechanisms."
1998,On the Design and Security of RC2.,"Abstract
The block cipher RC2 was designed in 1989 by Ron Rivest for RSA Data Security Inc. In this paper we describe both the cipher and preliminary attempts to use both differential and linear cryptanalysis."
1997,"The risks of key recovery, key escrow, and trusted third-party encryption.","A variety of ""key recovery,"" ""key escrow,"" and ""trusted third-party"" encryption requirements have been suggested in recent years by government agencies seeking to conduct covert surveillance within the changing environments brought about by new technologies. This report examines the fundamental properties of these requirements and attempts to outline the technical risks, costs, and implications of deploying systems that provide government access to encryption keys.
"
1997,Perspectives on Financial Cryptography.,"Abstract
I present some debatable propositions about financial systems and financial cryptography. (Warning: the propositions expressed may or may not be believed by the author, and may be phrased in a deliberately provocative manner. They may contradict each other. This paper follows the author's slides closely, and does not have all of the ancillary comments of the author and the audience.)"
1997,Electronic Lottery Tickets as Micropayments.,"Abstract
We present a new micropayment scheme based on the use of ‚Äúelectronic lottery tickets.‚Äù This scheme is exceptionally efficient since the bank handles only winning tickets, instead of handling each micropayment."
1997,All-or-Nothing Encryption and the Package Transform.,"Abstract
We present a new mode of encryption for block ciphers, which we call all-or-nothing encryption. This mode has the interesting defining property that one must decrypt the entire ciphertext before one can determine even one message block. This means that brute-force searches against all-or-nothing encryption are slowed down by a factor equal to the number of blocks in the ciphertext. We give a specific way of implementing all-or-nothing encryption using a ‚Äúpackage transform‚âì as a pre-processing step to an ordinary encryption mode. A package transform followed by ordinary codebook encryption also has the interesting property that it is very efficiently implemented in parallel. All-or-nothing encryption can also provide protection against chosen-plaintext and related-message attacks."
1996,On breaking a Huffman code.,"Abstract:
We examine the problem of deciphering a file that has been Huffman coded, but not otherwise encrypted. We find that a Huffman code can be surprisingly difficult to cryptanalyze. We present a detailed analysis of the situation for a three-symbol source alphabet and present some results for general finite alphabets."
1996,PayWord and MicroMint: Two Simple Micropayment Schemes.,n/a
1995,"Complete Variable-Length ""Fix-Free"" Codes.","A set of codewords is fix-free if it is both prefix-free and suffix-free: no codeword in the set is a prefix or a suffix of any other. A set of codewords {x 1,x 2,...,x n } over at-letter alphabet ? is said to becomplete if it satisfies the Kraft inequality with equality, so that

?1?i?nt?|xi|=1.
The set ?k of all codewords of lengthk is obviously both fix-free and complete. We show, surprisingly, that there are other examples of complete fix-free codes, ones whose codewords have a variety of lengths. We discuss such variable-length (complete) fix-free codes and techniques for constructing them."
1995,Piecemeal Learning of an Unknown Environment.,"Abstract
We introduce a new learning problem: learning a graph bypiecemeal search, in which the learner must return every so often to its starting point (for refueling, say). We present two linear-time piecemeal-search algorithms for learningcity-block graphs: grid graphs with rectangular obstacles."
1995,Picking the Best Expert from a Sequence.,"Abstract
We examine the problem of finding a good expert from a sequence of experts. Each expert has an ‚Äúerror rate‚Äù; we wish to find an expert with a low error rate. However, each expert‚Äôs error rate is unknown and can only be estimated by a sequence of experimental trials. Moreover, the distribution of error rates is also unknown. Given a bound on the total number of trials, there is thus a tradeoff between the number of experts examined and the accuracy of estimating their error rates.
We present a new expert-finding algorithm and prove an upper bound on the expected error rate of the expert found. A second approach, based on the sequential ratio test, gives another expert-finding algorithm that is not provably better but which performs better in our empirical studies."
1995,Being Taught can be Faster than Asking Questions.,"We explore the power of teaching by studying two online learning models: teacher-directed learning and self-directed learning. In both models, the learner tries to identify an unknown concept based on examples of the concept presented one at a time. The learner predicts whether each example is positive or negative with immediate feedback, and the objective is to minimize the number of prediction mistakes. The examples are selected by the teacher in teacher-directed learning and the learner itself in self-directed learning. Roughly, teacher-directed learning represents the scenario in which a teacher teaches a class of learners, and self-directed learning represents the scenario in which a smart learner asks questions and learns by itself. For all previously studied concept classes, the minimum number of mistakes in teacher-directed learning is always larger than that in self-directed learning. This raises an interesting question of whether teaching is helpful for all learners including the smart learner. Assuming the existence of one-way functions, we construct concept classes for which the minimum number of mistakes is linear in teacher-directed learning but superpolynomial in self-directed learning, demonstrating the power of a helpful teacher in a learning process."
1995,Piecemeal Graph Exploration by a Mobile Robot (Extended Abstract).,"We introduce a new learning problem: learning a graph bypiecemeal search, in which the learner must return every so often to its starting point (for refueling, say). We present two linear-time piecemeal-search algorithms for learningcity-block graphs: grid graphs with rectangular obstacles.
"
1994,A Formal Model of Hierarchical Concept Learning.,"Abstract
We show how to learn from examples (Valiant style) any concept representable as a boolean function or circuit, with the help of a teacher who breaks the concept into subconcepts and teaches one subconcept per lesson. Each subconcept corresponds to a gate in the boolean circuit. The learner learns each subconcept from examples which have been randomly drawn according to an arbitrary probability distribution, and labeled as positive or negative instances of the subconcept by the teacher. The learning procedure runs in time polynomial in the size of the circuit. The learner outputs not the unknown boolean circuit, but rather a program that, for any input, either produces the same answer as the unknown boolean circuit, or else says ""I don‚Ä≤t know."" Thus the output of this learning procedure is reliable. Furthermore, with high probability the output program is nearly always useful in that it says ""I don‚Ä≤t know"" very rarely. A key technique is to maintain a hierarchy of explicit ""version spaces."" Our main contribution is thus a learning procedure whose output is reliable and nearly always useful; this has not been previously accomplished within Valiant‚Ä≤s model of learnability."
1994,Diversity-Based Inference of Finite Automata.,"We present new procedures for inferring the structure of a finite-state automaton (FSA) from its input/output behavior, using access to the automaton to perform experiments. Our procedures use a new representation for finite automata, based on the notion of equivalence between tests. We call the number of such equivalence classes the diversity of the automaton; the diversity may be as small as the logarithm of the number of states of the automaton. For the special class of permutation automata, we describe an inference procedure that runs in time polynomial in the diversity and log(1/&dgr;), where &dgr; is a given upper bound on the probability that our procedure returns an incorrect result. (Since our procedure uses randomization to perform experiments, there is a certain controllable chance that it will return an erroneous result.) We also discuss techniques for handling more general automata. We present evidence for the practical efficiency of our approach. For example, our procedure is able to infer the structure of an automaton based on Rubik's Cube (which has approximately 1019 states) in about 2 minutes on a DEC MicroVax. This automaton is many orders of magnitude larger than possible with previous techniques, which would require time proportional at least to the number of global states. (Note that in this example, only a small fraction (10-14) of the global states were even visited.) Finally, we present a new procedure for inferring automata of a special type in which the global state is composed of a vector of binary local state variables, all of which are observable (or visible) to the experimenter. Our inference procedure runs provably in time polynomial in the size of this vector (which happens to be the diversity of the automaton), even though the global state space may be exponentially larger. The procedure plans and executes experiments on the unknown automaton; we show that the number of input symbols given to the automaton during this process is (to within a constant factor) the best possible."
1994,The RC5 Encryption Algorithm.,"Abstract
This document describes the RC5 encryption algorithm, a fast symmetric block cipher suitable for hardware or software implementations. A novel feature of RC5 is the heavy use of data-dependent rotations. RC5 has a variable word size, a variable number of rounds, and a variable-length secret key. The encryption and decryption algorithms are exceptionally simple."
1993,To Tap or not to Tap.,n/a
1993,Inference of Finite Automata Using Homing Sequences.,"Abstract
We present new algorithms for inferring an unknown finite-state automaton from its input/output behavior, even in the absence of a means of resetting the machine to a start state. A key technique used is inference of a homing sequence for the unknown automaton. Our inference procedures experiment with the unknown machine, and from time to time require a teacher to supply counterexamples to incorrect conjectures about the structure of the unknown automaton. In this setting, we describe a learning algorithm that, with probability 1 ‚àí Œ¥, outputs a correct description of the unknown machine in time polynomial in the automaton‚Ä≤s size, the length of the longest counterexample, and log(1/Œ¥). We present an analogous algorithm that makes use of a diversity-based representation of the finite-state system. Our algorithms are the first which are provably effective for these problems, in the absence of a ""reset."" We also present probabilistic algorithms for permutation automata which do not require a teacher to supply counterexamples. For inferring a permutation automaton of diversity D, we improve the best previous time bound by roughly a factor of D3/log D."
1993,On Choosing between Experimenting and Thinking when Learning.,"Abstract
We introduce a model of inductive inference, or learning, that extends the conventional Bayesian approach by explicitly considering the computational cost of formulating predictions to be tested. We view the learner as a scientist who must divide her time between doing experiments and deducing predictions from promising theories, and we wish to know how she can do so most effectively. We explore several approaches based on the cost of making a prediction relative to the cost of performing an experiment. The resulting strategies share many qualitative characteristics with ""real"" science. This model is significant for the following reasons:
‚Ä¢
It allows us to study how a scientist might go about acquiring knowledge in a world where (as in real life) both performing experiments and making predictions from theories require time and effort.
‚Ä¢
It lays the foundation for a rigorous machine-implementable notion of ""subjective probability."" Good (1959, , 443-447) argues persuasively that subjective probability is at the heart of probability theory. Previous treatments of subjective probability do not handle the complication that the learner‚Ä≤s subjective probabilities may change as the result of pure thinking; our model captures this and other effects in a realistic manner. In addition, we begin to answer the question of how to trade off versus -a question that is fundamental for computers that must exist in the world and learn from their experience."
1993,Learning Binary Relations and Total Orders.,"he problem of learning a binary relation between two sets of objects or between a set and itself is studied. This paper represents a binary relation between a set of size n and a set of size m as an $n \times m$ matrix of bits whose $(i,j)$ entry is 1 if and only if the relation holds between the corresponding elements of the two sets. Polynomial prediction algorithms are presented for learning binary relations in an extended on-line learning model, where the examples are drawn by the learner, by a helpful teacher, by an adversary, or according to a uniform probability distribution on the instance space.

The first part of this paper presents results for the case in which the matrix of the relation has at most k row types. It presents upper and lower bounds on the number of prediction mistakes any prediction algorithm makes when learning such a matrix under the extended on-line learning model. Furthermore, it describes a technique that simplifies the proof of expected mistake bounds against a randomly chosen query sequence.

In the second part of this paper the problem of learning a binary relation that is a total order on a set is considered. A general technique using a fully polynomial randomized approximation scheme (fpras) to implement a randomized version of the halving algorithm is described. This technique is applied to the problem of learning a total order, through the use of an fpras for counting the number of extensions of a partial order, to obtain a polynomial prediction algorithm that with high probability makes at most $n\lg n + (\lg e)\lg n$ mistakes when an adversary selects the query sequence. The case in which a teacher or the learner selects the query sequence is also considered

"
1993,Piecemeal Learning of an Unknown Environment.,"We introduce a new learning problem: learning a graph by piecemeal search, in which the learner must return every so often to its starting point (for refueling, say). We present two linear-time piecemeal-search algorithms for learning city-block graphs: grid graphs with rectangular obstacles. "
1993,Strategic Directions in Machine Learning.,n/a
1993,Introduction.,n/a
1993,Training a 3-Node Neural Network is NP-Complete.,"Abstract
We show for many simple two-layer networks whose nodes compute linear threshold functions of their inputs that training is NP-complete. For any training algorithm for one of these networks there will be some sets of training data on which it performs poorly, either by running for more than an amount of time polynomial in the input length, or by producing sub-optimal weights. Thus, these networks differ fundamentally from the perceptron in a worst-case computational sense.
The theorems and proofs are in a sense fragile; they do not imply that training is necessarily hard for networks other than those specifically mentioned. They do, however, suggest that one cannot escape computational difficulties simply by considering only very simple or very regular networks.
On a somewhat more positive note, we present two networks such that the second is both more powerful than the first and can be trained in polynomial time, even though the first is NP-complete to train. This shows that computational intractability does not depend directly on network power and provides theoretical support for the idea that finding an appropriate network and input encoding for one's training problem is an important part of the training process.
An open problem is whether the NP-completeness results can be extended to neural networks that use the differentiable logistic linear functions. We conjecture that training remains NP-complete when these functions are used since their use does not seem to alter significantly the expressive power of a neural network. However, our proof techniques break down. Note that Judd [9], for the networks he considers, shows NP-completeness for a wide variety of node functions including logistic linear functions."
1993,Inference of Finite Automata Using Homing Sequences.,n/a
1993,Scapegoat Trees.,"We present an algorithm for maintaining binary search trees. The amortized complexity per INSERT or DELETE is O(log n) while the worst-case cost of a SEARCH is O(log n). Scapegoat trees, unlike most balanced-tree schemes, do not require keeping extra data (e.g. ìcolorsî or ìweightsî) in the tree nodes. Each node in the tree contains only a key value and pointers. to its two children. Associated with the root of the whole tree are the only two extra values needed by the scapegoat scheme: the number of nodes in the whole tree, and the maximum number of nodes in the tree since the tree was last completely rebuilt. In a scapegoat tree a typical rebalancing operation begins at a leaf, and successively examines higher ancestors until a node (the ìscapegoatî) is found that is so unbalanced that the entire subtree rooted at the scapegoat can be rebuilt at zero cost, in an amortized sense. Hence the name."
1992,Training a 3-node neural network is NP-complete.,"Abstract
We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for this network so that it produces output consistent with a given set of training examples. We extend the result to other simple networks. We also present a network for which training is hard but where switching to a more powerful representation makes training easier. These results suggest that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. They also suggest the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with nonlinear functions such as sigmoids."
1991,Results on Learnability and the Vapnik-Chervonenkis Dimension.,"Abstract
We consider the problem of learning a concept from examples in the distribution-free model by Valiant. (An essentially equivalent model, if one ignores issues of computational difficulty, was studied by Vapnik and Chervonenkis.) We introduce the notion of dynamic sampling, wherein the number of examples examined may increase with the complexity of the target concept. This method is used to establish the learnability of various concept classes with an infinite Vapnik-Chervonenkis dimension. We also discuss an important variation on the problem of learning from examples, called approximating from examples. Here we do not assume that the target concept T is a member of the concept class
from which approximations are chosen. This problem takes on particular interest when the VC dimension of
is infinite. Finally, we discuss the problem of computing the VC dimension of a finite concept set defined on a finite domain and consider the structure of classes of a fixed small dimension."
1991,Cryptography and Machine Learning.,"Abstract
This paper gives a survey of the relationship between the fields of cryptography and machine learning, with an emphasis on how each field has contributed ideas and techniques to the other. Some suggested directions for future cross-fertilization are also proposed."
1991,On NIST's Proposed Digital Signature Standard.,"Abstract
The proposal by NIST/NSA of a digital signature standard has some positive aspects, but is marred by serious flaws. Most notably, it is not sufficiently secure for a standard and seems vulnerable to ‚Äútrapdoors.‚Äù"
1991,Incrementally Learning Time-Varying Half Planes.,"We present a distribution-free model for incremental learning when concepts vary with time. Concepts are caused to change by an adversary while an incremental learning algorithm attempts to track the changing concepts by minimizing the error between the current target concept and the hypothesis. For a single half-plane and the intersection of two half-planes, we show that the average mistake rate depends on the maximum rate at which an adversary can modify the concept. These theoretical predictions are verified with simulations of several learning algorithms including back propagation."
1990,A fair protocol for signing contracts.,"Abstract:
Two parties, A and B, want to sign a contract C over a communication network. To do so, they must simultaneously exchange their commitments to C. Since simultaneous exchange is usually impossible in practice, protocols are needed to approximate simultaneity by exchanging partial commitments in piece-by-piece manner. During such a protocol, one party or another may have a slight advantage; a fair protocol keeps this advantage within acceptable limits. A new protocol is proposed. It is fair in the sense that, at any stage in its execution, the conditional probability that one party cannot commit both parties to the contract given that the other party can, is close to zero. This is true even if A and B have vastly different computing powers and is proved under very weak cryptographic assumptions.< >"
1990,On the Sample Complexity of PAC-Learning Using Random and Chosen Examples.,"Membership queries have been used to efficiently and exactly learn a concept class C that is too difficult to pac-learn using random examples. We ask whether using membership queries--in conjunction with or instead of random examples--can serve a new purpose: helping to reduce the total number of examples needed to pac-learn a concept class C already known to be pac-learnable using just random examples. We focus on concept classes that are dense in themselves, such as half-spaces of * , rectangles in the plane, and the class I = { [ 0, a ] : 0 # a > 1 } of initial segments of [0, 1]."
1990,Inferring Graphs from Walks.,"We consider the problem of inferring an undirected, degree-bounded, edge-colored graph from the sequence of edge colors seen in a walk of that graph. This problem can be viewed as reconstructing the structure of a Markov chain from its output. (That is, we are not concerned with inferring the transition probabilities, but only the underlying graph structure of the Markov chain.) We present polynomial-time algorithms for the inference of underlying graphs of degree-bound 2 (linear chains and cycles), based on some surprising properties about the confluence of various sets of rewrite rules."
1990,The MD4 Message Digest Algorithm.,"Abstract
The MD4 message digest algorithm takes an input message of arbitrary length and produces an output 128-bit ‚Äúfingerprint‚Äù or ‚Äúmessage digest‚Äù, in such a way that it is (hopefully) computationally infeasible to produce two messages having the same message digest, or to produce any message having a given prespecified target message digest. The MD4 algorithm is thus ideal for digital signature applications: a large file can be securely ‚Äúcompressed‚Äù with MD4 before being signed with (say) the RSA public-key cryptosystem.
The MD4 algorithm is designed to be quite fast on 32-bit machines. For example, on a SUN Sparc station, MD4 runs at 1,450,000 bytes/second (11.6 Mbit/sec). In addition, the MD4 algorithm does not require any large substitution tables; the algorithm can be coded quite compactly.
The MD4 algorithm is being placed in the public domain for review and possible adoption as a standard."
1990,Finding Four Million Large Random Primes.,"Abstract
A number n is a (base two) pseudoprime if it is composite and satisfies the identity
2
n‚àí1
‚â°1(modn).
2n‚àí1‚â°1(modn).
(1)
Every prime satisifies (1), but very few composite numbers are pseudoprimes. If pseu- doprimes are very raTe, then one could even find large ‚Äúindustrial strength‚Äù primes (say for cryptographic use) by simply choosing large random values for n until an n is found that satisfies (1). H ow rare are pseudoprimes? We performed an experiment that attempts to provide an answer. We also provide some references to the literature for theoretical analyses."
1990,Learning Time-Varying Concepts.,"This work extends computational learning theory to situations in which concepts vary over time, e.g., system identification of a time-varying plant. We have extended formal definitions of concepts and learning to provide a framework in which an algorithm can track a concept as it evolves over time. Given this framework and focusing on memory-based algorithms, we have derived some PAC-style sample complexity results that determine, for example, when tracking is feasible. We have also used a similar framework and focused on incremental tracking algorithms for which we have derived some bounds on the mistake or error rates for some specific concept classes."
1989,Inferring Decision Trees Using the Minimum Description Length Principle.,"Abstract
We explore the use of Rissanen's minimum description length principle for the construction of decision trees. Empirical results comparing this approach to other methods are given."
1989,Learning Binary Relations and Total Orders (Extended Abstract).,"Abstract:
The problem of designing polynomial prediction algorithms for learning binary relations is studied for an online model in which the instances are drawn by the learner, by a helpful teacher, by an adversary, or according to a probability distribution on the instance space. The relation is represented as an n*m binary matrix, and results are presented when the matrix is restricted to have at most k distinct row types, and when it is constrained by requiring that the predicate form a total order.< >"
1989,Inference of Finite Automata Using Homing Sequences (Extended Abstract).,"We present new algorithms for inferring an unknown finite-state automaton from its input/output behavior in the absence of a means of resetting the machine to a start state. A key technique used is inference of a homing sequence for the unknown automaton. Our inference procedures experiment with the unknown machine, and from time to time require a teacher to supply counterexamples to incorrect conjectures about the structure of the unknown automaton. In this setting, we describe a learning algorithm which, with probability 1-&dgr;, outputs a correct description of the unknown machine in time polynomial in the automaton's size, the length of the longest counterexample, and log (1/&dgr;). We present an analogous algorithm which makes use of a diversity-based representation of the finite-state system. Our algorithms are the first which are provably effective for these problems, in the absence of a ‚Äúreset.‚Äù We also present probabilistic algorithms for permutation automata which do not require a teacher to supply counterexamples. For inferring a permutation automaton of diversity D, we improve the best previous time bound by roughly a factor of D3/logD."
1988,Is the Data Encryption Standard a Group? (Results of Cycling Experiments on DES).,"Abstract
The Data Encryption Standard (DES) defines an indexed set of permutations acting on the message space ‚Ñ≥ ={0,1}64. If this set of permutations were closed under functional composition, then the two most popular proposals for strengthening DES through multiple encryption would be equivalent to single encryption. Moreover, DES would be vulnerable to a known-plaintext attack that runs in 228 steps on the average. It is unknown in the open literature whether or not DES has this weakness.
Two statistical tests are presented for determining if an indexed set of permutations acting on a finite message space forms a group under functional composition. The first test is a ‚Äúmeet-in-the-middle‚Äù algorithm which uses O(‚àöK) time and space, where K is the size of the key space. The second test, a novel cycling algorithm, uses the same amount of time but only a small constant amount of space. Each test yields a known-plaintext attack against any finite, deterministic cryptosystem that generates a small group.
The cycling closure test takes a pseudorandom walk in the message space until a cycle is detected. For each step of the pseudorandom walk, the previous ciphertext is encrypted under a key chosen by a pseudorandom function of the previous ciphertext. Results of the test are asymmetrical: long cycles are overwhelming evidence that the set of permutations is not a group; short cycles are strong evidence that the set of permutations has a structure different from that expected from a set of randomly chosen permutations.
Using a combination of software and special-purpose hardware, the cycling closure test was applied to DES. Experiments show, with overwhelming confidence, that DES is not a group. Additional tests confirm that DES is free of certain other gross algebraic weaknesses. But one experiment discovered fixed points of the so-called ‚Äúweak-key‚Äù transformations, thereby revealing a previously unpublished additional weakness of the weak keys."
1988,A Digital Signature Scheme Secure Against Adaptive Chosen-Message Attacks.,"We present a digital signature scheme based on the computational difficulty of integer factorization.The scheme possesses the novel property of being robust against an adaptive chosen-message attack: an adversary who receives signatures for messages of his choice (where each message may be chosen in a way that depends on the signatures of previously chosen messages) cannot later forge the signature of even a single additional message. This may be somewhat surprising, since in the folklore the properties of having forgery being equivalent to factoring and being invulnerable to an adaptive chosen-message attack were considered to be contradictory.More generally, we show how to construct a signature scheme with such properties based on the existence of a ìclaw-freeî pair of permutationsóa potentially weaker assumption than the intractibility of integer factorization.The new scheme is potentially practical: signing and verifying signatures are reasonably fast, and signatures are compact."
1988,A knapsack-type public key cryptosystem based on arithmetic in finite fields.,"Abstract:
A knapsack-type public key cryptosystem is introduced that is the system is based on a novel application of arithmetic in finite fields. By appropriately choosing the parameters, one can control the density of the resulting knapsack, which is the ratio between the number of elements in the knapsack and their size in bits. In particular, the density can be made high enough to foil so-called low-density attacks against the system. At the moment, no attacks capable of breaking the system in a reasonable amount of time are known.< >"
1988,Learning Complicated Concepts Reliably and Usefully.,"We show how to learn from examples (Valiant style) any concept representable as a boolean function or circuit, with the help of a teacher who breaks the concept into subconcepts and teaches one subconcept per lesson. Each subconcept corresponds to a gate in the boolean circuit. The learner learns each subconcept from examples which have been randomly drawn according to an arbitrary probability distribution, and labeled as positive or negative instances of the subconcept by the teacher. The learning procedure runs in time polynomial in the size of the circuit. The learner outputs not the unknown boolean circuit, but rather a program which, for any input, either produces the same answer as the unknown boolean circuit, or else says ""I don‚Äôt know."" Thus the output of this learning procedure is reliable. Furthermore, with high probability the output program is nearly always useful in that it says ""I don‚Äôt know"" very rarely. A key technique is to maintain a hierarchy of explicit ""version spaces."" Our main contribution is thus a learning procedure whose output is reliable and nearly always useful; this has not been previously accomplished within Valiant‚Äôs model of learnability."
1988,Training a 3-Node Neural Network is NP-Complete.,"We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for the three nodes of this network so that it will produce output consistent with a given set of training examples. We extend the result to other simple networks. This result suggests that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. It also suggests the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with non-linear functions such as sigmoids."
1988,Results on Learnability and the Vapnick-Chervonenkis Dimension.,"We consider the problem of learning a concept from examples in the distribution-free model by Valiant. (An essentially equivalent model, if one ignores issues of computational difficulty, was studied by Vapnik and Chervonenkis.) We introduce the notion of dynamic sampling, wherein the number of examples examined may increase with the complexity of the target concept. This method is used to establish the learnability of various concept classes with an infinite Vapnik-Chervonenkis dimension. We also discuss an important variation on the problem of learning from examples, called approximating from examples. Here we do not assume that the target concept T is a member of the concept class C from which approximations are chosen. This problem takes on particular interest when the VC dimension of C is infinite. Finally, we discuss the problem of computing the VC dimension of a finite concept set defined on a finite domain and consider the structure of classes of a fixed small dimension."
1988,Learning Complicated Concepts Reliably and Usefully.,"We show how to learn from examples (Valiant style) any concept representable as a boolean function or circuit, with the help of a teacher who breaks the concept into sub concepts and teaches one subconcept per lesson. Each subconcept corresponds to a gate in the boolean circuit. The learner learns each subconcept from examples which have been randomly drawn according to an arbitrary probability distribution, and labeled as positive or negative instances of the sub-concept by the teacher. The learning procedure runs in time polynomial in the size of the circuit. The learner outputs not the unknown boolean circuit, but rather a program which, for any input, either produces the same answer as the unknown boolean circuit, or else says ""I don't know."" Thus the output of this learning procedure is reliable. Furthermore, with high probability the output program is nearly always useful in that it says ""I don't know"" very rarely. A key technique is to maintain a hierarchy of explicit ""version spaces."" Our main contribution is thus a learning procedure whose output is reliable and nearly always useful; this has not been previously accomplished within Valiant's model of learn ability."
1988,Results on learnability and the Vapnik-Chervonenkis dimension (Extended Abstract).,"Abstract:
The problem of learning a concept from examples in a distribution-free model is considered. The notion of dynamic sampling, wherein the number of examples examined can increase with the complexity of the target concept, is introduced. This method is used to establish the learnability of various concept classes with an infinite Vapnik-Chervonenkis (VC) dimension. An important variation on the problem of learning from examples, called approximating from examples, is also discussed. The problem of computing the VC dimension of a finite concept set defined on a finite domain is considered.< >"
1988,Training a 3-Node Neural Network is NP-Complete.,"We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for the three nodes of this network so that it will produce output consistent with a given set of training examples. We extend the result to other simple networks. This result suggests that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. It also suggests the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with non-linear functions such as sigmoids."
1988,A New Model for Inductive Inference.,"We introduce a new model for inductive inference, by combining a Bayesian approach for representing the current state of knowledge with a simple model for the computational cost of making predictions from theories. We investigate the optimization problem: how should a scientist divide his time between doing experiments and deducing predictions for promising theories. We propose an answer to this question, as a function of the relative costs of making predictions versus performing experiments. We believe our model captures many of the qualitative characteristics of ""real"" science.

We believe that this model makes two important contributions. First, it allows us to study how a scientist might go about acquiring knowledge in a world where (as in real life) there are costs associated with both performing experiments and with computing the predictions of various theories.

This model also lays the groundwork for a rigorous treatment of a machine-implementable notion of ""subjective probability"". Subjective probability is at the heart of probability theory [5]. Previous treatments have not been able to handle the difficulty that subjective probabilities can change as the result of ""pure thinking""; our model captures this (and other effects) in a realistic manner. In addition, we begin to provide an answer to the question of how to trade-off ""thinking"" versus ""doing""---a question that is fundamental for computers that must exist in the world and learn from their experience. "
1987,Game Tree Searching by Min/Max Approximation.,"Abstract
We present an iterative method for searching min/max game trees based on the idea of approximating the ‚Äúmin‚Äù and ‚Äúmax‚Äù operators by generalized mean-valued operators. This approximation is used to guide the selection of the next leaf node to expand, since the approximations allow one to select efficiently that leaf node upon whose value the (approximate) value at the root most highly depends. Experimental results from almost 1,000 games of Connect-Four1 suggest that our scheme is superior to minimax search with alpha-beta pruning, for the same number of calls to the move routine. However, our scheme has higher overhead, so that further work is needed before it becomes competitive when CPU time per turn is the limiting resource."
1987,Global Wire Routing in Two-Dimensional Arrays.,"We examine the problem of routing wires on a VLSI chip, where the pins to be connected are arranged in a regular rectangular array. We obtain tight bounds for the worst-case ""channel-width"" needed to route an n ◊ n array, and develop provably good heuristics for the general case. An interesting ""rounding algorithm"" for obtaining integral approximations to solutions of linear equations is used to show the near-optimality of single-turn routings in the worst-case. "
1987,Learning Decision Lists.,"Abstract
This paper introduces a new representation for Boolean functions, called decision lists, and shows that they are efficiently learnable from examples. More precisely, this result is established for k-DL the set of decision lists with conjunctive clauses of size k at each decision. Since k-DL properly includes other well-known techniques for representing Boolean functions such as k-CNF (formulae in conjunctive normal form with at most k literals per clause), k-DNF (formulae in disjunctive normal form with at most k literals per term), and decision trees of depth k, our result strictly increases the set of functions that are known to be polynomially learnable, in the sense of Valiant (1984). Our proof is constructive: we present an algorithm that can efficiently construct an element of k-DL consistent with a given set of examples, if one exists."
1987,Network control by Bayesian broadcast.,"Abstract:
A transmission control strategy is described for slotted-ALOHA-type broadcast channels with ternary feedback. At each time slot, each station estimates the probability that n stations are ready to transmit a packet for each n , using Bayes' rule and the observed history of collisions, successful transmissions, and holes (empty slots). A station transmits a packet in a probabilistic manner based on these estimates. This strategy is called Bayesian broadcast. An elegant and very practical strategy--pseudo-Bayesian broadcast--is then derived by approximating the probability estimates with a Poisson distribution with mean \nu and further simplifying. Each station keeps a copy of \nu , transmits a packet with probability 1/\nu , and then updates \nu in two steps: For collisions, increment \nu by (e-2)^{-l}=1.39221 \cdots . For successes and holes, decrement \nu by 1 . Set \nu to \max (\nu + \hat{\lambda}, 1) , where \hat{\lambda} is an estimate of the arrival rate \lambda of new packets into the system. Simulation results are presented showing that pseudo-Bayesian broadcast performs well in practice, and methods that can be used to prove that certain versions of pseudo-Bayesian broadcast are stable for \lambda < e^{-1} are discussed."
1987,Diversity-Based Inference of Finite Automata (Extended Abstract).,"Abstract:
We present a new procedure for inferring the structure of a finitestate automaton (FSA) from its input/output behavior, using access to the automaton to perform experiments. Our procedure uses a new representation for FSA's, based on the notion of equivalence between testa. We call the number of such equivalence classes the diversity of the automaton; the diversity may be as small as the logarithm of the number of states of the automaton. The size of our representation of the FSA, and the running time of our procedure (in some case provably, in others conjecturally) is polynomial in the diversity and ln(1/Œµ), where Œµ is a given upper bound on the probability that our procedure returns an incorrect result. (Since our procedure uses randomization to perform experiments, there is a certain controllable chance that it will return an erroneous result.) We also present some evidence for the practical efficiency of our approach. For example, our procedure is able to infer the structure of an automaton based on Rubik's Cube (which has approximately 1019 states) in about 2 minutes on a DEC Micro Vax. This automaton is many orders of magnitude larger than possible with previous techniques, which would require time proportional at least to the number of global states. (Note that in this example, only a small fraction (10-14) of the global states were even visited.)"
1986,An application of number theory to the organization of raster-graphics memory.,"A high-resolution raster-graphics display is usually combined with processing power and a memory organization that facilitates basic graphics operations. For many applications, including interactive text processing, the ability to quickly move or copy small rectangles of pixels is essential. This paper proposes a novel organization of raster-graphics memory that permits all small rectangles to be moved efficiently. The memory organization is based on a doubly periodic assignment of pixels to M memory chips according to a ‚ÄúFibonacci‚Äù lattice. The memory organization guarantees that, if a rectilinearly oriented rectangle contains fewer than M/ @@@@5 pixels, then all pixels will reside in different memory chips and thus can be accessed simultaneously. Moreover, any M consecutive pixels, arranged either horizontally or vertically, can be accessed simultaneously. We also define a continuous analog of the problem, which can be posed as: ‚ÄúWhat is the maximum density of a set of points in the plane such that no two points are contained in the interior of a rectilinearly oriented rectangle of unit area?‚Äù We show the existence of such a set with density 1/ @@@@5, and prove this is optimal by giving a matching upper bound."
1986,Estimating a probability using finite memory.,"Abstract:
Let \{X_{i}\}_{i=1}^{\infty} be a sequence of independent Bernoulli random variables with probability p that X_{i} = 1 and probability q=1-p that X_{i} = 0 for all i \geq 1 . Time-invariant finite-memory (i.e., finite-state) estimation procedures for the parameter p are considered which take X_{1}, \cdots as an input sequence. In particular, an n-state deterministic estimation procedure is described which can estimate p with mean-square error O(\log n/n) and an n -state probabilistic estimation procedure which can estimate p with mean-square error O(1/n) . It is proved that the O(1/n) bound is optimal to within a constant factor. In addition, it is shown that linear estimation procedures are just as powerful (up to the measure of mean-square error) as arbitrary estimation procedures. The proofs are based on an analog of the well-known matrix tree theorem that is called the Markov chain tree theorem."
1986,A non-iterative maximum entropy algorithm.,"We present a new algorithm for computing the maximum entropy probability distribution satisfying a set of constraints. Unlike previous approaches, our method is integrated with the planning of data collection and tabulation. We show how adding constraints and performing the associated additional tabulations can substantially speed up computation by replacing the usual iterative techniques with a non-iterative computation. We note, however, that the constraints added may contain significantly more variables than any of the original constraints so there may not be enough data to collect meaningful statistics. These extra constraints are shown to correspond to the intermediate tables in Cheeseman's method. Furthermore, we prove that acyclic hypergraphs and decomposable models are equivalent, and discuss the similarities and differences between our algorithm and Spiegelhalter's algorithm. Finally, we compare our work to Kim and Pearl's work on singly-connected networks."
1985,Is DES a Pure Cipher? (Results of More Cycling Experiments on DES).,"Abstract
During summer 1985, we performed eight cycling experiments on the Data Encryption Standard (DES) to see if DES has certain algebraic weaknesses. Using special-purpose hardware, we applied the cycling closure test described in our Eurocrypt 85 paper to determine whether DES is a pure cipher. We also carried out a stronger version of this test. (A cipher is pure if, for any keys i, j, k, there exists some key l such that T i T j ‚àí1 T k = T l, where T w denotes encryption under key w.) In addition, we followed the orbit of a randomly chosen DES transformation for 236 steps, as well as the orbit of the composition of two of the ‚Äúweak key‚Äù transformations. Except for the weak key experiment, our results are consistent with the hypothesis that DES acts like a set of randomly chosen permutations. In particular, our results show with overwhelming confidence that DES is not pure. The weak key experiment produced a short cycle of about 233 steps, the consequence of hitting a fixed point for each weak key."
1985,Efficient Factoring Based on Partial Information.,"Abstract
Many recently proposed cryptosystems are based on the assumption that factoring large composite integers is computationally difficult. In this paper we examine this assumption when the cryptanalyst has ‚Äúside information‚Äù available."
1985,Is the Data Encryption Standard a Group? (Preliminary Abstract).,"Abstract
The Data Encryption Standard (DES) defines an indexed set of permutations acting on the message space M = {0,1}64. If this set of permutations were closed under functional composition, then DES would be vulnerable to a known-plaintext attack that runs in 228 steps, on the average. It is unknown in the open literature whether or not DES has this weakness.
We describe two statistical tests for determining if an indexed set of permutations acting on a finite message space forms a group under functional composition. The first test is a ‚Äúmeet-in-the-middle‚Äù algorithm which uses O(‚àöK) time and space, where K is the size of the key space. The second test, a novel cycling algorithm, uses the same amount of time but only a small constant amount of space. Each test yields a known-plaintext attack against any finite, deterministic cryptosystem that generates a small group.
The cycling test takes a pseudo-random walk in the message space until a cycle is detected. For each step of the pseudo-random walk, the previous ciphertext is encrypted under a key chosen by a pseudo-random function of the previous ciphertext. Results of the test are asymmetrical: long cycles are overwhelming evidence that the set of permutations is not a group; short cycles are strong evidence that the set of permutations has a structure different from that expected from a set of randomly chosen permutations.
Using a combination of software and special-purpose hardware, we applied the cycling test to DES. Our experiments show, with a high degree of confidence, that DES is not a group."
1985,A Fair Protocol for Signing Contracts (Extended Abstract).,"Abstract
Assume that two parties, A and B, want to sign a contract over a communication network, i.e. they want to exchange their ‚Äúcommitments‚Äú to the contract. We consider a contract signing protocol to be fair if, at any stage in its execution, the following hold: the conditional probability that party A obtains B's signature to the contract given that B has obtained A's signature to the contract, is close to 1. (Symmetrically, when switching the roles of A and B).
Contract signing protocols cannot be fair without relying on a trusted third party. We present a fair, cryptographic protocol for signing contracts that makes use of the weakest possible form of a trusted third party (judge). If both A and B are honest, the judge will never be called upon. Otherwise, the judge rules by performing a simple computation, without referring to previous verdicts. Thus, no bookkeeping is required from the judge. Our protocol is fair even if A and B have very different computing powers. Its fairness is proved under the very general cryptographic assumption that functions that are one-way in a weak sense exist. Our protocol is also optimal with respect to the number of messages exchanged."
1984,How to Expose an Eavesdropper.,We present a new protocol for establishing secure communications over an insecure communications charmel in the absence of trusted third parties or authenticated keys. The protocol is an improvement over the simpler protocol in which the communicating parties exchanged their public encryption keys and used them to encrypt messages. It forces a potential eavesdropper--if he wants to understand the messages--to reveal his existence by modifying and seriously garbling the communication. 
1984,A Knapsack Type Public Key Cryptosystem Based On Arithmetic in Finite Fields.,"Abstract
We introduce a new knapsack type public key cryptosystem. The system is based on a novel application of arithmetic in finite fields, following a construction by Bose and Chowla. Appropriately choosing the parameters, we can control the density of the resulting knapsack. In particular, the density can be made high enough to foil ‚Äúlow density‚Äù attacks against our system. At the moment, we do not know of any attacks capable of ‚Äúbreaking‚Äù this system in a reasonable amount of time."
1984,"A ""Paradoxical'""Solution to the Signature Problem (Abstract).","Brief Abstract
We present a general signature scheme which uses any pair of trap-door permutations (f0, f1) for which it is infeasible to find any x, y with f0(x) = f1(y). The scheme possesses the novel property of being robust against an adaptive chosen message attack: no adversary who first asks for and then receives sgnatures for messages of his choice (which may depend on previous signatures seen) can later forge the signature of even a singl additional message.
For specific instance of our general scheme, we prove that
(1)
forging signatures is provably equivalent to factoring (i.e., factoring is polynomial-time reducible to forging signatures, and vice versa) while
 (2)
forging an additional signature, after an adaptive chosen message attack is still equivalent to factoring.
  Such scheme is ‚Äúparadoxical‚Äù since the above two properties were believed (and even ‚Äúproven‚Äù in the folklore) to be contradictory.
The new scheme is potentially practical: signing and verifying signatures are reasonably fast, and signatures are not too long."
1984,RSA Chips (Past/Present/Future).,"Abstract
We review the issues involved in building a special-purpose chip for performing RSA encryption/decryption, and review a few of the current implementation efforts."
1984,"A ""Paradoxical"" Solution to the Signature Problem (Extended Abstract).","We present a general signature scheme which uses any pair of trap-door permutations (f0, f1) for which it is infeasible to find any x, y with f0(x) = f1(y). The scheme possesses the novel property of being robust against an adaptive chosen message attack: no adversary who first asks for and then receives sgnatures for messages of his choice (which may depend on previous signatures seen) can later forge the signature of even a singl additional message. "
1983,A Method for Obtaining Digital Signatures and Public-Key Cryptosystems (Reprint).,"An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences: Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intended recipient. Only he can decipher the message, since only he knows the corresponding decryption key. A message can be ‚Äúsigned‚Äù using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed encryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in ‚Äúelectronic mail‚Äù and ‚Äúelectronic funds transfer‚Äù systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product, n, of two large secret prime numbers p and q. Decryption is similar; only a different, secret, power d is used, where e * d = 1(mod (p - 1) * (q - 1)). The security of the system rests in part on the difficulty of factoring the published divisor, n."
1983,Estimating a Probability Using Finite Memory (Extended Abstract).,"Abstract
Let {Xi i=1 ‚àû } be a sequence of independent Bernoulli random variables with probability p that X i =1 and probability q=1 ‚àí p that X i =0 for all i‚â•1. We consider time-invariant finite-memory (i.e., finite-state) estimation procedures for the parameter p which take X1, ... as an input sequence. In particular, we describe an n-state deterministic estimation procedure that can estimate p with mean-square error O(log n/n) and an n-state probabilistic estimation procedure that can estimate p with mean-square error O(1/n). We prove that the O(1/n) bound is optimal to within a constant factor. In addition, we show that linear estimation procedures are just as powerful (up to the measure of mean-square error) as arbitrary estimation procedures. The proofs are based on the Markov Chain Tree Theorem."
1983,Global Wire Routing in Two-Dimensional Arrays (Extended Abstract).,"Abstract:
We examine the problem of routing wires on a VLSI chip, where the pins to be connected are arranged in a regular rectangular array. We obtain tight bounds for the worst-case ""channel-width"" needed to route an n √ó n array, and develop provably good heuristics for the general case. An interesting ""rounding algorithm"" for obtaining integral approximations to solutions of linear equations is used to show the near-optimality of single-turn routings in the worst-case."
1982,"How to Reuse a ""Write-Once"" Memory.","Storage media such as digital optical disks, PROMS, or paper tape consist of a number of ‚Äúwrite-once‚âì bit positions (wits); each wit initially contains a ‚Äú0‚âì that may later be irreversibly overwritten with a ‚Äú1.‚âì It is demonstrated that such ‚Äúwrite-once memories‚âì (woms) can be ‚Äúrewritten‚âì to a surprising degree. For example, only 3 wits suffice to represent any 2-bit value in a way that can later be updated to represent any other 2-bit value. For large k, 1.29‚Ä¶ ¬∑ k wits suffice to represent a k-bit value in a way that can be similarly updated. Most surprising, allowing t writes of a k-bit value requires only t + o(t) wits, for any fixed k. For fixed t, approximately k ¬∑ t/log(t) wits are required as k ‚Üí ‚àû. An n-wit WOM is shown to have a ‚Äúcapacity‚âì (i.e., k ¬∑ t when writing a k-bit value t times) of up to n ¬∑ log(n) bits."
1982,Randomized Encryption Techniques.,"Abstract
A randomized encryption procedure enciphers a message by randomly choosing a ciphertext from a set of ciphertexts corresponding to the message under the current encryption key. At the cost of increasing the required bandwidth, such procedures may achieve greater cryptographic security than their deterministic counterparts by increasing the apparent size of the message space, eliminating the threat of chosen plaintext attacks, and improving the a priori statistics for the inputs to the encryption algorithms. In this paper we explore various ways of using randomization in encryption."
1982,A Short Report on the RSA Chip.,"Abstract
The nMOS ‚ÄúRSA chip‚Äù described in our article [1] was initially fabricated by Hewlett-Packard. Testing revealed that while the control portion of the chip worked correctly, the arithmetic section suffered from transient errors and was usually too unreliable to complete a full encryption. We tested a number of chips and found the same problem, enough to convonce us that the cause was probably a design error and not a fabrication problem."
1982,"A ""greedy"" channel router.","We present a new, ‚Äúgreedy‚Äù, channel-router that is quick, simple, and highly effective. It always succeeds, usually using no more than one track more than required by channel density. (It may be forced in rare cases to make a few connections ‚Äúoff the end‚Äù of the channel, in order to succeed.) It assumes that all pins and wiring lie on a common grid, and that vertical wires are on one layer, horizontal on another. The greedy router wires up the channel in a left-to-right, column-by-column manner, wiring each column completely before starting the next. Within each column the router tries to maximize the utility of the wiring produced, using simple, ‚Äúgreedy‚Äù heuristics. It may place a net on more than one track for a few columns, and ‚Äúcollapse‚Äù the net to a single track later on, using a vertical jog. It may also use a jog to move a net to a track closer to its pin in some future column. The router may occasionally add a new track to the channel, to avoid ‚Äúgetting stuck‚Äù."
1982,"The ""PI"" (placement and interconnect) system.","‚ÄúPI‚Äù is an advanced LISP-based placement and interconnect system for custom NMOS or CMOS (single-layer metal) designs. When fully implemented, PI will handle placement of arbitrarily-sized rectangular modules, routing of power and ground, signal routing, and compaction. In this paper we briefly review the structure of PI, and present details on the signal-routing heuristics, focusing on the definition of ‚Äúchannels‚Äù, the global router, the ‚Äúcrossing placer‚Äù, and the channel routers. The signal router is fully operational; the rest of PI is currently being coded and will be more fully described in later papers and theses."
1982,An Application of Number Theory to the Organization of Raster-Graphics Memory (Extended Abstract).,"Abstract:
A high-resolution raster-graphics display is usually combined with processing power and a memory organization that facilitates basic graphics operations. For many applications, including interactive text processing, the ability to quickly move or copy small rectangles of pixels is essential. This paper proposes a novel organization of raster-graphics memory that permits all small rectangles to be moved efficiently. The memory organization is based on a doubly periodic assignment of pixels to M memory chips according to a ""Fibonacci"" lattice. The memory organization guarantees that if a rectilinearly oriented rectangle contains fewer than M/‚àö5 pixels, then all pixels will reside in different memory chips, and thus can be accessed simultaneously. We also define a continuous amdogue of the problem which can be posed as, ""What is the maximum density of a set of points in the plane such that no two points are contained in the interior of a rectilinearly oriented rectangle of area N."" We give a lower bound of 1/2N on the density of such a set, and show that 1/‚àö5N can be achieved."
1982,"How to Reuse a ""Write-Once"" Memory (Preliminary Version).","Storage media such as digital optical disks, PROMS, or paper tape consist of a number of -&-ldquo;write-once-&-rdquo; bit positions (wits); each wit initially contains a -&-ldquo;0-&-rdquo; that may later be irreversibly overwritten with a -&-ldquo;I-&-rdquo;. We demonstrate that such -&-ldquo;write-once memories-&-rdquo; (woms) can be -&-ldquo;rewritten-&-rdquo; to a surprising degree. For example, only 3 wits suffice to represent any 2-bit value in a way that can later be updated to represent any other 2-bit value. For large k, 1.29... k wits suffice to represent a k-bit value in a way that can be similarly updated. Most surprising, allowing t writes of a k-bit value requires only t + o(t) wits, for any fixed k. For fixed t, approximately k.t/log(t) wits are required as k -&-rarr; @@@@. An n-wit WOM is shown to have a -&-ldquo;capacity-&-rdquo; (i.e. k.t when writing a k-bit value t times) of up to n.log(n) bits."
1981,Statistical Analysis of the Hagelin Cryptograph.,"This article may be used for research, teaching, and private study purposes. Any substantial or systematic reproduction, redistribution, reselling, loan, sub-licensing, systematic supply, or distribution in any form to anyone is expressly forbidden. The publisher does not give any warranty express or implied or make any representation that the contents will be complete or accurate or up to date. The accuracy of any instructions, formulae, and drug doses should be independently verified with primary sources. The publisher shall not be liable for any loss, actions, claims, proceedings, demand, or costs or damages whatsoever or howsoever caused arising directly or indirectly in connection with or arising out of the use of this material."
1980,"""Forwards and Backwards"" Encryption.","This article may be used for research, teaching, and private study purposes. Any substantial or systematic reproduction, redistribution, reselling, loan, sub-licensing, systematic supply, or distribution in any form to anyone is expressly forbidden. The publisher does not give any warranty express or implied or make any representation that the contents will be complete or accurate or up to date. The accuracy of any instructions, formulae, and drug doses should be independently verified with primary sources. The publisher shall not be liable for any loss, actions, claims, proceedings, demand, or costs or damages whatsoever or howsoever caused arising directly or indirectly in connection with or arising out of the use of this material."
1980,The Subgraph Homeomorphism Problem.,"Abstract
We investigate the problem of finding a homeomorphic image of a ‚Äúpattern‚Äù graph H in a larger input graph G. We view this problem as finding specified sets of edge disjoint or node disjoint paths in G. Our main result is a linear time algorithm to determine if there exists a simple cycle containing three given nodes in G (here H is a triangle). No polynomial time algorithm for this problem was previously known. We also discuss a variety of reductions between related versions of this problem and a number of open problems."
1980,Coping with Errors in Binary Search Procedures.,"Abstract
We consider the problem of identifying an unknown value x œµ {1, 2,‚Ä¶, n} using only comparisons of x to constants when as many as E of the comparisons may receive erroneous answers. For a continuous analogue of this problem we show that there is a unique strategy that is optimal in the worst case. This strategy for the continuous problem is then shown to yield a strategy for the original discrete problem that uses log2n + E ¬∑ log2log2n + O(E ¬∑ log2E) comparisons in the worst case. This number is shown to be optimal even if arbitrary ‚ÄúYes-No‚Äù questions are allowed.
We show that a modified version of this search problem with errors is equivalent to the problem of finding the minimal root of a set of increasing functions. The modified version is then also shown to be of complexity log2n + E ¬∑ log2log2n + O(E ¬∑ log2E)."
1980,On the Polyhedral Decision Problem.,"Computational problems sometimes can be cast in the following form: Given a point ${\bf x}$ in $R^n $, determine if ${\bf x}$ lies in some fixed polyhedron. In this paper we give a general lower bound to the complexity of such problems, showing that $\frac{1}{2}\log _2 f_s $ linear comparisons are needed in the worst case, for any polyhedron with $f_s$s-dimensional faces. For polyhedra with abundant faces, this leads to lower bounds nonlinear in n, the number of variables.
"
1980,Orthogonal Packings in Two Dimensions.,"We consider problems of packing an arbitrary collection of rectangular pieces into an open-ended, rectangular bin so as to minimize the height achieved by any piece. This problem has numerous applications in operations research and studies of computer operation. We devise efficient approximation algorithms, study their limitations, and derive worst-case bounds on the performance of the packings they produce."
1979,An Omega(n/lg n)1/2 Lower Bound on the Number of Additions Necessary to Compute 0-1 Polynomials over the Ring of Integer Polynomials.,n/a
1978,A Method for Obtaining Digital Signatures and Public-Key Cryptosystems.,"An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences: (1) Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intented recipient. Only he can decipher the message, since only he knows the corresponding decryption key. (2) A message can be ‚Äúsigned‚Äù using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed encryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in ‚Äúelectronic mail‚Äù and ‚Äúelectronic funds transfer‚Äù systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product, n, of two large secret primer numbers p and q. Decryption is similar; only a different, secret, power d is used, where e * d ‚â° 1(mod (p - 1) * (q - 1)). The security of the system rests in part on the difficulty of factoring the published divisor, n."
1978,Optimal Arrangement of Keys in a Hash Table.,"When open addressing IS used to resolve collisions in a hash table, a given set of keys may be arranged in many ways, typically this depends on the order in which the keys are inserted It is shown that arrangements minimizing either the average or worst-case number of probes required to retrieve any key in the table can be found using an algorithm for the assignment problem. The worst-case retrieval time can be reduced to O(log2(M)) with probablhty 1 - e(M) when storing Mkeys In a table of size M, where ~(M)~ 0 as M ~ ~ We also examine insertion algorithms to see how to apply these ideas for a dynamically changing set of keys "
1978,k+1 Heads Are Better than k.,"There are languages which can be recognized by a deterministic (k + 1) -headed one-way finite automaton but which cannot be recognized by a k-headed one-way (deterministic or non-deterministic) finite automaton. Furthermore, there is a language accepted by a 2-headed nondeterministic finite automaton which is accepted by no k-headed deterministic finite automaton."
1978,The Subgraph Homeomorphism Problem.,We investigate the problem of finding a homeomorphic image of a ‚Äúpattern‚Äù graph H in a larger input graph G. We view this problem as finding specified sets of edge disjoint or node disjoint paths in G. Our main result is a linear time algorithm to determine if there exists a simple cycle containing three given nodes in G; here H is a triangle. No polynomial time algorithm for this problem was previously known. We also discuss a variety of reductions between related versions of this problem and a number of open problems.
1978,Coping with Errors in Binary Search Procedures (Preliminary Report).,"We consider the problem of identifying an unknown value x&egr;{1,2,...,n} using only comparisons of x to constants when as many as E of 'the comparisons may receive erroneous answers. For a continuous analogue of this problem we show that there is a unique strategy that is optimal in the worst case. This strategy for the continuous problem is then shown to yield a strategy for the original discrete problem that uses log2n+E.log2log2n+O(E.log2E) comparisons in the worst case. This number is shown to be optimal even if arbitrary ‚ÄúYes-No‚Äù questions are allowed. We show that a modified version of this search problem with errors is equivalent to the problem of finding the minimal root of a set of increasing functions. The modified version is then also shown to be of complexity log2n+E.log2log2n+0(E.log2E)."
1977,"The game of ""N questions"" on a tree.","Abstract
We consider the minimax number of questions required to determine which leaf in a finite binary tree T your opponent has chosen, where each question may ask if the leaf is in a specified subtree of T. The requisite number of questions is shown to be approximately the logarithm (base &0slash;) of the number of leaves in T as T becomes large, where √ò = 1.61803‚Ä¶ is the ‚Äúgolden ratio‚Äù. Specifically, q questions are sufficient to reduce the number of possibilities by a factor of
(where F, is the ith Fibonacci number), and this is the best possible."
1977,On the Worst-Case Behavior of String-Searching Algorithms.,"Any algorithm for finding a pattern of length k in a string of length n must examine at least $n - k + 1$ of the characters of the string in the worst case. By considering the pattern $00 \cdots 0$, we prove that this is the best possible result. Therefore there do not exist pattern matching algorithms whose worst-case behavior is ìsublinearî in n (that is, linear with constant less than one), in contrast with the situation for average behavior (the Boyer-Moore algorithm is known to be sublinear on the average).
"
1977,The Necessity of Feedback in Minimal Monotone Combinational Circuits.,"Abstract:
We present a specific n-input 2n-output positive unate Boolean function which can be realized with 2n two-input gates if feedback is used, but which requires 3n-2 gates if feedback is not used."
1977,An Omega(n^2 log n) Lower Bound to the Shortest Paths Problem.,Let P be a polyhedron with fs s-dimensional faces. We show that Œ©(log fs) linear comparisons are needed to determine if a point lies in P. This is used to establish an Œ©(n2 log n) lower bound to the all-pairs shortest path problem between n points.
1976,On Self-Organizing Sequential Search Heuristics.,"This paper examines a class of heuristics for maintaining a sequential list in approximately optimal order with respect to the average time required to search for a specified element, assuming that each element is searched for with a fixed probability independent of previous searches performed. The ‚Äúmove to front‚Äù and ‚Äútransposition‚Äù heuristics are shown to be optimal to within a constant factor, and the transposition rule is shown to be the more efficient of the two. Empirical evidence suggests that transposition is in fact optimal for any distribution of search probabilities."
1976,Constructing Optimal Binary Decision Trees is NP-Complete.,n/a
1976,Linear Expected Time of a Simple Union-Find Algorithm.,n/a
1976,Partial-Match Retrieval Algorithms.,"We examine the efficiency of hash-coding and tree-search algorithms for retrieving from a file of k-letter words all words which match a partially-specified input query word (for example, retrieving all six-letter English words of the form S**R*H where ì*î is a ìdonít careî character).

We precisely characterize those balanced hash-coding algorithms with minimum average number of lists examined. Use of the first few letters of each word as a list index is shown to be one such optimal algorithm. A new class of combinatorial designs (called associative block designs) provides better hash functions with a greatly reduced worst-case number of lists examined, yet with optimal average behavior maintained. Another efficient variant involves storing each word in several lists.

Tree-search algorithms are shown to be approximately as efficient as hash-coding algorithms, on the average.

In general, these algorithms require time about $O(n^{(k - s)/k} )$ to respond to a query word with s letters specified, given a file of nk-letter words. Previous algorithms either required time $O(s \cdot n/k)$ or else used exorbitant amounts of storage.
"
1976,On Recognizing Graph Properties from Adjacency Matrices.,"Abstract
A conjecture of Aanderaa and Rosenberg [15] motivates this work. We investigate the maximum number C(P) of arguments of P that must be tested in order to compute P, a Boolean function of d Boolean arguments. We present evidence for the general conjecture that C(P) = d whenever P(0d) ‚â† P(1d) and P is invariant under a transitive permutation group acting on the arguments. A non-constructive argument (not based on the construction of an ‚Äúoracle‚Äù) settles this question for d a prime power. We use this result to prove the Aanderaa-Rosenberg conjecture: at least
entries of the adjacency matrix of a v-vertex undirected graph G must be examined in the worst case to determine if G has any given non-trivial monotone graph property."
1976,The Mutual Exclusion Problem for Unreliable Processes: Preliminary Report.,"Abstract:
Consider n processes operating asynchronously in parallel, each of wich maintains a single ""special"" variable which can be read (but not written) by the other processes. All coordination between processes is to be accomplished by means of the execution of the primitive operations of a process (1) reading another process's special variable, and (2) setting its own special variable to some value. A process may ""die"" at any time, when its special variable is (automatically) set a special ""dead"" value. A dead process may revive. Reading a special variable which is being simultaneously written returns either the old or the new value. Each process may be in a certain ""critical"" state (which it leaves if it dies). We present a coordination scheme with the following properties. (1) At most one process is ever in its critical state at a time. (2) If a process wants to enter its critical state, it may do so before any other process enters its critical state more than once. (3) The special variables are bounded in value. (4) Some process wanting to enter its critical state can always make progress to that goal. By the definition of the problem, no process can prevent another from entering its critical state by repeatedly failing and restarting. In the case of two processes, what makes our solution of particular interest is its remarkable simplicity when compared with the extant solutions to this problem. Our n-process solution uses the two-process solution as a subroutine, and is not quite as elegant as the two-process solution."
1976,k+1 Heads Are Better than k.,"Abstract:
There are languages which can be recognized by a deterministic (k + 1)-headed oneway finite automaton but which cannot be recognized by a k-headed one-way (deterministic or non-deterministic) finite automaton. Furthermore, there is a language accepted by a 2-headed nondeterministic finite automaton which is accepted by no k-headed deterministic finite automaton."
1975,Expected Time Bounds for Selection.,"A new selection algorithm is presented which is shown to be very efficient on the average, both theoretically and practically. The number of comparisons used to select the ith smallest of n numbers is n + min(i,n-i) + o(n). A lower bound within 9 percent of the above formula is also derived."
1975,The Algorithm SELECT - for Finding the ith Smallest of n Elements [M1] (Algorithm 489).,"SELECT will rearrange the values of array segment X[L: R] so that X[K] (for some given K; L ‚â§ K ‚â§ R) will contain the (K-L+1)-th smallest value, L ‚â§ I ‚â§ K will imply X[I] ‚â§ X[K], and K ‚â§ I ‚â§ R will imply X[I] ‚â• X[K. While SELECT is thus functionally equivalent to Hoare's algorithm FIND [1], it is significantly faster on the average due to the effective use of sampling to determine the element T about which to partition X. The average time over 25 trials required by SELECT and FIND to determine the median of n elements was found experimentally to be: n 500 1000 5000 10000 SELECT 89 ms. 141 ms. 493 ms. 877 ms. FIND 104 ms. 197 ms. 1029 ms. 1964 ms. The arbitrary constants 600, .5, and .5 appearing in the algorithm minimize execution time on the particular machine used. SELECT has been shown to run in time asymptotically proportional to N + min (I, N-I), where N = L - R + 1 and I = K - L + 1. A lower bound on the running time within 9 percent of this value has also been proved [2]. Sites [3] has proved SELECT terminates."
1975,A Generalization and Proof of the Aanderaa-Rosenberg Conjecture.,"We investigate the maximum number C(P) of arguments of P that must be tested in order to compute P, a Boolean function of d Boolean arguments. We present evidence for the general conjecture that C(P)&equil;d whenever P(0d) @@@@ P(1d) and P is left invariant by a transitive permutation group acting on the arguments. A non-constructive argument (not based on the construction of an ‚Äúoracle‚Äù) proves the generalized conjecture for d a prime power. We use this result to prove the Aanderaa-Rosenberg conjecture by showing that at least v2/9 entries of the adjacency matrix of a v-vertex undirected graph G must be examined in the worst case to determine if G has any given non-trivial monotone graph property."
1974,Asymptotic bounds for the number of convex n-ominoes.,"Abstract
Unit squares having their vertices at integer points in the Cartesian plane are called cells. A point set equal to a union of n distinct cells which is connected and has no finite cut set is called an n-omino. Two n-ominoes are considered the same if one is mapped onto the other by some translation of the plane. An n-omino is convex if all cells in a row or column form a connected strip. Letting c(n) denote the number of different convex n-ominoes, we show that the sequence ((c(n))1n: n=1,2,‚Ä¶) tends to a limit Œ≥ and Œ≥=2.309138‚Ä¶."
1974,On Hash-Coding Algorithms for Partial-Match Retrieval (Extended Abstract).,"Abstract:
We examine the efficiency of generalized hash-coding algorithms for performing partial-match searches of a random-access file of binary words. A precise characterization is given of those hash functions which minimize the average number of buckets examined for a search; and a new class of combinatorial designs is introduced which permits the construction of hash functions with worst-case behavior approaching the best achievable average behavior in many cases."
1974,On Self-Organizing Sequential Search Heuristics.,"Abstract:
We examine a class of heuristics for maintaining a sequential list in approximately optimal order with respect to the average time required to search for a specified element, assuming that we search for each element with a fixed probability independent of previous searches performed. The ""move to front"" and ""transposition"" heuristics are shown to be optimal to within a constant factor, and the transposition rule is shown to be the more efficient of the two. Empirical evidence suggests that transposition is in fact optimal for any distribution of search probabilities."
1974,On the Optimality of Elia's Algorithm for Performing Best-Match Searches.,n/a
1973,Time Bounds for Selection.,"The number of comparisons required to select the i-th smallest of n numbers is shown to be at most a linear function of n by analysis of a new selection algorithm‚ÄîPICK. Specifically, no more than 5.4305 n comparisons are ever required. This bound is improved for extreme values of i, and a new lower bound on the requisite number of comparisons is also proved."
1972,Linear Time Bounds for Median Computations.,"New upper and lower bounds are presented for the maximum number of comparisons, f(i,n), required to select the i-th largest of n numbers. An upper bound is found, by an analysis of a new selection algorithm, to be a linear function of n: f(i,n) ‚â§ 103n/18 < 5.73n, for 1 ‚â§ i ‚â§ n. A lower bound is shown deductively to be: f(i,n) ‚â• n+min(i,n‚àíi+l) + [log2(n)] ‚àí 4, for 2 ‚â§ i ‚â§ n‚àí1, or, for the case of computing medians: f([n/2],n) ‚â• 3n/2 ‚àí 3"
