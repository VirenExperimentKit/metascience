2019,new golden age computer architecture,n/a
2019,spectre meltdown processor security vulnerability,paper first review spectre meltdown processor security vulnerability revealed january-october  allow extraction protected information billion processor large small system discus short-term mitigation action speculates longer term implication computer software hardware paper expands keynote/panel author ieee hot chip 
2016,common bond mips hp two-level branch prediction compressed code risc processor,column feature retrospective author six micro test time award-winning paper mips microprocessor architecture norman jouppi colleague hp new microarchitecture rationale introduction yale patt wen-mei hwu mike shebanow critical issue regarding hp high performance microarchitecture yale patt colleague hardware support large atomic unit dynamically scheduled machine steve melvin mike shebanow yale patt two-level adaptive training branch prediction tse-yu yeh yale patt executing compressed program embedded risc architecture andy wolfe alex chanin
2003,latency occupancy bandwidth dsm multiprocessor performance evaluation,desire use commodity part communication architecture dsm multiprocessor offer advantage cost design time impact application performance unclear study performance impact detailed simulation analytical modeling experiment flexible dsm prototype using range parallel application adapt logp model characterize communication architecture dsm machine l   parameter key performance machine g  parameter becoming important fastest controller show logp parameter controller occupancy greatest impact application performance two contribution occupancy performance degradation-the latency add contention induces-it contention component governs performance regardless network latency showing quadratic dependence expected technique reduce impact latency make controller occupancy greater bottleneck surprisingly performance impact occupancy substantial even highly-tuned application even absence latency hiding technique scaling problem size often used technique overcome limitation communication latency bandwidth experiment dsm prototype show important class application performance lost using higher occupancy controller cannot regained easily scaling problem size
2000,efficient performance prediction modern microprocessor,generating accurate estimate performance program given system important large number people computer architect compiler writer developer need insight machine performance number performance estimation technique use profile-based approach full machine simulation paper discus profile-based performance estimation technique us lightweight instrumentation phase run order number dynamic instruction followed analysis phase run roughly order number static instruction technique accurately predicts performance core pipeline detailed out-of-order issue processor model scheduling far fewer instruction full simulation difference predicted execution time time obtained full simulation percent
1999,future system research, year academia silicon valley new provost stanford university call shift focus system research performance-long centerpiece-needs share spotlight availability maintainability quality although performance increase past  year truly amazing hard continue trend sticking basically evolutionary path research community currently author advocate evolutionary approach system problem think approach need integrated researcher need think hardware software continuum separate part see society threshold post pc era computing ubiquitous everyone use information service everyday utility everyone start using system guess expect work easy use era drive system design toward greater availability maintainability scalability require real refocusing current research direction
1999,quantitative analysis performance scalability distributed shared memory,scalable cache coherence protocol become key technology creating moderate large-scale shared-memory multiprocessor although performance multiprocessor depends critically performance cache coherence protocol little comparative performance data available existing commercial implementation use variety different protocol including bit-vector/coarse-vector protocol sci-based protocol coma protocol using programmable protocol processor stanford flash multiprocessor provide detailed implementation-oriented evaluation four popular cache coherence protocol addition measurement characteristic protocol execution  overall performance examine effect scaling processor count   processor surprisingly optimal protocol change different application change processor count even within application  help identify strength specific protocol illustrate benefit providing flexibility choice cache coherence protocol
1998,retrospective evaluation directory dchemes cache coherence,paper born work done  aimed exploring way build scalable shared-memory multiprocessor research began special graduate seminar class spring  initiated brainstorm architecture sharedmemory multiprocessor prior beginning work paper considered variety scheme avoiding cache coherence particular initially explored softwarebased cache coherence scheme concluded scheme might work well highly structured loop-intensive scientific application software-based cache coherence tiefficient le structured scientific application well operating system software software environment concluded either many cache invalidation would needed much data would need kept uncached concluding software-based coherence would sufficient broad range application still felt coherence scheme fully general might expensive hard scale began exploring variety scheme significantly limited sharing one first thought consider scheme allowed single reader shared writable data item scheme generated part basis paper well broader exploration directory approach
1998,flexible use memory replication/migration cache-coherent dsm multiprocessor,given limitation bus-based multiprocessor cc-numa scalable architecture choice shared-memory machine important characteristic cc-numa architecture latency access data remote node considerably larger latency access local memory machine good data locality reduce memory stall time therefore critical factor application performance paper study various option available system designer transparently decrease fraction data miss serviced remotely work done context stanford flash multiprocessor flash unique node single pool dram used variety way programmable memory controller use programmability flash explore different option cache-coherence data-locality compute-server workload first consider two protocol providing base cache-coherence one centralized directory information  another distributed directory information  several commercial system based sci find centralized scheme superior performance next consider different hardware software technique use local memory node improve data locality finally propose hybrid scheme combine hardware software technique scheme work base platform user kernel reference workload paper thus offer realistic fair comparison replication/migration technique previously feasible
1998,evaluation directory scheme cache coherence,problem cache coherence shared-memory multiprocessor addressed using two basic approach directory scheme snoopy cache system directory scheme cache coherence potentially attractive large multiprocessor system beyond scaling limit snoopy cache scheme slight modification directory scheme make competitive performance snoopy cache scheme small multiprocessor trace-driven simulation using data collected several real multiprocessor application used compare performance standard directory scheme modification scheme snoopy cache protocol addition simulation show block written present small number cache make broadcast invalidates inefficient result suggests directory structure store block small number pointer cache containing block sufficient
1998,memory consistency event ordering scalable shared-memory multiprocessor,new model memory consistency called release consistency allows buffering pipelining previously proposed model introduced framework classifying shared access reasoning event ordering developed release consistency model shown equivalent sequential consistency model parallel program sufficient synchronization possible performance gain le strict constraint release consistency model explored finally practical implementation issue discussed discussion concentrating issue relevant scalable architecture
1998,dash prototype implementation performance,fundamental premise behind dash project feasible build large-scale shared-memory multiprocessor hardware cache coherence paper study software simulator useful understanding many high-level design tradeoff prototype essential ensure critical detail overlooked prototype provides convincing evidence feasibility design allows one accurately estimate hardware complexity cost various feature provides platform studying real workload -processor prototype dash multiprocessor operational last six month paper hardware overhead directory-based cache coherence prototype examined also discus performance system speedup obtained parallel application running prototype using sophisticated hardware performance monitor characterize effectiveness coherent cache relationship application reference behavior speedup
1998,stanford flash multiprocessor,flash multiprocessor efficiently integrates support cache-coherent shared memory high-performance message passing minimizing hardware software overhead node flash contains microprocessor portion machine global memory port interconnection network magic chip handle communication within node among node using hardwired data path efficient data movement programmable processor optimized executing protocol operation use protocol processor make flash flexible/spl minus/it support variety different communication mechanisms/spl minus/and simplifies design implementation paper present architecture flash magic discus base cache-coherence message-passing protocol latency occupancy number derived system-level simulator verilog code given several common protocol operation paper also describes software strategy flash current status
1997,nationwide parallel computing environment,national computational science alliance  hope accelerate creation nationwide parallel computing environment u developing hardware software ease desktop access national technology grid ncsas plan focus experimental computer architecture high performance user program machine independent analyzer parallel computer advanced application support leverage parallel adaptive method numerical optimization 
1997,evaluation commercial cc-numa architecture - convex exemplar spp1200,study done academic cc-numa machine simulator indicate good potential application performance goal therefore investigate whether convex exemplar commercial distributed shared memory machine life expected potential cc-numa machine would like understand architectural implementation decision make le efficient evaluating delivered performance exemplar find moderate-scale exemplar machine work well several application important class performance affected four fundamental characteristic machine due basic implementation design choice made exemplar effect processor clustering together limited node-to-network bandwidth effect tertiary cache limited user control data placement sequential memory consistency model together cache-based cache coherence protocol lastly longer remote latency
1996,softflash analyzing performance clustered distributed virtual shared memory,one potentially attractive way build large-scale shared-memory machine use small-scale medium-scale shared-memory machine cluster interconnected off-the-shelf network create shared-memory programming environment across cluster possible use virtual shared-memory software layer low latency high bandwidth interconnect available within cluster clear advantage making cluster large possible critical question becomes whether latency bandwidth top-level network software system sufficient support communication demand generated clustersto explore question built aggressive kernel implementation virtual shared-memory system using sgi multiprocessor mbyte/sec hippi interconnects system obtains speedup  processor  range  communication-intensive fft program  ocean  general clustering effective reducing internode miss rate cluster size increase increase remote latency mostly due increased tlb synchronization cost offset advantage communication-intensive application fft overhead sending network request limited network bandwidth long network latency prevent achievement good performance overall approach still appears promising  indicate large low latency network may needed make cluster-based virtual shared-memory machine broadly useful large-scale shared-memory multiprocessor
1996,application architectural bottleneck large scale distributed shared memory machine,many programming challenge encountered small moderate-scale hardware cache-coherent shared memory machine extensively studied work remains done basic technique needed efficiently program machine well explored recently number researcher presented architectural technique scaling cache coherent shared address space much larger processor count paper examine extent application achieve reasonable performance large-scale cache-coherent distributed shared address space machine determining problem size needed achieve reasonable level efficiency also look much programming effort optimization needed achieve high efficiency beyond needed small processor count application discus main architectural bottleneck prevent smaller problem size le optimized program achieving good efficiency  show application either scale must heavily optimized application studied necessary heavily modify code restructure algorithm scale well upto several hundred processor basic technique load balancing data locality used needed small-scale system well program written care perform well without substantially compromising ease programming advantage shared address space problem size required achieve good performance surprisingly small important careful data structure layout interact system granularity optimization usually needed moderate-scale machine well
1996,computer architecture curriculum stanford challenge approach,n/a
1995,effectiveness data dependence analysis,data dependence testing basic step detecting loop level parallelism numerical program problem undecidable general case therefore work concentrated simplified problem affine memory disambiguation simpler domain array reference loop bound assumed linear integer function loop variable dataflow information ignored domain shown practice problem solved accurately efficiently paper study empirically effectiveness domain restriction many real reference affine flow insensitive use larus llpp system find data dependence dynamically compare  given affine memory disambiguation system system exact case see practice show affine approximation reasonable memory disambiguation sufficient approximation data dependence analysis propose extension improve analysis
1995,load balancing data locality adaptive hierarchical n-body method barnes-hut fast multipole rasiosity,hierarchical n-body method based fundamental insight nature many physical process increasingly used solve large-scale problem variety scientific/engineering domain application use method challenging parallelize effectively however owing nonuniform dynamically changing characteristic need long-range communication paper study partitioning scheduling technique required obtain effective parallel performance application use range hierarchical n-body method obtain representative coverage first examine application use two best method known classical n-body problem barnes-hut method fast multipole method examine recent hierarchical method radiosity calculation computer graphic applies hierarchical n-body approach problem different characteristic find straightforward decomposition technique automatic scheduler might implement scale well unable simultaneously provide load balancing data locality however application yield good parallel performance appropriate partitioning scheduling technique implemented programmer application use barnes-hut fast multipole method simple yet effective partitioning technique developed exploiting key insight method classical problem solve using novel partitioning technique even relatively small problem achieve -fold speedup -processor stanford dash machine  -fold speedup -processor simulated architecture different characteristic radiosity application require different partitioning/scheduling approach used however yield good parallel performance
1995,implication hierarchical n-body method multiprocessor architecture,design effective large-scale multiprocessor designer need understand characteristic application use machine application characteristic particular interest include amount communication relative computation structure communication local cache memory requirement well characteristic scale larger problem machine one important class application based hierarchical n-body method used solve wide range scientific engineering problem efficiently important characteristic method include nonuniform dynamically changing nature domain applied use long-range irregular communication article examines key architectural implication representative application use two dominant hierarchical n-body method barnes-hut method fast multipole methodwe first show exploiting temporal locality access communicated data critical obtaining good performance application argue coherent cache shared-address-space machine exploit locality automatically effectively next examine implication scaling application run larger machine use scaling method reflect concern application scientist find lead different communication traffic local cache memory usage scale scaling based data set size particular show realistic form scaling communication-to-computation ratio well working-set size  grow slowly larger problem run larger machine finally examine effect using two dominant abstraction interprocessor communication shared address space explicit message passing private address space show lack efficiently supported shared address space substantially increase programming complexity performance overhead application
1995,position paper,n/a
1994,cool object-based language parallel programming,effectively using shared-memory multiprocessor requires substantial programming effort present programming language cool  designed exploit coarse-grained parallelism task level shared-memory multiprocessor cool primary design goal efficiency expressiveness efficiency mean language construct efficient implement program pay feature use expressiveness imply program flexibly support different concurrency pattern thereby allowing various decomposition problem cool emphasizes integration concurrency synchronization data abstraction ease task creating modular efficient parallel program extension c++ chosen support abstract data type definition widely used< >
1994,suif infrastructure research parallelizing optimizing compiler,compiler infrastructure support experimental research crucial advancement high-performance computing new compiler technology must implemented evaluated context complete compiler developing infrastructure requires huge investment time resource spent number year building suif compiler powerful flexible system would like share  effortssuif consists small clearly documented kernel toolkit compiler pass built top kernel kernel defines intermediate representation provides function access manipulate intermediate representation structure interface compiler pass toolkit currently includes c fortran front end loop-level parallelism locality optimizer optimizing mips back end set compiler development tool support instructional usealthough expect suif suitable everyone think may useful many researcher thus invite use suif welcome contribution infrastructure direction obtaining suif software included end paper
1994,false sharing spatial locality multiprocessor cache,performance data cache shared-memory multiprocessor shown different uniprocessors particular cache miss rate multiprocessor show sharp drop typical uniprocessors size cache block increase resulting high cache miss rate cause concern since significantly limit performance multiprocessor researcher speculated effect due false sharing coherence transaction result different processor update different word cache block interleaved fashion analysis six application paper confirms false sharing significant impact miss rate measurement also show poor spatial locality among access shared data even larger impact mitigate false sharing enhance spatial locality optimize layout shared data cache block programmer-transparent manner show approach reduce number miss shared data  average< >
1994,performance advantage integrating block data trabsfer cache-coherent multiprocessor,integrating support block data transfer become important emphasis recent cache-coherent shared address space multiprocessor paper examines potential performance benefit adding support set ambitious hardware mechanism used study performance gain five important scientific computation appear good candidate using block transfer  benefit block transfer substantial hardware cache-coherent multiprocessor main reason  relatively modest fraction time application spend communication amenable block transfer  difficulty finding enough independent computation overlap communication latency remains block transfer  long cache line often capture many benefit block transfer efficient cache-coherent machine case block transfer improves performance prefetching often provide comparable superior performance benefit also examine impact varying important communication parameter processor speed effectiveness block transfer comment useful feature block transfer facility support real application
1994,performance impact flexibility stanford flash multiprocessor,flexible communication mechanism desirable feature multiprocessor allows support multiple communication protocol expands performance monitoring capability lead simpler design debug process stanford flash multiprocessor flexibility obtained requiring transaction node pas programmable node controller called magic paper evaluate performance cost flexibility comparing performance flash idealized hardwired machine representative parallel application multiprogramming workload measure performance flash use detailed simulator flash magic design together code sequence implement cache-coherence protocol find range optimized parallel application performance difference idealized machine flash small program either miss rate small latency programmable protocol hidden behind memory access time application incur large number remote miss exhibit substantial hot-spotting performance poor machine though increased remote access latency occupancy magic lead lower performance flexible design case however flash  slower idealized machine
1994,evaluating memory overhead required coma architecture,cache memory architecture  inherent memory overhead due organization main memory large cache called attraction memory overhead consists memory left unallocated performance reason well additional physical memory required due cache organization memory author examine effect data reshuffling data replication memory overhead data reshuffling occurs space need allocated store remote memory line local memory data reshuffled sent memory via replacement message simple mathematical model predicts frequency data reshuffling function attraction memory parameter simulation data show frequency data reshuffling sensitive allocation policy associativity memory relatively unaffected block size chosen simulation data also show data replication attraction memory important good performance gain achieved replication processor caches< >
1993,scaling parallel program multiprocessor methodology example,model constraint application scaled including constant problem-size scaling memory-constrained scaling time-constrained scaling reviewed realistic method described scale relevant parameter consideration imposed application domain method lead different effectiveness design large multiprocessor naive practice scaling data set size primary example application simulation galaxy using barnes-hut hierarchical n-body method< >
1993,compile-time copy elimination,singleassignment functional language value semantics permit sideeffects lack sideeffects make automatic detection parallelism optimization data locality program much easier however property pose challenge implementing language efficiently paper describes optimizing compiler system solves key problem aggregate copy elimination method developed rely exclusively compiletime algorithm including interprocedural analysis applied intermediate data flow representation dividing problem updateinplace buildinplace analysis small set relatively simple techniquesedge substitution graph pattern matching substructure sharing substructure targetingwas found powerful combined properly implemented carefully algorithm eliminate unnecessary copy operation high degree runtime overhead imposed compiled program
1993,mtool integrated system performance debugging shared memory multiprocessor application,author describe mtool software tool analyzing performance loss shared memory parallel program mtool augments program low overhead instrumentation perturbs program execution little possible generating enough information isolate memory synchronization bottleneck running instrumented version parallel program programmer use mtools window-based user interface view compute time memory synchronization object author describe mtools low overhead instrumentation method memory bottleneck detection technique attention focusing mechanism contrast mtool approach offer case study demonstrate effectiveness< >
1993,dash prototype logic overhead performance,fundamental premise behind dash project feasible build large-scale shared-memory multiprocessor hardware cache coherence hardware overhead directory-based cache coherence -processor examined data show overhead - appears small cost ease programming offered coherent cache potential higher performance performance system discussed speedup obtained variety parallel application running prototype shown using sophisticated hardware performance monitor effectiveness coherent cache relationship application reference behavior speedup characterized optimization incorporated dash protocol evaluated term effectiveness parallel application atomic test stress memory system< >
1993,data locality load balancing cool,hierarchical n-body method based fundamental insight nature many physical process increasingly used solve large-scale problem variety scientific/engineering domain application use method challenging parallelize effectively however owing nonuniform dynamically changing characteristic need long-range communication paper study partitioning scheduling technique required obtain effective parallel performance application use range hierarchical n-body method obtain representative coverage first examine application use two best method known classical n-body problem barnes-hut method fast multipole method examine recent hierarchical method radiosity calculation computer graphic applies hierarchical n-body approach problem different characteristic find straightforward decomposition technique automatic scheduler might implement scale well unable simultaneously provide load balancing data locality however application yield good parallel performance appropriate partitioning scheduling technique implemented programmer application use barnes-hut fast multipole method simple yet effective partitioning technique developed exploiting key insight method classical problem solve using novel partitioning technique even relatively small problem achieve -fold speedup -processor stanford dash machine  -fold speedup -processor simulated architecture different characteristic radiosity application require different partitioning/scheduling approach used however yield good parallel performance
1993,parallel adaptive fast multipole method,parallel version representative n-body application us l greengard v rokhlins  adaptive fast multipole method  presented parallel implementation uniform fmm straightforward developed different architecture adaptive version complicates task obtaining effective parallel performance owing nonuniform dynamically changing nature problem domain applied author propose evaluate two technique providing load balancing data locality take advantage key insight method typical application using better technique demonstrate -fold speedup galactic simulation -processor stanford dash machine state-of-the-art shared address space multiprocessor even relatively small problem also show good speedup two-ring kendall square research ksr- finally summarize key architectural implication important computational method
1993,empirical comparison kendall square research ksr-1 stanford dash multiprocessor,two interesting variant large-scale shared-address-space parallel architecture cache-coherent non-uniform-memory-access machine  cache-only memory architecture  distributed main memory use directory-based cache coherence architecture migrate replicate data cache level automatically hardware control coma machine main memory level well author compare parallel performance recent realization type architecture stanford dash multiprocessor  kendall square research ksr-  using suite important computational kernel complete scientific application examine performance difference resulting cc-numa/coma nature machine well specific difference system implementation
1993,accuracy trace-driven simulation multiprocessor,trace-driven simulation trace generated one set system characteristic used simulate system different characteristic however execution path multiprocessor workload may depend order event occurring different processing element event order turn depends system charcteristics memory-system latency buffer-sizes trace-driven simulation multiprocessor workload inaccurate unless dependency eliminated traceswe measured effect inaccuracy comparing trace-driven simulation direct simulation workload simulator predicted identical performance workload whose trace timing-independent workload used first-come first-served scheduling and/or non-deterministic algorithm produced timing-dependent trace simulation trace produced inaccurate performance prediction two type performance metric particularly affected related synchronization latency derived relatively small number event accurately predict performance metric timing-independent trace direct simulation used
1992,stanford dash multiprocessor,overall goal major feature directory architecture shared memory  presented fundamental premise behind architecture possible build scalable high-performance machine single address space coherent cache dash architecture scalable achieves linear near-linear performance growth number processor increase thousand performance  distributing memory among processing node using network scalable bandwidth connect node architecture allows shared data cached significantly reducing latency memory access yielding higher processor utilization higher overall performance distributed directory-based protocol provides cache coherence without compromising scalability discussed detail dash prototype machine corresponding software support described< >
1992,finding exploiting parallelism ocean simulation program experience result implication,code compile large application program execution parallel processor perhaps biggest challenge facing widespread adoption multiprocessing gain insight problem ocean simulation application converted parallel version parallel program demonstrated near-linear speed-up encore multimax -processor bus-based shared-memory machine parallelizing existing sequential application—not single loop computational kernel—leads interesting insight issue significant process finding implementing parallelism major challenge three level approach problem finding parallelism—loop-level parallelization program restructuring algorithm modification—were attempted widely varying  loop-level parallelization scale sufficiently high-level restructuring useful much application obtaining efficient parallel program required algorithm change one portion implementation issue scalable performance data locality synchronization also discussed nature requirement success various transformation lend insight design parallelizing tool parallel programming environment
1992,programming different memory consistency model,memory consistency model memory model supported shared-memory multiprocessor directly affect performance commonly assumed memory model sequential consistency  sc provides simple model programmer imposes rigid constraint ordering memory access restricts use common hardware compiler optimization remedy shortcoming sc several relaxed memory model proposed literature include processor consistency  weak ordering  release consistency  total store ordering  partial store ordering  relaxed model provide potential higher performance present complex model programmer compared sc previous research addressed tradeoff taking programmer-centric approach proposed memory model  allow programmer reason sc require certain information memory access information used system relax ordering among memory access still maintaining sc programmer previous model formalized information allowed optimization associated wo rcsc used paper extends approach defining new model plpc allows optimization tso pso pc rcpc model well thus plpc provides unified programming model maintains ease reasoning sc providing efficiency portability across wide range proposed system design
1992,characterizing caching synchronization performance multiprocessor operating system,good cache memory performance essential achieving high cpu utilization shared-memory multiprocessor performance cache determined application end operating system  reference research focused cache performance application afone partiafly due difficulty measuring o activity resrtl~ cache performance o largely unknown paper characterize cache performance commercial system v unix rtrttrtittg four-cpu multiprocessor related issue performance impact o synchronization activity tdso stttdicd study use hardware monitor record cache miss machine without perturbing study three multiprocessor workload parallel compilq multiprogrsmmed load commercial database  show o miss occur frequently enough stall cpu - yoof non-idle time include application miss induced o interference cache squ time reach  detailed analysis reveals three major source o miss instruction fetehea process migratiom data access block operation synchronization behavior find o syncfrrordzation low overhead supported correctly end o lock show good locality low contention
1992,hiding memory latency using dynamic scheduling shared-memory multiprocessor,large latency memory access major impediment achieving high performance large scale shared-memory multi-processsors relaxing memory consistency model attractive technique hiding latency allowing overlap memory access computation memory access previous study relaxed model shown latency write access hidden buffering writes allowing read bypass pending writes hiding latency read exploiting overlap allowed relaxed model inherently difficult however simply processor depends return value future computationthis paper explores use dynamically scheduled processor exploit overlap allowed relaxed model hiding latency read  based detailed simulation study several parallel application  show substantial fraction read latency hidden using technique however major improvement performance achieved large instruction window size
1992,sharlit - tool building optimizers,complex time-consuming function modern compiler global optimization unlike function compiler parsing code generation examine one statement one basic block time optimizers much larger scope examining changing large portion program larger scope mean optimizers must perform many program transformation transformation make particular demand internal representation program interact depend others different way make optimizers large complexdespite complexity tool exist help building optimizers stark contrast part compiler year experience culminated tool part constructed easily example parser generator used build front-ends peephole optimizers tree matcher used build code generatorsthis paper present sharlit tool support construction modular extensible global optimizers show sharlit help constructing data-flow analyzer transformation use data-flow analysis information major component optimizersharlit implemented c++ us c++ way yacc us c thus assume reader familiarity c++
1991,computer technology architecture evolving interaction,interaction computer architecture ic technology examined evaluate attractiveness particular technology computer design assessed primarily basis performance cost focus mainly cpu performance easier measure impact technology easily seen cpu technology trend discussed concern memory size design complexity time design scaling architectural trend area pipelining memory system multiprocessing considered opportunity problem solved year ahead identified< >
1991,performance evaluation memory consistency model shared memory multiprocessor,memory consistency model supported multiprocessor architecture determines amount buffering pipelining may used hide reduce latency memory access several different consistency model proposed range sequential consistency one end allowing limited buffering release consistency end allowing extensive buffering pipelining processor consistency weak consistency model fall advantage le strict model increased performance potential disadvantage increased hardware complexity complex programming model make informed decision tradeoff requires performance data various model paper address issue performance benefit four consistency model  based simulation study done three application  show environment processor read blocking writes buffered significant performance increase achieved allowing read bypass previous writes pipelining writes determines rate writes retired write buffer secondary importance result show sequential consistency model performs poorly relative model processor consistency model provides benefit weak release consistency model
1991,multiprocessor simulation tracing using tango,tango software simulation tracing system used obtain data evaluating parallel program multiprocessor system system provides simulated multiprocessor environment multiplexing application process onto single processor user application compiled simulation system customized meet accuracy efficiency requirement particular set experiment tango support trace-driven simulation user also option using execution-driven simulation unlike trace-driven simulation execution-driven simulation ensures accurate ordering program event permit accurate simulation contention process interaction tango avoids costly instruction interpretation emulation directly executing user application code whenever possible simulation time also greatly reduced allowing user focus program operation primary interest shared data reference synchronization operation system currently used wide range investigation algorithm study detailed hardware evaluation
1991,mtool method isolating memory bottleneck shared memory multiprocessor program,paper present new relatively inexpensive method detecting region  program memory hierarchy performing poorly observing actual measured execution time differs time predicted given perfect memory system isolate memory bottleneck mtool implementation approach aimed application program running mips-chip based workstation described  perfect club spec benchmark summarized
1991,two technique enhance performance memory consistency model,memory consistency model supported multiprocessor directly affect performance thus several attempt made relax consistency model allow buffering pipelining memory access unfortunately potential increase performance afforded relaxing consistency model accompanied complex programming model paper introduces two general implementation technique provide higher performance model first technique involves prefetching value access delayed due consistency model constraint second technique employ speculative execution allow processor proceed even though consistency model requires memory access delayed combined technique alleviate limitation imposed consistency model buffering pipelining memory access thus significantly reducing impact memory consistency model performance
1991,comparative evaluation latency reducing tolerating technique,technique cope large latency memory access essential achieving high processor utilization large-scale shared-memory multiprocessor paper consider four architectural technique address latency problem  hardware coherent cache  relaxed memory consistency  softwarecontrolled prefetching  multiple-context support study benefit individual technique done study evaluates technique within consistent framework paper attempt remedy providing comprehensive evaluation benefit four technique individually combination using consistent set architectural assumption  paper obtained using detailed simulation large-scale shared-memory multiprocessor  show cache relaxed consistency uniformly improve performance improvement due prefetching multiple context sizeable much applicationdependent combination various technique generally attain better performance one overall show using suitable combination technique performance improved   time
1991,integrating scalar optimization parallelization,compiling program use parallelism memory hierarchy efficiently requires parallelizing  transformation traditional scalar  optimization compiler one intermediate language — suif  — parallelization scalar optimization one intermediate language offer two advantage duplication function two level availability high-level information low-level paper show suif integrates two level abstraction suif compiler organized take advantage integration
1991,efficient exact data dependence analysis,data dependence testing basic step detecting loop level parallelism numerieal program problem equivalent integer linear programming thus general cannot solved efficiently current method use employ inexact method sacrifice potential parallelism order improve compiler efficiency paper show practice data dependence computed exactly efficiently three major idea lead result first developed assembled small set efficient algorithm one exact special case input combined moderately expensive backup test exact case seen practice second introduce memorization technique save  previous test thus avoiding calling data dependence routine multiple time input third show approach extended compute distance direction vector use unknown ymbolic term without loss accuracy efficiency implemented algorithm suif system general purpose compiler system developwl stanford ran algorithm perfect club benchmark data dependence analyzer gave exact solution case efficiently
1991,performance debugging shared memory multiprocessor program mtool,paper describes mtool sofware tool analyzing performance loss shared memory parallel program mtool augments program low overhead instrumentatwn perturbs program execution little possible generating enough irylormation isolate memory synchronization bottleneck running instrumented version parallel program programmer use mtools window-based user interface view memory synchronization compute time bottleneck increasing level detailfrom whole program level level individual procedure loop synchrordzation object initial implementation mtool run silicon graphic multiprocessor use several group stanford
1991,mtool method detecting memory bottleneck,paper present new relatively inexpensive method detecting region  program memory hierarchy performing poorly observing actual measured execution time differs time predicted given perfect memory system isolate memory bottleneck mtool implementation approach aimed application program running mips-chip based workstation described  perfect club spec benchmark summarized
1990,spectral lower bound techniqye size decision tree two level and/or circuit,universal lower-bound technique size implementation characteristic arbitrary boolean function decision tree two-level and/or circuit derived technique based power spectrum coefficient n dimensional fourier transform function bound vary constant exponential tight many case several example presented< >
1990,priority-based coloring approach register allocation,global register allocation play major role determining efficacy optimizing compiler graph coloring used central paradigm register allocation modern compiler straightforward coloring approach suffer several shortcoming shortcoming addressed paper coloring graph using priority ordering natural method dealing spilling emerges approach detailed algorithm priority-based coloring approach presented contrasted basic graph-coloring algorithm various extension basic algorithm also presented measurement large program used determine effectiveness algorithm extension well cause imperfect allocation running time allocator impact heuristic aimed reducing time also measured
1990,design scalable shared-memory multiprocessor dash approach,dash  multiprocessor combine programmability shared-memory machine scalability message-passing machine described hardware-supported coherent cache provide low-latency access shared data ease programming cache kept coherent mean distributed directory-based protocol shared memory machine distributed among processing node scalable memory bandwidth provided connecting node general interconnection network prototype dash machine consists  high-performance microprocessor aggregate performance  mips  scalar mflop fundamental premise dash possible build scalable shared-memory machine hardware-supported coherent cache using distributed directory-based cache coherence protocol mechanism providing scalable memory bandwidth reducing tolerating memory latency supporting efficient synchronization described brief description machine implementation given<>
1990,estimating performance advantage relaxing consistency shared memory multiprocessor,n/a
1990,share data placement optimization reduce multiprocessor cache miss rate,n/a
1990,directory-based cache coherence protocol dash multiprocessor,dash scalable shared-memory multiprocessor currently developed stanford computer system laboratory architecture consists powerful processing node portion shared-memory connected scalable interconnection network key feature dash distributed directory-based cache coherence protocol unlike traditional snoopy coherence protocol dash protocol rely broadcast instead us point-to-point message sent processor memory keep cache consistent furthermore dash system contain single serialization control point feature provide basis scalability also force reevaluation many fundamental issue involved design protocol include issue correctness performance protocol complexity paper present design dash coherence protocol discus address issue also discus strategy verifying correctness protocol briefly compare protocol ieee scalable coherent interface protocol
1990,analysis critical architectural program parameter hierarchical shared memory multiprocessor,scalable shared-memory multiprocessor subject much current research little known performance behavior machine paper study performance effect two machine characteristic two program characteristic seem major factor determining performance hierarchical shared-memory machine develop analytical model traffic machine loosely based stanford dash multiprocessor use program parameter extracted multiprocessor trace study performance shown locality data reference stream amount data sharing program important impact performance although le obvious bandwidth within cluster hierarchy also significant performance effect optimization improve intracluster cache coherence protocol increase bandwidth within cluster quite effective
1989,analytical cache model,trace-driven simulation hardware measurement technique often used obtain accurate performance figure cache former requires large amount simulation time evaluate cache configuration latter restricted measurement existing cache analytical cache model us parameter extracted address trace program efficiently provide estimate cache performance show effect varying cache parameter representing factor affect cache performance develop analytical model give miss rate given trace function cache size degree associativity block size subblock size multiprogramming level task switch interval observation interval predicted value closely approximate  trace-driven simulation requiring small fraction computation cost
1989,simple interprocedural register allocation algorithm effectiveness lisp,register allocation important optimization many compiler per-procedure register allocation often possible make good use large register set procedure call limit improvement global register allocation since force variable allocated register saved restored limitation pronounced lisp program due higher frequency procedure call interprocedural register allocation algorithm developed simplifying version interprocedural graph coloring simplification corresponds bottom-up coloring interference graph scheme evaluated using number lisp program evaluation considers scheme limitation compare software register window hardware register window used berkeley risc spur processor
1989,characteristic performance-optimal multi-level cache hierarchy,increasing speed new generation processor exacerbate already large difference cpu cycle time main memory access time difference grows increasingly difficult build single-level cache fast enough match fast cycle time large enough effectively hide slow main memory access time one solution problem use multi-level cache hierarchy paper examines relationship cache organization program execution time multi-level cache show first-level cache dramatically reduces number reference seen second-level cache without large effect number second-level cache miss reduction number second-level cache hit change optimal design point decreasing importance cycle-time second-level cache relative size lower first-level cache miss rate le important second-level cycle time becomes change relative importance cycle time miss rate make associativity attractive increase optimal cache size second-level cache would equivalent single-level cache system
1989,copy elimination functional language,copy elimination important optimization compiling functional language copy arise language lack concept state variable hence updating object involves copy naive implementation copy also possible proper targeting carried inside function across function call targeting proper selection storage area evaluating expression abstracting collection function target operator compute target function body used define optimized interpreter eliminate copy due update copy across function call language consider typed lambda calculus higher-order function special construct array operation approach eliminate copy divide conquer problem like quicksort bitonic sort previous approach could handlewe also present  implementing compiler single assignment language called sal small tough program  indicate possible approach performance comparable imperative language like pascal
1988,lisp reduced-instruction-set processor characterization optimization,factor motivated choice reduced-instruction-set computer  implement lisp examined dynamic profiling measurement used characterise lisp reported implementation tag lisp cost function call discussed interprocedural register allocation examined execution  various benchmark presented discussed< >
1988,measurement evaluation mips architecture processor,mips -bit processor architecture implemented nmos vlsi chip instruction set architecture risc-based close coupling compiler efficient use instruction set compiled program goal architecture mips architecture requires software implement constraint design normally considered part hardware implementation paper present experimental  effectiveness processor program host using set large small benchmark instruction operand usage pattern examined optimized unoptimized code several architectural organizational innovation mips including software pipeline scheduling multiple-operation instruction word-based addressing examined light data
1988,cache performance operating system multiprogramming workload,large cache necessary current high-performance computer system provide required high memory bandwidth small decrease cache performance result significant system performance degradation accurately characterizing performance large cache important although measurement actual system shown operating system multiprogramming affect cache performance previous study focused effect developed program tracing technique called atum  capture realistic trace multitasking workload including operating system examining cache behavior using trace vax processor show operating system multiprogramming activity significantly degrade cache performance even greater proportional impact large cache careful analysis cause degradation explore various technique reduce loss seemingly little done mitigate effect system reference multitasking cache miss activity substantially reduced small hardware addition
1988,performance tradeoff cache design,series simulation explore interaction various organizational decision program execution time presented tradeoff cache size cpu/cache cycle-time set associativity cycle time block size main-memory speed investigated  indicate neither cycle time cache size dominates across entire design space common implementation technology performance maximized size increased size increased -kb -kb range modest penalty cycle time set associativity impact cycle time nanosecond increase overall execution time since block size memory-transfer rate combine affect cache miss penalty optimum block size substantially smaller minimizes miss rate interdependence optimal cache configuration main memory speed necessitates multilevel cache hierarchy high-performance uniprocessors< >
1988,simple efficient implementation approach single assignment language,functional single assignment language semantically pure feature permit side effect lack side effect make detection parallelism program much easier however property pose challenge implementing language efficiently preliminary implementation compiler single assignment language sisal described compiler us reference count memory management copy avoidance performance  wide range benchmark program show sisal program compiled implementation run - time slower average program written c pascal fortran order magnitude faster implementation single assignment language extension technique multiprocessor implementation proposed
1988,characterizing synchronization behavior parallel program,contention synchronization lock delay waiting synchronization event substantially increase running time parallel program make important characterize synchronization behavior program provide analysis tool aid hardware software designer evaluating design alternative paper describes tracing facility incorporated synchronization package facility provides portable mean accurately efficiently characterize parallel program behavior several application monitored uncovering program characteristic make difficult achieve linear speedup monitoring facility allows programmer determine performance implication synchronization structure used allows architect evaluate various hardware support mechanism
1987,tag type checking lisp hardware software approach,one major factor distinguishes lisp many language  need run-time type checking run-time type checking implemented adding data object tag encodes type information tag must compared type compatibility removed using data inserted new data item created tag manipulation together work related dynamic type checking generic operation constitutes significant component execution time lisp program led development lisp machine support tag checking hardware avoidance type checking user running stock hardware understand role necessity special-purpose hardware tag handling first measure cost type checking operation group lisp program examine hardware software implementation tag operation estimate cost tag handling different tag implementation scheme data show minimal level support provide benefit tag operation relatively inexpensive even special hardware support present
1986,reducing cost branch,pipelining major organizational technique computer use reach higher single-processor performance fundamental disadvantage pipelining loss incurred due branch require stalling flushing pipeline hardware solution architectural change proposed overcome problem paper examines range scheme reducing branch cost focusing static  dynamic  prediction branch scheme investigated quantitative performance implementation viewpoint
1986,lisp reduced-instruction-set-processor,factor motivated choice reduced-instruction-set computer  implement lisp examined dynamic profiling measurement used characterise lisp reported implementation tag lisp cost function call discussed interprocedural register allocation examined execution  various benchmark presented discussed< >
1986,partitioning parallel program macro-dataflow,partitioning technique necessary execute functional program coarse granularity fine-granularity execution inefficient general-purpose multiprocessor trade-off parallelism overhead exploiting parallelism compile-time partitioning approach achieve trade-off presented 
1986,compile-time partitioning scheduling parallel program,partitioning scheduling technique necessary implement parallel language multiprocessor multiprocessor performance maximized parallelism task optimally traded communication synchronization overhead present compile-time partitioning scheduling technique achieve trade-off
1985,swami flexible logic implementation system,new system logic synthesis vlsi layout stanford weinberger array minimizer implementor  developed paper describes system algorithm present preliminary  minimization logic expression us local sum-of-products minimization kernel factorization common subexpression recognition generates improved expression arbitrary depth logic expression realized nmos technology using one dimensional weinberger array structure new extension weinberger array two dimension placement phase us heuristic technique including simulated annealing min-cut linear arrangement local constructive clustering
1984,vlsi processor architecture,processor architecture attempt compromise need program hosted architecture performance attainable implementing architecture need program accurately reflected dynamic use instruction set target high level language compiler vlsi issue implementation instruction set architecture significant determining feature architecture recent processor architecture focused two major trend large microcoded instruction set simplified reduced instruction set attractiveness two approach affected choice single-chip implementation two different style require different tradeoff attain implementation silicon reasonable area two style consume chip area different purpose thus achieving performance different strategy vlsi implementation architecture many problem arise base technology limitation although circuit design technique help alleviate many problem architect must aware limitation understand implication instruction set level
1984,register allocation priority-based coloring,classic problem global register allocation treated heuristic practical manner adopting notion priority node-coloring assignment priority based estimate benefit derived allocating individual quantity register using priority exponential coloring process made run linear time since cost involved register allocation taken account algorithm over-allocate algorithm parameterized cater different fetch characteristic register configuration among machine measurement indicate register allocation scheme effective number target machine  confirm using priority-based coloring global register allocation performed practically efficiently
1983,postpass code optimization pipeline constraint,pipeline interlock used pipelined architecture prevent execution machine instruction operand available alternative complex piece hardware rearrange instruction compile time avoid pipeline interlock problem called code reorganization studied basic problem reorganization machine-level instruction compile time shown np-complete heuristic algorithm proposed property effectiveness explored empirical data mips vlsi processor design given impact code reorganization technique rest compiler system discussed  reference
1982,retargetable compiler code generation,classification automated retargetable code generation technique survey work technique presented retargetable code generation research classified three category interpretive code generation pattern-matched code generation table-driven code generation interpretive code generation approach generate code virtual machine expand real target code pattern-matched code generation approach separate machine description code generation algorithm table-driven code generation approach employ formal machine description use code-generator generator produce code generator automatically analysis technique critique automatic code generation algorithm presented
1982,design implementation parametric type pascal,parametric type offer attractive solution problem dealing array pascal problem arise use strong static type checking especially arraytype procedure argument parametric type provide solution array procedure argument problem allow consistent inclusion array dynamic bound parametric type mechanism proposed design issue discussed inclusion parametric type major effect implementation language like pascal implementation issue implementation versus design tradeoff examined implementation strategy used extended accommodate standard generic type ada
1982,compilation pascal case statement,pascal case statement compiled using variety method including comparison tree branch table scheme discussed combine two technique allow comparison tree entry branch table use combination two technique shown adapt well certain instance case statement extension standard case statement also require scheme obtain efficient implementation
1982,symbolic debugging optimized code,long standing conflict optimization code ability symbolically debug code examined effect local global optimization variable program categorized model representing effect optimization given model used algorithm determine subset variable whose value correspond original program algorithm restoring variable correct value also developed empirical  application algorithm local optimization presented
1982,hardware/software tradeoff increased performance,new computer architecture concerned maximizing performance providing suitable instruction set compiled code providing support system function argue effective design methodology must make simultaneous tradeoff across three area hardware software support system support recent trend lean towards extensive hardware support compiler operating system software however consideration possible design tradeoff may often lead le hardware support several example approach presented including omission condition code word-addressed machine imposing pipeline interlock software specific performance approach examined respect mips processor
1982,mips machine,n/a
1982,mips microprocessor architecture,mips new single chip vlsi microprocessor attempt achieve high performance use simplified instruction set similar found microengines processor fast pipelined engine without pipeline interlock software solution several traditional hardware problem providing pipeline interlock used
1982,optimizing delayed branch,delayed branch commonly found micro-architectures compiler assembler exploit delayed branch achieved moving code one several point position following branch instruction present several strategy moving code utilize branch delay discus requirement benefit strategy algorithm processing branch delay implemented give empirical  performance data show reasonable percentage delay avoided
1982,code generation reorganization presence pipeline constraint,pipeline interlock used pipelined architecture prevent execution machine instruction operand available alternative complex piece hardware rearrange instruction compile-time avoid pipeline interlock problem called code reorganization studied basic problem reorganization machine level instruction compile-time shown np-complete heuristic algorithm proposed property effectiveness explored impact code reorganization technique rest compiler system discussed
1981,formal definition real-time language,paper present formal definition tomal  programming language intended real-time system running small processor formal definition address aspect language mode semantic definition seem particularly well-suited certain aspect language suitable others formal definition employ several complementary mode definitionthe primary definition axiomatic employed define statement language simple denotational  semantics complement axiomatic semantics define type-related feature binding name type data type coercion evaluation expression together axiomatic denotational semantics define feature sequential language operational definition used define real-time execution extend axiomatic definition account aspect concurrent execution semantic constraint sufficient guarantee conformity program axiomatic definition checked analysis tomal program compilation
1981,program optimization exception handling,optimization program may contain exception handling facility requires new technique program optimization account exception handling facility may incorrectly transform procedure raise exception producing different  unoptimized version restricted exception handling mechanism allow optimization discussed data flow equation determining effect exception handling presented
1981,wsclock - simple effective algorithm virtual memory management,new virtual memory management algorithm wsclock synthesized local working set  algorithm global clock algorithm new load control mechanism auxiliary memory access new algorithm combine useful feature wsa natural effective load control prevents thrashingwith simplicity efficiency clock study presented show performance w wsclock equivalent even saving overhead ignored
1980,parallelism representation problem distributed system,hierarchical view program representation used explain problem matching various representation underlying distributed architecture program effectively use distributed computer system necessary represent detect high degree parallelism method detecting parallelism limitation discussed actual machine level representation high-level language program also affect ability achieve good match computer system resource program concept ideal machine program lead naturally representation employing directly executed language initial program representation profoundly influence possibility obtaining good representation level hierarchy poor initial language representation lead unnecessary architectural contraints insufficient information efficiently execute program issue suitable initial representation distributed hardware approached employing functional language basis
1980,interactive graphic system custom design,interactive graphic system/  one series highly interactive program  used extensively within ibm design multiplanar chip macro module card board program developed ibms internal use marketed ibmthis paper describes hardware system software environment design function capacity performance igs/the geometric description associated attribute defining lsi device entered beam-directed refresh cathode ray tube digitizer graphic interface language many capability exist creation modification phase design data library item called cell consisting rectangle line polygon circle alphanumerics attribute reference cell created edited manipulated final design final design output graphic interface language become source data checking post-processing program result manufactured product
