2019,equivalence equilibrium propagation recurrent backpropagation,recurrent backpropagation equilibrium propagation supervised learning algorithm fixed-point recurrent neural network differ second phase first phase algorithm converge fixed point corresponds configuration prediction made second phase equilibrium propagation relaxes another nearby fixed point corresponding smaller prediction error whereas recurrent backpropagation us side network compute error derivative iteratively work establish close connection two algorithm show every moment second phase temporal derivative neural activity equilibrium propagation equal error derivative computed iteratively recurrent backpropagation side network work show required side network computation error derivative support hypothesis biological neural network temporal derivative neural activity may code error signal
2019,gated orthogonal recurrent unit learning forget,present novel recurrent neural network based model combine remembering ability unitary evolution rnns ability gated rnns effectively forget redundant irrelevant information memory achieve extending restricted orthogonal evolution rnns gating mechanism similar gated recurrent unit rnns reset gate update gate model able outperform long short-term memory gated recurrent unit vanilla unitary orthogonal rnns several long-term-dependency benchmark task empirically show orthogonal unitary rnns lack ability forget ability play important role rnns provide competitive  along analysis model many natural sequential task including question answering speech spectrum prediction character-level language modeling synthetic task involve long-term dependency algorithmic denoising copying task
2019,depth nonlinearity creates bad local minimum resnets,paper prove depth nonlinearity creates bad local minimum type arbitrarily deep resnets arbitrary nonlinear activation function sense value local minimum worse global minimum value corresponding classical machine-learning model guaranteed improve via residual representation result paper provides affirmative answer open question stated paper conference neural information processing system  paper advance optimization theory deep learning resnets network architecture
2019,towards non-saturating recurrent unit modelling long-term dependency,modelling long-term dependency challenge recurrent neural network primarily due fact gradient vanish training sequence length increase gradient attenuated transition operator attenuated dropped activation function canonical architecture like lstm alleviate issue skipping information memory mechanism propose new recurrent architecture  relies memory mechanism forgoes saturating activation function saturating gate order alleviate vanishing gradient series synthetic real world task demonstrate proposed model model performs among top  model across task without long-term dependency compared range architecture
2019,combined reinforcement learning via abstract representation,quest efficient robust reinforcement learning method model-free model-based approach offer advantage paper propose new way explicitly bridging approach via shared low-dimensional learned encoding environment meant capture summarizing abstraction show modularity brought approach lead good generalization computationally efficient planning happening smaller latent state space addition approach recovers sufficient low-dimensional representation environment open new strategy interpretable ai exploration transfer learning
2019,neural dialog system use conversation history effectively empirical study,neural generative model become increasingly popular building conversational agent offer flexibility easily adapted new domain require minimal domain engineering common criticism system seldom understand use available dialog history effectively paper take empirical approach understanding model use available dialog history studying sensitivity model artificially introduced unnatural change perturbation context test time experiment  different type perturbation  multi-turn dialog datasets find commonly used neural dialog architecture like recurrent transformer-based seqseq model rarely sensitive perturbation missing reordering utterance shuffling word etc also open-sourcing code believe serve useful diagnostic tool evaluating dialog system future
2019,interpolated adversarial training achieving robust neural network without sacrificing much accuracy,adversarial robustness become central goal deep learning theory practice however successful method improve adversarial robustness  greatly hurt generalization performance unperturbed data could major impact achieving adversarial robustness affect real world system  propose interpolated adversarial training employ recently proposed interpolation based training method framework adversarial training cifar- adversarial training increase standard test error    whereas interpolated adversarial training retain adversarial robustness achieving standard test error  technique relative increase standard error robust model reduced  
2019,interactive language learning question answering,human observe interact world acquire knowledge however existing machine reading comprehension  task miss interactive information-seeking component comprehension task present model static document contain necessary information usually concentrated single short substring thus model achieve strong performance simple word- phrase-based pattern matching address problem formulating novel text-based question answering task question answering interactive text  qait agent must interact partially observable text-based environment gather information required answer question qait pose question existence location attribute object found environment data built using text-based game generator defines underlying dynamic interaction environment propose evaluate set baseline model qait task includes deep reinforcement learning agent experiment show task present major challenge machine reading system human solve relative ease
2019,transferable feature convolutional neural network acoustic model across language,characterization representation learned intermediate layer deep network provide valuable insight nature task guide development well-tailored learning strategy study convolutional neural network -based acoustic model context automatic speech recognition adapting method proposed  measure transferability layer english dutch german ass language-specificity observed three distinct region transferability  first two layer entirely transferable language  layer - also highly transferable found evidence language specificity  subsequent fully connected layer language specific could successfully finetuned target language probe effect weight freezing performed follow-up experiment using freeze-training   consistent observation cnns converge `bottom training demonstrate benefit freeze training especially transfer learning
2019,highly adaptive acoustic model accurate multi-dialect speech recognition,despite success deep learning speech recognition multi-dialect speech recognition remains difficult problem although dialect-specific acoustic model known perform well general easy maintain dialect-specific data scarce number dialect language large therefore single unified acoustic model  generalizes well many dialect demand paper propose novel acoustic modeling technique accurate multi-dialect speech recognition single proposed dynamically adapted based dialect information internal representation  highly adaptive handling multiple dialect simultaneously also propose simple effective training method deal unseen dialect experimental  large scale speech datasets show proposed outperforms previous one reducing word error rate   relative compared single all-dialects  relative compared dialect-specific am
2019,representation mixing tt synthesis,recent character phoneme-based parametric tt system using deep learning shown strong performance natural speech generation however choice character phoneme input create serious limitation practical deployment direct control pronunciation crucial certain case demonstrate simple method combining multiple type linguistic information single encoder named representation mixing enabling flexible choice character phoneme mixed representation inference experiment user study public audiobook corpus show efficacy approach
2019,pytorch-kaldi speech recognition toolkit,availability open-source software playing remarkable role popularization speech recognition deep learning kaldi instance nowadays established framework used develop state-of-the-art speech recognizers pytorch used build neural network python language recently spawn tremendous interest within machine learning community thanks simplicity flexibility pytorch-kaldi project aim bridge gap popular toolkits trying inherit efficiency kaldi flexibility pytorch pytorch-kaldi simple interface software embeds several useful feature developing modern speech recognizers instance code specifically designed naturally plug-in user-defined acoustic model alternative user exploit several pre-implemented neural network customized using intuitive configuration file pytorch-kaldi support multiple feature label stream well combination neural network enabling use complex neural architecture toolkit publicly-released along rich documentation designed properly work locally hpc cluster experiment conducted several datasets task show pytorch-kaldi effectively used develop modern state-of-the-art speech recognizers
2019,babyai platform study sample efficiency grounded language learning,allowing human interactively train artificial agent understand language instruction desirable practical scientific reason  though given  lack sample efficiency current learning method reaching goal may require substantial research effort introduce babyai research platform goal supporting investigation towards including human loop grounded language learning babyai platform comprises extensible suite  level increasing difficulty level gradually lead agent towards acquiring combinatorially rich synthetic language proper subset english platform also provides hand-crafted bot agent simulates human teacher  report estimated amount supervision required training neural reinforcement behavioral-cloning agent babyai level put forward strong evidence current deep learning method yet sufficiently sample-efficient context learning language compositional property
2019,adversarial domain adaptation stable brain-machine interface,brain-machine interface  recently emerged clinically viable optionto restore voluntary movement paralysis device based theability extract information movement intent neural signal recordedusing multi-electrode array chronically implanted motor cortex thebrain however inherent loss turnover recorded neuron requires repeatedrecalibrations interface potentially alter day-to-dayuser experience resulting need continued user adaptation interferes withthe natural subconscious use bmi introduce new computationalapproach decodes movement intent low-dimensional latent representationof neural data implement various domain adaptation methodsto stabilize interface significantly long time includes canonicalcorrelation analysis used align latent variable across day methodrequires prior point-to-point correspondence time series across domainsalternatively match empirical probability distribution latent variablesacross day minimization kullback-leibler divergencethese two method provide significant comparable improvement performanceof interface however implementation adversarial domainadaptation network trained match empirical probability distribution theresiduals reconstructed neural signal outperforms two method basedon latent variable requiring remarkably data point solve domainadaptation problem
2019,recall trace backtracking model efficient reinforcement learning,many environment tiny subset state yield high reward  case interaction environment provide relevant learning signal hence may want preferentially train high-reward state probable trajectory leading end advocate use {backtracking model} predicts preceding state terminate given high-reward state  train model starting high value state  predicts sample -tuples may led high value state trace  pair refer recall trace sampled backtracking model starting high value state informative terminate good state hence use trace improve policy provide variational interpretation idea practical algorithm backtracking model sample approximate posterior distribution trajectory lead large reward method improves sample efficiency on- off-policy rl algorithm across several environment task  
2019,infobot transfer exploration via information bottleneck,central challenge reinforcement learning discovering effective policy task reward sparsely distributed postulate absence useful reward signal effective exploration strategy seek { decision states} state lie critical junction state space agent transition new potentially unexplored region propose learn decision state prior experience training goal-conditioned model information bottleneck identify decision state examining model access goal state bottleneck find simple mechanism effectively identifies decision state even partially observed setting effect model learns sensory cue correlate potential subgoals new environment model identify novel subgoals exploration guiding agent sequence potential decision  state new region state space
2019,learning deep representation mutual information estimation maximization,work investigates unsupervised learning representation maximizing mutual information input output deep neural network encoder importantly show structure matter incorporating knowledge locality input objective significantly improve representation suitability downstream task control characteristic representation matching prior distribution adversarially method call deep infomax  outperforms number popular unsupervised learning method compare favorably fully-supervised learning several classification task standard architecture dim open new avenue unsupervised learning representation important step towards flexible formulation representation learning objective specific end-goals
2019,relation sharpest direction dnn loss sgd step length,training deep neural network stochastic gradient descent  large learning rate small batch-size typically end flat region weight space indicated small eigenvalue hessian training loss found correlate good final generalization performance  paper extend previous work investigating curvature loss surface along whole training trajectory rather endpoint find initially sgd visit increasingly sharp region reaching maximum sharpness determined learning rate batch-size sgd peak value sgd start fail minimize loss along direction loss surface corresponding largest curvature  investigate effect dynamic training process study variant sgd using reduced learning rate along sharpest direction show improve training speed finding sharper better generalizing solution compared vanilla sgd overall  show sgd dynamic subspace sharpest direction influence region sgd steer  overall training speed generalization ability final model
2019,h-detach modifying lstm gradient towards better optimization,recurrent neural network known notorious exploding vanishing gradient problem  problem becomes evident task information needed correctly solve exist long time scale evgp prevents important gradient component back-propagated adequately large number step introduce simple stochastic algorithm  specific lstm optimization targeted towards addressing problem specifically show lstm weight large gradient component linear path  lstm computational graph get suppressed based hypothesis component carry information long term dependency  suppression prevent lstms capturing algorithm{our code available https//githubcom/bhargav/h-detach} prevents gradient flowing path getting suppressed thus allowing lstm capture dependency better show significant improvement vanilla lstm gradient based training term convergence speed robustness seed learning rate generalization using modification lstm gradient various benchmark datasets
2019,modeling long term future model-based reinforcement learning,model-based reinforcement learning agent interleaf model learning planning  two component  inextricably intertwined model able provide sensible long-term prediction executed planer would exploit model flaw yield catastrophic failure paper focus building model reason long-term future demonstrates use efficient planning exploration end build latent-variable autoregressive model leveraging recent idea variational inference argue forcing latent variable carry future information auxiliary task substantially improves long-term prediction moreover planning latent space planner solution ensured within region model valid exploration strategy devised searching unlikely trajectory model method achieves higher reward faster compared baseline variety task environment imitation learning model-based reinforcement learning setting 
2019,quaternion recurrent neural network,recurrent neural network  powerful architecture model sequential data due capability learn short long-term dependency basic element sequence nonetheless popular task speech image recognition involve multi-dimensional input feature characterized strong internal dependency dimension input vector propose novel quaternion recurrent neural network  alongside quaternion long-short term memory neural network  take account external relation internal structural dependency quaternion algebra similarly capsule quaternion allow qrnn code internal dependency composing processing multidimensional feature single entity recurrent operation reveals correlation element composing sequence show qrnn qlstm achieve better performance rnn lstm realistic application automatic speech recognition finally show qrnn qlstm reduce maximum factor x number free parameter needed compared real-valued rnns lstms reach better  leading compact representation relevant information
2019,probabilistic planning sequential monte carlo method,work propose novel formulation planning view probabilistic inference problem future optimal trajectory enables u use sampling method thus tackle planning continuous domain using fixed computational budget   design new algorithm  sequential monte carlo planning leveraging classical method sequential monte carlo bayesian smoothing context control inference furthermore show sequential monte carlo planning capture multimodal policy quickly learn continuous control task
2019,empirical study example forgetting deep neural network learning,inspired phenomenon catastrophic forgetting investigate learning dynamic neural network train single classification task goal understand whether related phenomenon occurs data undergo clear distributional shift define ``forgetting event occurred individual training example transition classified correctly incorrectly course learning across several benchmark data set find  certain example forgotten high frequency  data set forgettable example generalize across neural architecture  based forgetting dynamic significant fraction example omitted training data set still maintaining state-of-the-art generalization performance
2019,deep graph infomax,present deep graph infomax  general approach learning node representation within graph-structured data unsupervised manner dgi relies maximizing mutual information patch representation corresponding high-level summary graphs---both derived using established graph convolutional network architecture learnt patch representation summarize subgraphs centered around node interest thus reused downstream node-wise learning task contrast prior approach unsupervised learning gcns dgi rely random walk objective readily applicable transductive inductive learning setup demonstrate competitive performance variety node classification benchmark time even exceeds performance supervised learning
2019,perceptual generative autoencoders,framework training autoencoder-based generative model non-adversarial loss unrestricted neural network architecture
2019,state-reification network improving generalization modeling distribution hidden representation,machine learning promise method generalize well finite labeled data however brittleness existing neural net approach revealed notable failure existence adversarial example misclassified despite nearly identical training example inability recurrent sequence-processing net stay track without teacher forcing introduce method refer _state reification_ involves modeling distribution hidden state training data projecting hidden state observed testing toward distribution intuition network remain familiar manifold hidden space subsequent layer net well trained respond appropriately show state-reification method help neural net generalize better especially labeled data sparse also help overcome challenge achieving robust generalization adversarial training
2019,gmnn graph markov neural network,paper study semi-supervised object classification relational data fundamental problem relational data modeling problem extensively studied literature statistical relational learning  graph neural network  statistical relational learning method effectively model dependency object label conditional random field collective classification whereas graph neural network learn effective object representation classification end-to-end training paper propose graph markov neural network  combine advantage world gmnn model joint distribution object label conditional random field effectively trained variational em algorithm e-step one graph neural network learns effective object representation approximating posterior distribution object label m-step another graph neural network used model local label dependency experiment object classification link classification unsupervised node representation learning show gmnn achieves state-of-the-art 
2019,spectral bias neural network,neural network known class highly expressive function able fit even random input-output mapping  accuracy work present property neural network complement aspect expressivity using tool fourier analysis highlight learning bias deep network towards low frequency function – ie function vary globally without local fluctuation – manifest frequency-dependent learning speed intuitively property line observation over-parameterized network prioritize learning simple pattern generalize across data sample also investigate role shape data manifold presenting empirical theoretical evidence somewhat counter-intuitively learning higher frequency get easier increasing manifold complexity
2019,manifold mixup better representation interpolating hidden state,deep neural network excel learning training data often provide incorrect confident prediction evaluated slightly different test example includes distribution shift outlier adversarial example address issue propose {} simple regularizer encourages neural network predict le confidently interpolation hidden representation {} leverage semantic interpolation additional training signal obtaining neural network smoother decision boundary multiple level representation result neural network trained {} learn flatter class-representations fewer direction variance prove theory flattening happens ideal condition validate empirically practical situation connect previous work information theory generalization spite incurring significant computation implemented line code {} improves strong baseline supervised learning robustness single-step adversarial attack test log-likelihood
2019,data-efficient framework training sim-to-real transfer navigation policy,learning effective visuomotor policy robot purely data challenging also appealing since learning-based system require manual tuning calibration case robot operating real environment training process costly time-consuming even dangerous since failure common start training reason desirable able leverage simulation off-policy data extent possible train robot work introduce robust framework plan simulation transfer well real environment model incorporates gradient-descent based planning module given initial image goal image encodes image lower dimensional latent state plan trajectory reach goal model consisting encoder planner module first trained meta-learning strategy simulation subsequently perform adversarial domain transfer encoder using bank unlabelled random image simulation real environment enable encoder map image real simulated environment similarly distributed latent representation fine tuning entire model  real world expert demonstration show successful planning performance different navigation task
2019,interpolation consistency training semi-supervised learning,introduce interpolation consistency training  simple computation efficient algorithm training deep neural network semi-supervised learning paradigm ict encourages prediction interpolation unlabeled point consistent interpolation prediction point classification problem ict move decision boundary low-density region data distribution experiment show ict achieves state-of-the-art performance applied standard neural network architecture cifar- svhn benchmark dataset
2019,infomask masked variational latent representation localize chest disease,scarcity richly annotated medical image limiting supervised deep learning based solution medical image analysis task localizing discriminatory radiomic disease signature therefore desirable leverage unsupervised weakly supervised model recent weakly supervised localization method apply attention map region proposal multiple instance learning formulation attention map noisy leading erroneously highlighted region simple decide optimal window/bag size multiple instance learning approach paper propose learned spatial masking mechanism filter irrelevant  signal attention map proposed method minimizes mutual information masked variational representation input maximizing information masked representation class label  accurate localization discriminatory region tested proposed model chestx-ray dataset localize pneumonia chest x-ray image without using pixel-level bounding-box annotation
2018,deep convolutional network quality assessment protein fold,motivationthe computational prediction protein structure sequence generally relies method ass quality protein model assessment method rank candidate model using heavily engineered structural feature defined complex function atomic coordinate however method attempted learn feature directly datawe show deep convolutional network used predict ranking model structure solely basis raw three-dimensional atomic density without feature tuning develop deep neural network performs par state-of-the-art algorithm literature network trained decoy casp casp datasets performance tested casp dataset additional testing decoy casp cameo drobot datasets confirms network performs consistently well across variety protein structure network learns ass structural decoy globally rely predefined feature analyzed show implicitly identifies region deviate native structure
2018,fine-grained attention mechanism neural machine translation,neural machine translation  new paradigm machine translation attention mechanism become dominant approach state-of-the-art record many language pair variant attention mechanism use temporal attention one scalar value assigned one context vector corresponding source word paper propose fine-grained  attention mechanism dimension context vector receive separate attention score experiment task en-de en-fi translation fine-grained attention method improves translation quality term bleu score addition alignment analysis reveals fine-grained attention mechanism exploit internal structure context vector
2018,learning normalized input iterative estimation medical image segmentation,abstractin paper introduce simple yet powerful pipeline medical image segmentation combine fully convolutional network  fully convolutional residual network  propose examine design take particular advantage recent advance understanding convolutional neural network well resnets approach focus upon importance trainable pre-processing using fc-resnets show low-capacity fcn model serve pre-processor normalize medical input data image segmentation pipeline use fcns obtain normalized image iteratively refined mean fc-resnet generate segmentation prediction fully convolutional approach pipeline used off-the-shelf different image modality show using pipeline exhibit state-of-the-art performance challenging electron microscopy benchmark compared method improve segmentation  ct image liver lesion contrasting standard fcn method moreover applying pipeline challenging mri prostate segmentation challenge reach  competitive even compared method obtained  illustrate strong potential versatility pipeline achieving accurate segmentation variety image modality different anatomical regionsgraphical abstractdownload  download high-res image download  download full-size image
2018,dynamic neural turing machine continuous discrete addressing scheme,extend neural turing machine  model dynamic neural turing machine  introducing trainable address vector addressing scheme maintains memory cell two separate vector content address vector allows d-ntm learn wide variety location-based addressing strategy including linear nonlinear one implement d-ntm continuous discrete read write mechanism investigate mechanism effect learning read write memory experiment facebook babi task using feedforward gru controller provide extensive analysis model compare different variation neural turing machine task show model outperforms long short-term memory ntm variant provide experimental  sequential mnist stanford natural language inference associative recall copy task
2018,drawing recognizing chinese character recurrent neural network,recent deep learning based approach achieved great success handwriting recognition chinese character among widely adopted writing system world previous research mainly focused recognizing handwritten chinese character however recognition one aspect understanding language another challenging interesting task teach machine automatically write  chinese character paper propose framework using recurrent neural network  discriminative model recognizing chinese character generative model drawing  chinese character recognize chinese character previous method usually adopt convolutional neural network  model require transforming online handwriting trajectory image-like representation instead rnn based approach end-to-end system directly deal sequential structure require domain-specific knowledge rnn system  state-of-the-art performance achieved icdar- competition database furthermore rnn framework conditional generative model character embedding proposed automatically drawing recognizable chinese character generated character  human-readable also recognized discriminative rnn model high accuracy experimental  verify effectiveness using rnns generative discriminative model task drawing recognizing chinese character
2018,light gated recurrent unit speech recognition,field directly benefited recent advance deep learning automatic speech recognition  despite great achievement past decade however natural robust human-machine speech interaction still appears reach especially challenging environment characterized significant noise reverberation improve robustness modern speech recognizers often employ acoustic model based recurrent neural network  naturally able exploit large time context long-term speech modulation thus great interest continue study proper technique improving effectiveness rnns processing speech signal paper revise one popular rnn model namely gated recurrent unit  propose simplified architecture turned effective asr contribution work twofold first analyze role played reset gate showing significant redundancy update gate occurs result propose remove former gru design leading efficient compact single-gate model second propose replace hyperbolic tangent rectified linear unit activation variation couple well batch normalization could help model learn long-term dependency without numerical issue  show proposed architecture called light gru reduces per-epoch training time  standard gru also consistently improves recognition accuracy across different task input feature noisy condition well across different asr paradigm ranging standard dnn-hmm speech recognizers end-to-end connectionist temporal classification model
2018,neural model key phrase extraction question generation,propose two-stage neural model tackle question generation document first model estimate probability word sequence document one human would pick selecting candidate answer training neural key-phrase extractor answer question-answering corpus predicted key phrase act target answer condition sequence-to-sequence question-generation model copy mechanism empirically key-phrase extraction model significantly outperforms entity-tagging baseline existing rule-based approach demonstrate question generation system formulates fluent answerable question key phrase two-stage system could used augment generate reading comprehension datasets may leveraged improve machine reading system educational setting
2018,straight tree constituency parsing neural syntactic distance,work propose novel constituency parsing scheme model first predicts real-valued scalar named syntactic distance split position sentence topology grammar tree determined value syntactic distance compared traditional shift-reduce parsing scheme approach free potentially disastrous compounding error also easier parallelize much faster model achieves state-of-the-art single model f score  ptb  ctb dataset surpasses previous single model  large margin
2018,iterative refinement densely connected representation level semantic segmentation,state-of-the-art semantic segmentation approach increase receptive field model using either downsampling path composed poolings/strided convolution successive dilated convolution however clear operation lead best  paper systematically study difference introduced distinct receptive field enlargement method impact performance novel architecture called fully convolutional denseresnet  fc-drn densely connected backbone composed residual network following standard image segmentation architecture receptive field enlargement operation change representation level interleaved among residual network allows model exploit benefit residual dense connectivity pattern namely gradient flow iterative refinement representation multi-scale feature combination deep supervision order highlight potential model test challenging camvid urban scene understanding benchmark make following observation  downsampling operation outperform dilation model trained scratch  dilation useful finetuning step model  coarser representation require le refinement step  resnets  good regularizers since reduce model capacity needed finally compare architecture alternative method report state-of-the-art result camvid dataset least twice fewer parameter
2018,hotpotqa dataset diverse explainable multi-hop question answering,existing question answering  datasets fail train qa system perform complex reasoning provide explanation answer introduce hotpotqa new dataset k wikipedia-based question-answer pair four key feature  question require finding reasoning multiple supporting document answer  question diverse constrained pre-existing knowledge base knowledge schema  provide sentence-level supporting fact required reasoning allowing qa system reason strong supervision explain prediction  offer new type factoid comparison question test qa systems’ ability extract relevant fact perform necessary comparison show hotpotqa challenging latest qa system supporting fact enable model improve performance make explainable prediction
2018,width minimum reached stochastic gradient descent influenced learning rate batch size ratio,show dynamic convergence property sgd set ratio learning rate batch size observe ratio key determinant generalization error suggest mediated controlling width final minimum found sgd verify analysis experimentally range deep neural network datasets
2018,monaural singing voice separation skip-filtering connection recurrent inference time-frequency mask,singing voice separation based deep learning relies usage time-frequency masking many case masking process learnable function encapsulated deep learning optimization consequently existing method rely post processing step using generalized wiener filtering work proposes method learns optimizes  source-dependent mask need aforementioned post processing step introduce recurrent inference algorithm sparse transformation step improve mask generation process learned denoising filter obtained  show increase  db signal distortion ratio  db signal interference ratio compared previous state-of-the-art approach monaural singing voice separation
2018,dynamic frame skipping fast speech recognition recurrent neural network based acoustic model,recurrent neural network powerful tool modeling sequential data text speech recurrent neural network achieved record-breaking  speech recognition one remaining challenge slow processing speed main cause come nature recurrent neural network read one frame time step therefore reducing number read effective approach reducing processing time paper propose novel recurrent neural network architecture called skip-rnn dynamically skip speech frame le important skip-rnn consists acoustic model network skip-policy network jointly trained classify speech frame determine many frame skip evaluate proposed approach wall street journal corpus show accelerate acoustic model computation  time without noticeable degradation transcription accuracy
2018,towards end-to-end spoken language understanding,spoken language understanding system traditionally designed pipeline number component first audio signal processed automatic speech recognizer transcription n-best hypothesis recognition  natural language understanding system classifies text structured data domain intent slot down-streaming consumer dialog system hands-free application component usually developed optimized independently paper present study end-to-end learning system spoken language understanding unified approach infer semantic meaning directly audio feature without intermediate text representation study showed trained model achieve reasonable good result demonstrated model capture semantic attention directly audio feature
2018,boundary seeking gans,generative adversarial network learning framework rely training discriminator estimate measure difference target generated distribution gans normally formulated rely generated sample completely differentiable wrt generative parameter thus work discrete data introduce method training gans discrete data us estimated difference measure discriminator compute importance weight generated sample thus providing policy gradient training generator importance weight strong connection decision boundary discriminator call method boundary-seeking gans  demonstrate effectiveness proposed algorithm discrete image character-based natural language generation  addition boundary-seeking objective extends continuous data used improve stability training demonstrate celeba large-scale scene understanding  bedroom imagenet without conditioning
2018,residual connection encourage iterative inference,residual network  become prominent architecture deep learning however comprehensive understanding resnets still topic ongoing research recent view argues resnets perform iterative refinement feature attempt expose property aspect end study resnets analytically empirically formalize notion iterative refinement resnets showing residual architecture naturally encourage feature move along negative gradient loss feedforward phase addition empirical analysis suggests resnets able perform representation learning iterative refinement general resnet block tends concentrate representation learning behavior first layer higher layer perform iterative refinement feature finally observe sharing residual layer naively lead representation explosion hurt generalization performance show simple existing strategy help alleviating problem
2018,finding flatter minimum sgd,discussed over-parameterized deep neural network  trained using stochastic gradient descent  smaller batch size generalize better compared trained larger batch size additionally model parameter found small batch size sgd tend flatter region extend empirical observation experimentally show large learning rate small batch size contribute towards sgd finding flatter minimum generalize well conversely find small learning rate large batch size lead sharper minimum correlate poor generalization dnns
2018,figureqa annotated figure dataset visual reasoning,introduce figureqa visual reasoning corpus one million question-answer pair grounded  image image synthetic scientific-style figure five class line plot dot-line plot vertical horizontal bar graph pie chart formulate reasoning task generating question  template question concern various relationship plot element examine characteristic like maximum minimum area-under-the-curve smoothness intersection resolve question often require reference multiple plot element synthesis information distributed spatially throughout figure facilitate training machine learning system corpus also includes side data used formulate auxiliary objective particular provide numerical data used generate figure well bounding-box annotation plot element study proposed visual reasoning task training several model including recently proposed relation network strong baseline preliminary  indicate task pose significant machine learning challenge envision figureqa first step towards developing model intuitively recognize pattern visual representation data
2018,universal successor representation transfer reinforcement learning,objective transfer reinforcement learning generalize set previous task unseen new task work focus transfer scenario dynamic among task goal differ although general value function  shown useful knowledge transfer learning universal value function challenging practice attack propose  use universal successor representation  represent transferable knowledge  usr approximator  trained interacting environment experiment show usr effectively applied new task agent initialized trained usra achieve goal considerably faster random initialization
2018,extending framework equilibrium propagation general dynamic,biological plausibility backpropagation algorithm long doubted neuroscientist two major reason neuron would need send two different type signal forward backward phase pair neuron would need communicate symmetric bidirectional connectionswe present simple two-phase learning procedure fixed point recurrent network address issuesin model neuron perform leaky integration synaptic weight updated local mechanismour learning method extends framework equilibrium propagation general dynamic relaxing requirement energy functionas consequence generalization algorithm compute true gradient objective functionbut rather approximates precision proven directly related degree symmetry feedforward feedback weightswe show experimentally intrinsic property system lead alignment feedforward feedback weight algorithm optimizes objective function
2018,twin network matching future sequence generation,propose simple technique encouraging generative rnns plan ahead train ``backward recurrent network generate given sequence reverse order encourage state forward model predict cotemporal state backward model backward network used training play role sampling inference hypothesize approach eas modeling long-term dependency implicitly forcing forward state hold information longer-term future  show empirically approach achieves  relative improvement speech recognition task achieves significant improvement coco caption generation task
2018,chatpainter improving text image generation using dialogue,synthesizing realistic image text description dataset like microsoft common object context  image contain several object challenging task prior work used text caption generate image however caption might informative enough capture entire image insufficient model able understand object image correspond word caption show adding dialogue describes scene lead significant improvement inception score quality generated image coco dataset
2018,learning general purpose distributed sentence representation via large scale multi-task learning,lot recent success natural language processing  driven distributed vector representation word trained large amount text unsupervised manner representation typically used general purpose feature word across range nlp problem however extending success learning representation sequence word sentence remains open problem recent work explored unsupervised well supervised learning technique different training objective learn general purpose fixed-length sentence representation work present simple effective multi-task learning framework sentence representation combine inductive bias diverse training objective single model train model several data source multiple training objective  million sentence extensive experiment demonstrate sharing single recurrent sentence encoder across weakly related task lead consistent improvement previous method present substantial improvement context transfer learning low-resource setting using learned general-purpose representation
2018,deep complex network,present vast majority building block technique architecture deep learning based real-valued operation representation however recent work recurrent neural network older fundamental theoretical analysis suggests complex number could richer representational capacity could also facilitate noise-robust memory retrieval mechanism despite attractive property potential opening entirely new neural architecture complex-valued deep neural network marginalized due absence building block required design model work provide key atomic component complex-valued deep neural network apply convolutional feed-forward network precisely rely complex convolution present algorithm complex batch-normalization complex weight initialization strategy complex-valued neural net use experiment end-to-end training scheme demonstrate complex-valued model competitive real-valued counterpart test deep complex model several computer vision task music transcription using musicnet dataset speech spectrum prediction using timit achieve state-of-the-art performance audio-related task
2018,graph attention network,present graph attention network  novel neural network architecture operate graph-structured data leveraging masked self-attentional layer address shortcoming prior method based graph convolution approximation stacking layer node able attend neighborhood feature enable  specifying different weight different node neighborhood without requiring kind computationally intensive matrix operation  depending knowing graph structure upfront way address several key challenge spectral-based graph neural network simultaneously make model readily applicable inductive well transductive problem gat model achieved matched state-of-the-art  across four established transductive inductive graph benchmark cora citeseer pubmed citation network datasets well protein-protein interaction dataset 
2018,fraternal dropout,recurrent neural network  important class architecture among neural network useful language modeling sequential prediction however optimizing rnns known harder compared feed-forward neural network number technique proposed literature address problem paper propose simple technique called fraternal dropout take advantage dropout achieve goal specifically propose train two identical copy rnn  different dropout mask minimizing difference  prediction way regularization encourages representation rnns invariant dropout mask thus robust show regularization term upper bounded expectation-linear dropout objective shown address gap due difference train inference phase dropout evaluate model achieve state-of-the-art  sequence modeling task two benchmark datasets - penn treebank wikitext- also show approach lead performance improvement significant margin image captioning  semi-supervised  task
2018,mutual information neural estimation,argue estimation mutual information high dimensional continuous random variable achieved gradient descent neural network present mutual information neural estimator  linearly scalable dimensionality well sample size trainable back-prop strongly consistent present handful application mine used minimize maximize mutual information apply mine improve adversarially trained generative model also use mine implement information bottleneck applying supervised classification  demonstrate substantial improvement flexibility performance setting
2018,focused hierarchical rnns conditional sequence processing,recurrent neural network  attention mechanism obtained state-of-the-art  many sequence processing task model use simple form encoder attention look entire sequence assigns weight token independently present mechanism focusing rnn encoders sequence modelling task allows attend key part input needed formulate using multi-layer conditional hierarchical sequence encoder read one token time make discrete decision whether token relevant context question asked discrete gating mechanism take context embedding current hidden state input control information flow layer train using policy gradient method evaluate method several type task different attribute first evaluate method synthetic task allow u evaluate model generalization ability probe behavior gate controlled setting evaluate approach large scale question answering task including challenging m marco searchqa task model show consistent improvement task prior work baseline also shown generalize significantly better synthetic task compared baseline
2018,mad twinnet masker-denoiser architecture twin network monaural sound source separation,monaural singing voice separation task focus prediction singing voice single channel music mixture signal current state art   monaural singing voice separation obtained deep learning based method work present novel recurrent neural approach learns long-term temporal pattern structure musical piece build upon recently proposed masker-denoiser  architecture enhance twin network technique regularize recurrent generative network using backward running copy network evaluate method using demixing secret dataset obtain increment signal-to-distortion ratio   db signal-to-interference ratio   db compared previous sota 
2018,quaternion convolutional neural network end-to-end automatic speech recognition,recently connectionist temporal classification  model coupled recurrent  convolutional neural network  made easier train speech recognition system end-to-end fashion however real-valued model time frame component mel-filter-bank energy cepstral coefficient obtained together first second order derivative processed individual element natural alternative process component composed entity propose group element form quaternion process quaternion using established quaternion algebra quaternion number quaternion neural network shown efficiency process multidimensional input entity encode internal dependency solve many task le learning parameter real-valued model paper proposes integrate multiple feature view quaternion-valued convolutional neural network  used sequence-to-sequence mapping ctc model promising  reported using simple qcnns phoneme recognition experiment timit corpus precisely qcnns obtain lower phoneme error rate  le learning parameter competing model based real-valued cnns
2018,twin regularization online speech recognition,online speech recognition crucial developing natural human-machine interface modality however significantly challenging off-line asr since real-time/low-latency constraint inevitably hinder use future information known helpful perform robust prediction popular solution mitigate issue consists feeding neural acoustic model context window gather future frame introduces latency depends number employed look-ahead feature paper explores different approach based estimating future rather waiting technique encourages hidden representation unidirectional recurrent network embed useful information future inspired recently proposed technique called twin network add regularization term force forward hidden state close possible cotemporal backward one computed twin neural network running backwards time experiment conducted number datasets recurrent architecture input feature acoustic condition shown effectiveness approach one important advantage method introduce additional computation test time compared standard unidirectional recurrent network
2018,image-to-image translation cross-domain disentanglement,deep image translation method recently shown excellent  outputting high-quality image covering multiple mode data distribution also increased interest disentangling internal representation learned deep method improve performance achieve finer control paper bridge two objective introduce concept cross-domain disentanglement aim separate internal representation three part shared part contains information domain exclusive part hand contain factor variation particular domain achieve bidirectional image translation based generative adversarial network cross-domain autoencoders novel network component model offer multiple advantage output diverse sample covering multiple mode distribution domain perform domain- specific image transfer interpolation cross-domain retrieval without need labeled data paired image compare model state-of-the-art multi-modal image translation achieve better  translation challenging datasets well cross-domain retrieval realistic datasets
2018,metagan adversarial approach few-shot learning,paper propose conceptually simple general framework called metagan few-shot learning problem state-of-the-art few-shot classification model integrated metagan principled straightforward way introducing adversarial generator conditioned task augment vanilla few-shot classification model ability discriminate real fake data argue gan-based approach help few-shot classifier learn sharper decision boundary could generalize better show metagan framework extend supervised few-shot learning model naturally cope unsupervised data different previous work semi-supervised few-shot learning algorithm deal semi-supervision sample-level task-level give theoretical justification strength metagan validate effectiveness metagan challenging few-shot image classification benchmark
2018,bayesian model-agnostic meta-learning,due inherent model uncertainty learning infer bayesian posterior few-shot dataset important step towards robust meta-learning paper propose novel bayesian model-agnostic meta-learning method proposed method combine efficient gradient-based meta-learning nonparametric variational inference principled probabilistic framework unlike previous method fast adaptation method capable learning complex uncertainty structure beyond simple gaussian approximation meta-update novel bayesian mechanism prevents meta-level overfitting remaining gradient-based method also first bayesian model-agnostic meta-learning method applicable various task including reinforcement learning experiment  show accuracy robustness proposed method sinusoidal regression image classification active learning reinforcement learning
2018,sparse attentive backtracking temporal credit assignment reminding,learning long-term dependency extended temporal sequence requires credit assignment event far back past common method training recurrent neural network back-propagation time  requires credit information propagated backwards every single step forward computation potentially thousand million time step becomes computationally expensive even infeasible used long sequence importantly biological brain unlikely perform detailed reverse replay long sequence internal state  however human often reminded past memory mental state associated current mental state consider hypothesis memory association past present could used credit assignment arbitrarily long sequence propagating credit assigned current state associated past state based principle study novel algorithm back-propagates temporal skip connection realized learned attention mechanism associate current state relevant past state demonstrate experiment method match outperforms regular bptt truncated bptt task involving particularly long-term dependency without requiring biologically implausible backward replay whole history state additionally demonstrate proposed method transfer longer sequence significantly better lstms trained bptt lstms trained full self-attention
2018,dendritic cortical microcircuit approximate backpropagation algorithm,deep learning seen remarkable development last year many inspired neuroscience however main learning mechanism behind advance – error backpropagation – appears odds neurobiology introduce multilayer neuronal network model simplified dendritic compartment error-driven synaptic plasticity adapts network towards global desired output contrast previous work model require separate phase synaptic learning driven local dendritic prediction error continuously time error originate apical dendrite occur due mismatch predictive input lateral interneurons activity actual top-down feedback use simple dendritic compartment different cell-types model represent error normal activity within pyramidal neuron demonstrate learning capability model regression classification task show analytically approximates error backpropagation algorithm moreover framework consistent recent observation learning brain area architecture cortical microcircuit overall introduce novel view learning dendritic cortical circuit brain may solve long-standing synaptic credit assignment problem
2018,learning hierarchical structure on-the-fly recurrent-recursive model sequence,propose hierarchical model sequential data learns tree on-the-fly ie reading sequence model recurrent network adapts structure reuses recurrent weight recursive manner creates adaptive skip-connections ease learning long-term dependency tree structure either inferred without supervision reinforcement learning learned supervised manner provide preliminary experiment novel math expression evaluation  task created hierarchical tree structure used study effectiveness model additionally test model well-known propositional logic language modelling task experimental  shown potential approach
2018,speaker recognition raw waveform sincnet,deep learning progressively gaining popularity viable alternative i-vectors speaker recognition promising  recently obtained convolutional neural network  fed raw speech sample directly rather employing standard hand-crafted feature latter cnns learn low-level speech representation waveform potentially allowing network better capture important narrow-band speaker characteristic pitch formants proper design neural network crucial achieve goalthis paper proposes novel cnn architecture called sincnet encourages first convolutional layer discover meaningful filter sincnet based parametrized sinc function implement band-pass filter contrast standard cnns learn element filter low high cutoff frequency directly learned data proposed method offer compact efficient way derive customized filter bank specifically tuned desired applicationour experiment conducted speaker identification speaker verification task show proposed architecture converges faster performs better standard cnn raw waveform
2017,integrating language model neural machine translation,recent advance end-to-end neural machine translation model achieved promising  high-resource language pair en→ fr en→ de one major factor behind success availability high quality parallel corpus explore two strategy leveraging abundant amount monolingual data neural machine translation observe improvement combining score neural language model trained target monolingual data neural machine translation model fusing hidden-states two model obtain  bleu improvement hierarchical phrase-based baseline low-resource language pair turkish→ english method initially motivated towards task le parallel data also show extends high resource language cs→ en de→ en translation task obtain   bleu improvement neural machine translation baseline respectively
2017,context-dependent word representation neural machine translation,first observe potential weakness continuous vector representation symbol neural machine translation continuous vector representation word embedding vector symbol encodes multiple dimension similarity equivalent encoding one meaning word consequence encoder decoder recurrent network neural machine translation need spend substantial amount capacity disambiguating source target word based context defined source sentence based observation paper propose contextualize word embedding vector using nonlinear bag-of-words representation source sentence additionally propose represent special token  typed symbol facilitate translating word well-suited translated via continuous vector experiment en–fr en–de reveal proposed approach contextualization symbolization improves translation quality neural machine translation system significantly
2017,multi-way multilingual neural machine translation,abstractwe propose multi-way multilingual neural machine translation proposed approach enables single neural translation model translate multiple language number parameter grows linearly number language made possible single attention mechanism shared across language pair train proposed multi-way multilingual model ten language pair wmt’ simultaneously observe clear performance improvement model trained one language pair empirically evaluate proposed model low-resource language translation task particular observe proposed multilingual model outperforms strong conventional statistical machine translation system turkish-english uzbek-english incorporating resource language pair
2017,equilibrium propagation bridging gap energy-based model backpropagation,introduce equilibrium propagation learning framework energy-based model involves one kind neural computation performed first phase  second phase training  although algorithm computes gradient objective function like backpropagation need special computation circuit second phase error implicitly propagated equilibrium propagation share similarity contrastive hebbian learning contrastive divergence solving theoretical issue algorithm algorithm computes gradient well-defined objective function objective function defined term local perturbation second phase equilibrium propagation corresponds nudging prediction  toward configuration reduces prediction error case recurrent multi-layer supervised network output unit slightly nudged toward target second phase perturbation introduced output layer propagates backward hidden layer show signal back-propagated second phase corresponds propagation error derivative encodes gradient objective function synaptic update corresponds standard form spike-timing dependent plasticity work make plausible mechanism similar backpropagation could implemented brain since leaky integrator neural computation performs inference error back-propagation model local difference two phase whether synaptic change allowed also show experimentally multi-layer recurrently connected network    hidden layer trained equilibrium propagation permutation-invariant mnist task
2017,quantized neural network training neural network low precision weight activation,introduce method train quantized neural network  --- neural network extremely low precision  weight activation run-time train-time quantized weight activation used computing parameter gradient forward pas qnns drastically reduce memory size access replace arithmetic operation bit-wise operation result power consumption expected drastically reduced trained qnns mnist cifar- svhn imagenet datasets resulting qnns achieve prediction accuracy comparable -bit counterpart example quantized version alexnet -bit weight -bit activation achieves  top- accuracy moreover quantize parameter gradient -bits well enables gradient computation using bit-wise operation quantized recurrent neural network tested penn treebank dataset achieved comparable accuracy -bit counterpart using -bits last least programmed binary matrix multiplication gpu kernel possible run mnist qnn  time faster unoptimized gpu kernel without suffering loss classification accuracy qnn code available online
2017,brain tumor segmentation deep neural network,highlights•a fast accurate fully automatic method brain tumor segmentation competitive term accuracy speed compared state art•the method based deep neural network  learns feature specific brain tumor segmentation•we present new dnn architecture exploit local feature well global contextual feature simultaneously•using gpu implementation convolutional output layer model order magnitude faster state art methods•introduces novel cascaded architecture allows system accurately model local label dependenciesabstractin paper present fully automatic brain tumor segmentation method based deep neural network  proposed network tailored glioblastoma  pictured mr image nature tumor appear anywhere brain almost kind shape size contrast reason motivate exploration machine learning solution exploit flexible high capacity dnn extremely efficient give description different model choice we’ve found necessary obtaining competitive performance explore particular different architecture based convolutional neural network  ie dnns specifically adapted image datawe present novel cnn architecture differs traditionally used computer vision cnn exploit local feature well global contextual feature simultaneously also different traditional us cnns network use final layer convolutional implementation fully connected layer allows  fold speed also describe -phase training procedure allows u tackle difficulty related imbalance tumor label finally explore cascade architecture output basic cnn treated additional source information subsequent cnn  reported  brat test data-set reveal architecture improves currently published state-of-the-art  time fastergraphical abstractdownload  download high-res image download  download full-size image
2017,representational geometry word meaning acquired neural machine translation model,work first comprehensive analysis property word embeddings learned neural machine translation  model trained bilingual text show word representation nmt model outperform learned monolingual text established algorithm skipgram cbow task require knowledge semantic similarity and/or lexicalsyntactic role effect hold translating english french english german argue desirable property nmt word embeddings emerge largely independently source target language apply recently-proposed heuristic method training nmt model large vocabulary show vocabulary expansion method  minimal degradation embedding quality allows u make large vocabulary nmt embeddings available future research application overall analysis indicate nmt embeddings used application require word concept organised according similarity and/or lexical function monolingual embeddings better suited modelling  inter-word relatedness
2017,stdp-compatible approximation backpropagation energy-based model,show langevin markov chain monte carlo inference energy-based model latent variable property early step inference starting stationary point correspond propagating error gradient internal layer similar backpropagation backpropagated error respect output unit received outside driving force pushing away stationary point backpropagated error gradient correspond temporal derivative respect activation hidden unit lead weight update proportional product presynaptic firing rate temporal rate change postsynaptic firing rate simulation theoretical argument suggest rate-based update rule consistent associated spike-timing-dependent plasticity idea presented article could element theory explaining brain perform credit assignment deep hierarchy efficiently backpropagation neural computation corresponding approximate inference continuous-valued latent variable error backpropagation time
2017,online offline handwritten chinese character recognition comprehensive study new benchmark,highlights•comprehensive study handwritten chinese character recognition •new benchmark online offline hccr general framework•combination convnet domain-specific knowledge directmap•writer adaptation deep convolutional neural networks•state-of-the-art performance icdar- competition databaseabstractrecent deep learning based method achieved state-of-the-art performance handwritten chinese character recognition  learning discriminative representation directly raw data nevertheless believe long-and-well investigated domain-specific knowledge still help boost performance hccr integrating traditional normalization-cooperated direction-decomposed feature map  deep convolutional neural network  able obtain new highest accuracy online offline hccr icdar- competition database new framework eliminate need data augmentation model ensemble widely used system achieve best  make framework efficient effective training testing furthermore although directmap+convnet achieve best  surpass human-level performance show writer adaptation case still effective new adaptation layer proposed reduce mismatch training test data particular source layer adaptation process efficiently effectively implemented unsupervised manner adding adaptation layer pre-trained convnet adapt new handwriting style particular writer recognition accuracy improved consistently significantly paper give overview comparison recent deep learning based approach hccr also set new benchmark online offline hccr
2017,end-to-end online writer identification recurrent neural network,writer identification important topic pattern recognition artificial intelligence traditional method rely heavily sophisticated hand-crafted feature represent characteristic different writer paper propose end-to-end framework online text-independent writer identification using recurrent neural network  specifically handwriting data particular writer represented set random hybrid stroke  rh randomly sampled short sequence representing pen tip movement xy-coordinates pen-down pen-up state rh independent content language involved handwriting therefore writer identification rh level general convenient character level word level also requires character/word segmentation rnn model bidirectional long short-term memory used encode rh fixed-length vector final classification rh writer classified independently posterior probability averaged make final decision proposed framework end-to-end require domain knowledge handwriting data analysis experiment english  chinese  database verify advantage method compared state-of-the-art approach
2017,denoising criterion variational auto-encoding framework,denoising autoencoders  trained reconstruct clean input noise injected input level variational autoencoders  trained noise injected stochastic hidden layer regularizer encourages noise injection paper show injecting noise input stochastic hidden layer advantageous propose modified variational lower bound improved objective function setup input corrupted standard vae lower bound involves marginalizing encoder conditional distribution input noise make training criterion intractable instead propose modified training criterion corresponds tractable bound input corrupted experimentally find proposed denoising variational autoencoder  yield better average log-likelihood vae importance weighted autoencoder mnist frey face datasets
2017,multiresolution recurrent neural network application dialogue response generation,introduce new class model called multiresolution recurrent neural network explicitly model natural language generation multiple level abstraction model extend sequence-to-sequence framework generate two parallel stochastic process sequence high-level coarse token sequence natural language word  coarse sequence follow latent stochastic process factorial representation help model generalize new example coarse sequence also incorporate task-specific knowledge available experiment coarse sequence extracted using automatic procedure designed capture compositional structure semantics procedure enable training multiresolution recurrent neural network maximizing exact joint log-likelihood sequence apply model dialogue response generation technical support domain compare several competing model multiresolution recurrent neural network outperform competing model substantial margin achieving state-of-the-art  according human evaluation study automatic evaluation metric furthermore experiment show proposed model generate fluent relevant goal-oriented response
2017,hierarchical latent variable encoder-decoder model generating dialogue,sequential data often posse hierarchical structure complex dependency sub-sequences found utterance dialogue model dependency generative framework propose neural network-based generative architecture stochastic latent variable span variable number time step apply proposed model task dialogue response generation compare recent neural-network architecture evaluate model performance human evaluation study experiment demonstrate model improves upon recently proposed model latent variable facilitate generation meaningful long diverse response maintaining dialogue state
2017,towards automatic turing test learning evaluate dialogue response,automatically evaluating quality dialogue response unstructured domain challenging problem unfortunately existing automatic evaluation metric biased correlate poorly human judgement response quality  yet accurate automatic evaluation procedure crucial dialogue research allows rapid prototyping testing new model fewer expensive human evaluation response challenge formulate automatic dialogue evaluation learning problemwe present evaluation model learns predict human-like score input response using new dataset human response score show adem model’s prediction correlate significantly level much higher word-overlap metric bleu human judgement utterance system-level also show adem generalize evaluating dialogue mod-els unseen training important step automatic dialogue evaluation
2017,one hundred layer tiramisu fully convolutional densenets semantic segmentation,state-of-the-art approach semantic image segmentation built convolutional neural network  typical segmentation architecture composed  downsampling path responsible extracting coarse semantic feature followed  upsampling path trained recover input image resolution output model optionally  post-processing module  refine model prediction recently new cnn architecture densely connected convolutional network  shown excellent  image classification task idea densenets based observation layer directly connected every layer feed-forward fashion network accurate easier train paper extend densenets deal problem semantic segmentation achieve state-of-the-art  urban scene benchmark datasets camvid gatech without post-processing module pretraining moreover due smart construction model approach much le parameter currently published best entry datasets
2017,plug & play generative network conditional iterative generation image latent space,generating high-resolution photo-realistic image long-standing goal machine learning recently nguyen et al  showed one interesting way synthesize novel image performing gradient ascent latent space generator network maximize activation one multiple neuron separate classifier network paper extend method introducing additional prior latent code improving sample quality sample diversity leading state-of-the-art generative model produce high quality image higher resolution  previous generative model  imagenet category addition provide unified probabilistic interpretation related activation maximization method call general class model plug play generative network ppgns composed  generator network g capable drawing wide range image type  replaceable condition network c tell generator draw demonstrate generation image conditioned class  also conditioned caption  method also improves state art multifaceted feature visualization  generates set synthetic input activate neuron order better understand deep neural network operate finally show model performs reasonably well task image inpainting image model used paper approach modality-agnostic applied many type data
2017,random weight texture generation one layer cnns,recent work literature shown experimentally one use lower layer trained convolutional neural network  model natural texture interestingly also experimentally shown one layer random filter also model texture although le variability paper ask question one layer cnns random filter effective generating texture theoretically show one layer convolutional architecture  paired energy function used previous literature fact preserve modulate frequency coefficient manner random weight pretrained weight generate type image based  analysis question whether similar property hold case one us one convolution layer non-linearity show case relu non-linearity situation one input give minimum possible energy whereas case nonlinearity always infinite solution give minimum possible energy thus show certain situation adding relu non-linearity generates le variable image
2017,network deep neural network distant speech recognition,despite remarkable progress recently made distant speech recognition state-of-the-art technology still suffers lack robustness especially adverse acoustic condition characterized non-stationary noise reverberation met prominent limitation current system lie lack matching communication various technology involved distant speech recognition process speech enhancement speech recognition module instance often trained independently moreover speech enhancement normally help speech recognizer output latter commonly used turn improve speech enhancement address concern propose novel architecture based network deep neural network component jointly trained better cooperate thanks full communication scheme experiment conducted using different datasets task acoustic condition revealed proposed framework overtake competitive solution including recent joint training approach
2017,count-ception counting fully convolutional redundant counting,counting object digital image process replaced machine tedious task time consuming prone error due fatigue human annotator goal system take input image return count object inside justification prediction form object localization repose problem originally posed lempitsky zisserman instead predict count map contains redundant count based receptive field smaller regression network regression network predicts count object exist inside frame processing image fully convolutional way pixel going accounted number time number window include size window  recover true count take average redundant prediction contribution redundant counting instead predicting density map order average error also propose novel deep neural network architecture adapted inception family network called count-ception network together approach   relative improvement  state art method xie noble zisserman 
2017,understanding intermediate layer using linear classifier probe,neural network model reputation black boxeswe propose new method better understand role dynamicsof intermediate layersour method us linear classifier referred probeswhere probe use hidden unit given intermediate layeras discriminating featuresmoreover probe cannot affect training phase modeland generally added trainingwe demonstrate used develop better intuitionabout model diagnose potential problem
2017,actor-critic algorithm sequence prediction,present approach training neural network generate sequence using actor-critic method reinforcement learning  current log-likelihood training method limited discrepancy training testing mode model must generate token conditioned previous guess rather ground-truth token address problem introducing textit{critic} network trained predict value output token given policy textit{actor} network  training procedure much closer test phase allows u directly optimize task-specific score bleu crucially since leverage technique supervised learning setting rather traditional rl setting condition critic network ground-truth output show method lead improved performance synthetic task german-english machine translation analysis pave way method applied natural language generation task machine translation caption generation dialogue modelling 
2017,mode regularized generative adversarial network,although generative adversarial network achieve state-of-the-art  avariety generative task regarded highly unstable prone missmodes argue bad behavior gans due particularfunctional shape trained discriminator high dimensional space whichcan easily make training stuck push probability mass wrong directiontowards higher concentration data generating distributionwe introduce several way regularizing objective dramaticallystabilize training gan model also show regularizers helpthe fair distribution probability mass across mode data generatingdistribution early phase training thus providing unified solutionto missing mode problem
2017,hierarchical multiscale recurrent neural network,learning hierarchical temporal representation among long- standing challenge recurrent neural network multiscale recurrent neural network considered promising approach resolve issue yet lack empirical evidence showing type model actually capture temporal dependency discovering latent hierarchical structure sequence paper propose novel multiscale approach called hierarchical multiscale recurrent neural network capture latent hierarchical structure sequence encoding temporal dependency different timescales using novel update mechanism show evidence proposed model discover underlying hierarchical structure sequence without using explicit boundary information evaluate proposed model character-level language modelling handwriting sequence generation
2017,mollifying network,optimization deep neural network challenging traditional convex optimization problem due highly non-convex nature loss function eg involve pathological landscape saddle-surfaces difficult escape algorithm based simple gradient descent paper attack problem optimization highly non-convex neural network objective starting smoothed -- mollified -- objective function becomes complex training proceeds  proposition inspired recent study continuation method similarly curriculum method begin learning easier  objective function let evolve training eventually becomes original difficult optimize objective function complexity mollified network controlled single hyperparameter annealed training show improvement various difficult optimization task establish relationship recent work continuation method neural network mollifiers
2017,zoneout regularizing rnns randomly preserving hidden activation,propose zoneout novel method regularizing rnnsat timestep zoneout stochastically force hidden unit maintain previous valueslike dropout zoneout us random noise train pseudo-ensemble improving generalizationbut preserving instead dropping hidden unit gradient information state information readily propagated time feedforward stochastic depth networkswe perform empirical investigation various rnn regularizers find zoneout give significant performance improvement across task achieve competitive  relatively simple model character- word-level language modelling penn treebank text datasets combining recurrent batch normalization yield state-of-the-art  permuted sequential mnist
2017,structured self-attentive sentence embedding,paper proposes new model extracting interpretable sentence embedding introducing self-attention instead using vector use -d matrix represent embedding row matrix attending different part sentence also propose self-attention mechanism special regularization term model side effect embedding come easy way visualizing specific part sentence encoded embedding evaluate model  different task author profiling sentiment classification textual entailment  show model yield significant performance gain compared sentence embedding method  task
2017,samplernn unconditional end-to-end neural audio generation model,paper propose novel model unconditional audio generation task generates one audio sample time show model profit combining memory-less module namely autoregressive multilayer perceptron stateful recurrent neural network hierarchical structure de facto powerful capture underlying source variation temporal domain long time three datasets different nature human evaluation generated sample indicate model preferred competing model also show component model contributes exhibited performance
2017,generalizable feature unsupervised learning,human learn predictive model world use model reason future event consequence action contrast machine predictor exhibit impressive ability generalize unseen scenario reason intelligently setting  one important aspect ability physical intuition work explore potential unsupervised learning find feature promote better generalization setting outside supervised training distribution  task predicting stability tower square block demonstrate unsupervised model trained predict future frame video sequence stable unstable block configuration yield feature support extrapolating stability prediction block configuration outside training set distribution
2017,diet network thin parameter fat genomics,learning task involving genomic data often pose serious challenge number input feature order magnitude larger number training example making difficult avoid overfitting even using known regularization technique focus task input description genetic variation specific patient single nucleotide polymorphism  yielding million ternary input improving ability deep learning handle datasets could important impact medical research specifically precision medicine high-dimensional data regarding particular patient used make prediction interest even though amount data task increasing mismatch number example number input remains concern naive implementation classifier neural network involve huge number free parameter first layer  input feature associated many parameter hidden unit propose novel neural network parametrization considerably reduces number free parameter based idea first learn provide distributed representation input feature  learn  map feature distributed representation  vector parameter specific feature classifier neural network  approach view problem producing parameter associated feature multi-task learning problem show experimentally population stratification task interest medical study proposed approach significantly reduce number parameter error rate classifier
2017,char2wav end-to-end speech synthesis,present charwav end-to-end model speech synthesis charwav two component reader neural vocoder reader encoder-decoder model attention encoder bidirectional recurrent neural network accepts text phoneme input decoder recurrent neural network  attention produce vocoder acoustic feature neural vocoder refers conditional extension samplernn generates raw waveform sample intermediate representation unlike traditional model speech synthesis charwav learns produce audio directly text
2017,improving generative adversarial network denoising feature matching,propose augmented training procedure generative adversarial network designed address shortcoming original directing generator towards probable configuration abstract discriminator feature estimate track distribution feature computed data denoising auto-encoder use propose high-level target generator combine new loss original evaluate hybrid criterion task unsupervised image synthesis datasets comprising diverse set visual category noting qualitative quantitative improvement ``objectness resulting sample
2017,closer look memorization deep network,examine role memorization deep learning drawing connection capacity generalization adversarial robustness deep network capable memorizing noise data  suggest tend prioritize learning simple pattern first experiment expose qualitative difference gradient-based optimization deep neural network  noise vs~real data also demonstrate appropriately tuned explicit regularization  degrade dnn training performance noise datasets without compromising generalization real data analysis suggests notion effective capacity dataset independent unlikely explain generalization performance deep network trained gradient based method training data play important role determining degree memorization
2017,sharp minimum generalize deep net,despite overwhelming capacity overfit deep learning architecture tend generalize relatively well unseen data allowing deployed practice however explaining case still open area research one standing hypothesis gaining popularity eg hochreiter & schmidhuber  keskar et al  flatness minimum loss function found stochastic gradient based method  good generalization paper argues notion flatness problematic deep model directly applied explain generalization specifically focusing deep network rectifier unit exploit particular geometry parameter space induced inherent symmetry architecture exhibit build equivalent model corresponding arbitrarily sharper minimum depending definition flatness given minimum furthermore allow reparametrize function geometry parameter change drastically without affecting generalization property
2017,robust adaptive stochastic gradient method deep learning,stochastic gradient algorithm main focus large-scale optimization problem led important success recent advancement deep learning algorithm convergence sgd depends careful choice learning rate amount noise stochastic estimate gradient paper propose adaptive learning rate algorithm utilizes stochastic curvature information loss function automatically tuning learning rate information element-wise curvature loss function estimated local statistic stochastic first order gradient propose new variance reduction technique speed convergence experiment deep neural network obtained better performance compared popular stochastic gradient algorithm
2017,improving speech recognition revising gated recurrent unit,speech recognition largely taking advantage deep learning showing substantial benefit obtained modern recurrent neural network  popular rnns long short-term memory  typically reach state-of-the-art performance many task thanks ability learn long-term dependency robustness vanishing gradient nevertheless lstms rather complex design three multiplicative gate might impair efficient implementation attempt simplify lstms recently led gated recurrent unit  based two multiplicative gatesthis paper build effort revising grus proposing simplified architecture potentially suitable speech recognition contribution work two-fold first suggest remove reset gate gru design resulting efficient single-gate architecture second propose replace tanh relu activation state update equation  show implementation revised architecture reduces per-epoch training time  consistently improves recognition performance across different task input feature noisy condition compared standard gru
2017,dynamic layer normalization adaptive neural acoustic modeling speech recognition,layer normalization recently introduced technique normalizing activity neuron deep neural network improve training speed stability paper introduce new layer normalization technique called dynamic layer normalization  adaptive neural acoustic modeling speech recognition dynamically generating scaling shifting parameter layer normalization dln adapts neural acoustic model acoustic variability arising various factor speaker channel noise environment unlike adaptive acoustic model proposed approach require additional adaptation data speaker information i-vectors moreover model size fixed dynamically generates adaptation parameter apply proposed dln deep bidirectional lstm acoustic model evaluate two benchmark datasets large vocabulary asr experiment wsj ted-lium release  experimental  show dln improves neural acoustic model term transcription accuracy dynamically adapting various speaker environment
2017,towards hardware-friendly deep learning,n/a
2017,variational walkback learning transition operator stochastic recurrent net,propose novel method { directly} learn stochastic transition operator whose repeated application provides generated sample traditional undirected graphical model approach problem indirectly learning markov chain model whose stationary distribution obeys detailed balance respect parameterized energy function energy function modified model data distribution match guarantee number step required markov chain converge moreover detailed balance condition highly restrictive energy based model corresponding neural network must symmetric weight unlike biological neural circuit contrast develop method directly learning arbitrarily parameterized transition operator capable expressing non-equilibrium stationary distribution violate detailed balance thereby enabling u learn biologically plausible asymmetric neural network general non-energy based dynamical system proposed training objective derive via principled variational method encourages transition operator walk back  multi-step trajectory start data-points quickly possible back original data point present series experimental  illustrating soundness proposed approach variational walkback  mnist cifar- svhn celeba datasets demonstrating superior sample compared earlier attempt learn transition operator also show although rapid training trajectory limited finite variable number step transition operator continues generate good sample well past length trajectory thereby demonstrating match non-equilibrium stationary distribution data distribution source codehttp//githubcom/anirudh/walkback_nips
2017,gibbsnet iterative adversarial inference deep graphical model,directed latent variable model formulate joint distribution p = p phave advantage fast exact sampling however model weakness needing specify p often simple fixed prior limit expressiveness model undirected latent variable model discard requirement pbe specified prior yet sampling generally requires iterative procedure blocked gibbs-sampling may require many step draw sample joint distribution p propose novel approach learning joint distribution data latent code us adversarially learned iterative procedure gradually refine joint distribution p better match data distribution step gibbsnet best world theory practice achieving speed simplicity directed latent variable model guaranteed  produce sample pwith sampling iteration achieving expressiveness flexibility undirected latent variable model gibbsnet away need explicit pand ability attribute prediction class-conditional generation joint image-attribute modeling single model trained specific task show empirically gibbsnet able learn complex pand show lead improved inpainting iterative refinement pfor dozen step stable generation without collapse thousand step despite trained step
2017,plan attend generate planning sequence-to-sequence model,investigate integration planning mechanism sequence-to-sequence model using attention develop model plan ahead future computes alignment input output sequence constructing matrix proposed future alignment commitment vector governs whether follow recompute plan mechanism inspired recently proposed strategic attentive reader writer  model reinforcement learning proposed model end-to-end trainable using primarily differentiable operation show outperforms strong baseline character-level translation task wmt algorithmic task finding eulerian circuit graph question generation text analysis demonstrates model computes qualitatively intuitive alignment converges faster baseline achieves superior performance fewer parameter
2017,z-forcing training stochastic recurrent network,many effort devoted training generative latent variable model autoregressive decoder recurrent neural network  stochastic recurrent model successful capturing variability observed natural sequential data speech unify successful idea recently proposed architecture stochastic recurrent model step sequence associated latent variable used condition recurrent dynamic future step training performed amortised variational inference approximate posterior augmented rnn run backward sequence addition maximizing variational lower bound ease training latent variable adding auxiliary cost force reconstruct state backward recurrent network provides latent variable task-independent objective enhances performance overall model found strategy perform better alternative approach kl annealing although conceptually simple model achieves state-of-the-art  standard speech benchmark timit blizzard competitive performance sequential mnist finally apply model language modeling imdb dataset auxiliary cost help learning interpretable latent variable
2017,plan attend generate character-level neural machine translation planning,investigate integration planning mechanism encoder-decoder architecture attention develop model plan ahead computes alignment source target sequence single time-step next k time-steps well constructing matrix proposed future alignment commitment vector governs whether follow recompute plan mechanism inspired strategic attentive reader writer  model recent neural architecture planning hierarchical reinforcement learning also learn higher level temporal abstraction proposed model end-to-end trainable differentiable operation show model outperforms strong baseline character-level translation task wmt’ fewer parameter computes alignment qualitatively intuitive
2016,knowledge matter importance prior information optimization,explored effect introducing prior knowledge intermediate level deep supervised neural network two task task designed black-box state-of-the-art machine learning algorithm tested failed generalize well motivate work hypothesis training barrier involved nature task human learn useful intermediate concept individual using form supervision guidance using curriculum  provide positive evidence favor hypothesis experiment trained two- tiered mlp architecture dataset input image contains three sprite binary target class  three shape belong category otherwise class  term generalization black-box machine learning algorithm could perform better chance task standard deep supervised neural network also failed generalize however using particular structure guiding learner providing intermediate target form intermediate concept  allowed u solve task efficiently obtained much better chance imperfect  exploring different architecture optimization variant observation might indication optimization difficulty neural network trained without hint task hypothesize learning difficulty due composition two highly non-linear task finding also consistent hypothesis cultural learning inspired observation training neural network sometimes getting stuck even though good solution exist term training generalization error
2016,emonets multimodal deep learning approach emotion recognition video,task emotion recognition wild  challenge assign one seven emotion short video clip extracted hollywood style movie video depict acted-out emotion realistic condition large degree variation attribute pose illumination making worthwhile explore approach consider combination feature multiple modality label assignment paper present approach learning several specialist model using deep learning technique focusing one modality among convolutional neural network focusing capturing visual information detected face deep belief net focusing representation audio stream k-means based “bag-of-mouths” model extract visual feature around mouth region relational autoencoder address spatio-temporal aspect video explore multiple method combination cue modality one common classifier achieves considerably greater accuracy prediction strongest single-modality classifier method winning submission  emotiw challenge achieved test set accuracy  %  dataset
2016,big data theoretical aspect scanning issue,special issue highlight number algorithmic approach fundamental data analysis formulating solving problem relate big data
2016,learning understand phrase embedding dictionary,distributional model learn rich semantic word representation success story recent nlp research however developing model learn useful representation phrase sentence proved far harder propose using definition found everyday dictionary mean bridging gap lexical phrasal semantics neural language embedding model effectively trained map dictionary definition   representation word defined definition present two application architecture reverse dictionary return name concept given definition description general-knowledge crossword question answerer task neural language embedding model trained definition handful freely-available lexical resource perform well better existing commercial system rely significant task-specific engineering  highlight effectiveness neural embedding architecture definition-based training developing model understand phrase sentence 
2016,building end-to-end dialogue system using generative hierarchical neural network model,investigate task building open domain conversational dialogue system based large dialogue corpus using generative model generative model produce system response autonomously generated word-by-word opening possibility realistic flexible interaction support goal extend recently proposed hierarchical recurrent encoder-decoder neural network dialogue domain demonstrate model competitive state-of-the-art neural language model back-off n-gram model investigate limitation similar approach show performance improved bootstrapping learning larger question-answer pair corpus pretrained word embeddings
2016,character-level decoder without explicit segmentation neural machine translation,existing machine translation system whether phrase-based neural relied almost exclusively word-level modelling explicit segmentation paper ask fundamental question neural machine translation generate character sequence without explicit segmentation answer question evaluate attention-based encoder-decoder subword-level encoder character-level decoder four language pairs--en-cs en-de en-ru en-fi-- using parallel corpus wmt experiment show model character-level decoder outperform one subword-level decoder four language pair furthermore ensemble neural model character-level decoder outperform state-of-the-art non-neural machine translation system en-cs en-de en-fi perform comparably en-ru
2016,pointing unknown word,problem rare unknown word important issue potentially influence performance many nlp system including traditional count-based deep learning model propose novel way deal rare unseen word neural network model using attention model us two softmax layer order predict next word conditional language model one predicts location word source sentence predicts word shortlist vocabulary time-step decision softmax layer use choose adaptively made mlp conditioned context~we motivate work psychological evidence human naturally tendency point towards object context environment name object known~we observe improvement two task neural machine translation europarl english french parallel corpus text summarization gigaword dataset using proposed model
2016,generating factoid question recurrent neural network 30m factoid question-answer corpus,past decade large-scale supervised learning corpus enabled machine learning researcher make substantial advance however date large-scale question-answer corpus available paper present factoid question-answer corpus enormous question answer pair corpus produced applying novel neural network architecture knowledge base freebase transduce fact natural language question produced question answer pair evaluated human evaluator using automatic evaluation metric including well-established machine translation sentence similarity metric across evaluation criterion question-generation model outperforms competing template-based baseline furthermore presented human evaluator generated question appear comparable quality real human-generated question
2016,oracle performance visual captioning,task associating image video natural language description attracted great amount attention recently state-of-the-art  standard datasets pushed regime become difficult make significant improvement instead proposing new model work investigates performance oracle obtain order disentangle contribution visual model language model oracle assumes high-quality visual concept extractor available focus language part demonstrate construction oracle ms-coco youtubetext lsmdc  surprisingly despite simplicity model training procedure show current state-of-the-art model fall short compared learned oracle furthermore suggests inability current model capturing important visual concept captioning task
2016,reseg recurrent neural network-based model semantic segmentation,propose structured prediction architecture exploit local generic feature extracted convolutional neural network capacity recurrent neural network  retrieve distant dependency proposed architecture called reseg based recently introduced renet model image classification modify extend perform challenging task semantic segmentation renet layer composed four rnn sweep image horizontally vertically direction encoding patch activation providing relevant global information moreover renet layer stacked top pre-trained convolutional layer benefiting generic local feature upsampling layer follow renet layer recover original image resolution final prediction proposed reseg architecture efficient flexible suitable variety semantic segmentation task evaluate reseg several widely-used semantic segmentation datasets weizmann horse oxford flower camvid achieving stateof-the-art performance  show reseg act suitable architecture semantic segmentation task may application structured prediction problem source code model hyperparameters available https//githubcom/fvisin/reseg
2016,batch normalized recurrent neural network,recurrent neural network  powerful model sequential data potential learn long-term dependency however computationally expensive train difficult parallelize recent work shown normalizing intermediate representation neural network significantly improve convergence rate feed-forward neural network  particular batch normalization us mini-batch statistic standardize feature shown significantly reduce training time paper investigate batch normalization applied rnns show speech recognition task language modeling way apply batch normalization lead faster convergence training criterion doesnt seem improve generalization performance
2016,end-to-end attention-based large vocabulary speech recognition,many state-of-the-art large vocabulary continuous speech recognition  system hybrid neural network hidden markov model  recently direct end-to-end method investigated neural architecture trained model sequence character  knowledge approach relied connectionist temporal classification  module investigate alternative method sequence modelling based attention mechanism allows recurrent neural network  learn alignment sequence input frame output label show setup applied lvcsr integrating decoding rnn n-gram language model speeding operation constraining selection made attention mechanism reducing source sequence length pooling information time recognition accuracy similar hmm-free rnn-based approach reported wall street journal corpus
2016,unitary evolution recurrent neural network,recurrent neural network  notoriously difficult train eigenvalue hidden hidden weight matrix deviate absolute value  optimization becomes difficult due well studied issue vanishing exploding gradient especially trying learn long-term dependency circumvent problem propose new architecture learns unitary weight matrix eigenvalue absolute value exactly  challenge address parametrizing unitary matrix way require expensive computation  weight update construct expressive unitary weight matrix composing several structured matrix act building block parameter learned optimization parameterization becomes feasible considering hidden state complex domain demonstrate potential architecture achieving state art  several hard task involving long-term dependency
2016,deconstructing ladder network architecture,ladder network recent new approach semi-supervised learning turned successful showing impressive performance ladder network many component intertwined whose contribution obvious complex architecture paper present extensive experimental investigation variant ladder network replaced removed individual component learn relative importance semi-supervised task conclude important contribution made lateral connection followed application noise choice refer ‘combinator function’ number labeled training example increase lateral connection reconstruction criterion become le important generalization improvement coming injection noise layer finally introduce combinator function reduces test error rate permutation-invariant mnist  supervised setting   semi-supervised setting   labeled example respectively
2016,bidirectional helmholtz machine,efficient unsupervised training inference deep generative model remains challenging problem one basic approach called helmholtz machine variational autoencoder involves training top-down directed generative model together bottom-up auxiliary model used approximate inference recent  indicate better generative model obtained better approximate inference procedure instead improving inference procedure propose new model bidirectional helmholtz machine guarantee top-down bottom-up distribution efficiently invert achieve interpreting top-down bottom-up directed model approximate inference distribution defining model distribution geometric mean two present lower-bound likelihood model show optimizing bound regularizes model bhattacharyya distance bottom-up top-down approximate distribution minimized approach  state art generative model prefer significantly deeper architecture allows order magnitude efficient likelihood estimation
2016,noisy activation function,common nonlinear activation function used neural network cause training difficulty due saturation behavior activation function may hide dependency visible vanilla-sgd  gating mechanism use softly saturating activation function emulate discrete switching digital logic circuit good example propose exploit injection appropriate noise gradient may flow easily even noiseless application activation function would yield zero gradient large noise dominate noise-free gradient allow stochastic gradient descent explore adding noise problematic part activation function allow optimization procedure explore boundary degenerate saturating well-behaved part activation function also establish connection simulated annealing amount noise annealed making easier optimize hard objective function find experimentally replacing saturating activation function noisy variant help optimization many context yielding state-of-the-art competitive  different datasets task especially training seems difficult eg curriculum learning necessary obtain good 
2016,towards end-to-end speech recognition deep convolutional neural network,convolutional neural network  effective model reducing spectral variation modeling spectral correlation acoustic feature automatic speech recognition  hybrid speech recognition system incorporating cnns hidden markov models/gaussian mixture model  achieved state-of-the-art various benchmark meanwhile connectionist temporal classification  recurrent neural network  proposed labeling unsegmented sequence make feasible train end-to-end speech recognition system instead hybrid setting however rnns computationally expensive sometimes difficult train paper inspired advantage cnns ctc approach propose end-to-end speech framework sequence labeling combining hierarchical cnns ctc directly without recurrent connection evaluating approach timit phoneme recognition task show proposed model computationally efficient also competitive existing baseline system moreover argue cnns capability model temporal correlation appropriate context information
2016,hemis hetero-modal image segmentation,introduce deep learning image segmentation framework extremely robust missing imaging modality instead attempting impute synthesize missing data proposed approach learns modality embedding input image single latent vector space arithmetic operation  well defined point space averaged modality available inference time processed yield desired segmentation combinatorial subset available modality provided input without learn combinatorial number imputation model evaluated two neurological mri datasets  approach yield state-of-the-art segmentation  provided modality moreover performance degrades remarkably gracefully modality removed significantly alternative mean-filling synthesis approach
2016,multi-way multilingual neural machine translation shared attention mechanism,propose multi-way multilingual neural machine translation proposed approach enables single neural translation model translate multiple language number parameter grows linearly number language made possible single attention mechanism shared across language pair train proposed multi-way multilingual model ten language pair wmt simultaneously observe clear performance improvement model trained one language pair particular observe proposed model significantly improves translation quality low-resource language pair
2016,architectural complexity measure recurrent neural network,paper systematically analyze connecting architecture recurrent neural network  main contribution twofold first present rigorous graph-theoretic framework describing connecting architecture rnns general second propose three architecture complexity measure rnns  recurrent depth capture rnn’s over-time nonlinear complexity  feedforward depth capture local input-output nonlinearity   recurrent skip coefficient capture rapidly information propagates time rigorously prove measure’s existence computability experimental  show rnns might benefit larger recurrent depth feedforward depth demonstrate increasing recurrent skip coefficient offer performance boost long term dependency problem
2016,multiplicative integration recurrent neural network,introduce general simple structural design called “multiplicative integration”  improve recurrent neural network  mi change way information flow get integrated computational building block rnn introducing almost extra parameter new structure easily embedded many popular rnn model including lstms grus empirically analyze learning behaviour conduct evaluation several task using different rnn model experimental  demonstrate multiplicative integration provide substantial performance boost many existing rnn model
2016,binarized neural network,introduce method train binarized neural network  - neural network binary weight activation run-time train-time binary weight activation used computing parameter gradient forward pas bnns drastically reduce memory size access replace arithmetic operation bit-wise operation expected substantially improve power-efficiency validate effectiveness bnns conducted two set experiment torch theano framework bnns achieved nearly state-of-the-art  mnist cifar- svhn datasets also report preliminary  challenging imagenet dataset last least wrote binary matrix multiplication gpu kernel possible run mnist bnn  time faster unoptimized gpu kernel without suffering loss classification accuracy code training running bnns available on-line
2016,professor forcing new algorithm training recurrent network,teacher forcing algorithm train recurrent network supplying observed sequence value input training using network’s one-step-ahead prediction multi-step sampling introduce professor forcing algorithm us adversarial domain adaptation encourage dynamic recurrent network training network sampling network multiple time step apply professor forcing language modeling vocal synthesis raw waveform handwriting generation image generation empirically find professor forcing act regularizer improving test likelihood character level penn treebank sequential mnist also find model qualitatively improves sample especially sampling large number time step supported human evaluation sample quality trade-off professor forcing scheduled sampling discussed produce t-snes showing professor forcing successfully make dynamic network training sampling similar
2016,batch-normalized joint training dnn-based distant speech recognition,improving distant speech recognition crucial step towards flexible human-machine interface current technology however still exhibit lack robustness especially adverse acoustic condition met despite significant progress made last year speech enhancement speech recognition one potential limitation state-of-the-art technology lie composing module well matched trained jointly
2016,nyu-mila neural machine translation system wmt16,describe neural machine translation system new york university  university montreal  translation task wmt main goal nyu-mila submission wmt evaluate new character-level decoding approach neural machine translation various language pair proposed neural machine translation system attention-based encoderdecoder subword-level encoder character-level decoder decoder neural machine translation system require explicit segmentation character used token character-level decoding approach provides benefit especially translating source language morphologically rich language
2016,neural network multiplication,deep learning algorithm training notoriously time consuming since computation training neural network typically spent floating point multiplication investigate approach training eliminates need method consists two part first stochastically binarize weight convert multiplication involved computing hidden state sign change second back-propagating error derivative addition binarizing weight quantize representation layer convert remaining multiplication binary shift experimental  across  popular datasets  show approach hurt classification performance result even better performance standard stochastic gradient descent training paving way fast hardware-friendly training neural network
2015,deep learning,deep learning allows computational model composed multiple processing layer learn representation data multiple level abstraction method dramatically improved state-of-the-art speech recognition visual object recognition object detection many domain drug discovery genomics deep learning discovers intricate structure large data set using backpropagation algorithm indicate machine change internal parameter used compute representation layer representation previous layer deep convolutional net brought breakthrough processing image video speech audio whereas recurrent net shone light sequential data text speech
2015,editorial introduction neural network special issue deep learning representation,n/a
2015,challenge representation learning report three machine learning contest,icml  workshop challenge representation learning focused three challenge black box learning challenge facial expression recognition challenge multimodal learning challenge describe datasets created challenge summarize  competition provide suggestion organizer future challenge comment kind knowledge gained machine learning competition
2015,using recurrent neural network slot filling spoken language understanding,semantic slot filling one challenging problem spoken language understanding  paper propose use recurrent neural network  task present several novel architecture designed efficiently model past future temporal dependency specifically implemented compared several important rnn architecture including elman jordan hybrid variant facilitate reproducibility implemented network publicly available theano neural network toolkit completed experiment well-known airline travel information system  benchmark addition compared approach two custom slu data set entertainment movie domain  show rnn-based model outperform conditional random field  baseline  absolute error reduction atis benchmark improve state-of-the-art  entertainment domain  movie domain
2015,describing multimedia content using attention-based encoder-decoder network,whereas deep neural network first mostly used classification task rapidly expanding realm structured output problem observed target composed multiple random variable rich joint distribution given input paper focus case input also rich structure input output structure somehow related describe system learn attend different place input element output variety task machine translation image caption generation video clip description speech recognition system based shared set building block gated recurrent neural network convolutional neural network along trained attention mechanism report experimental  system showing impressively good performance advantage attention mechanism
2015,using large target vocabulary neural machine translation,neural machine translation recently proposed approach machine translation based purely neural network shown promising  compared existing approach phrase-based statistical machine translation despite recent success neural machine translation limitation handling larger vocabulary training complexity well decoding complexity increase proportionally number target word paper propose method based importance sampling allows u use large target vocabulary without increasing training complexity show decoding efficiently done even model large target vocabulary selecting small subset whole target vocabulary model trained proposed approach empirically found outperform baseline model small vocabulary well lstm-based neural machine translation model furthermore use ensemble model large target vocabulary achieve state-of-the-art translation performance  english!german translation almost high performance state-of-the-art english!french translation system
2015,iapr keynote lecture iv deep learning,deep learning arisen around  renewal neural network research allowing model layer theoretical investigation shown function obtained deep composition simpler function  express highly varying function  much efficiently  otherwise empirical work variety application demonstrated well trained deep architecture highly successful remarkably breaking previous state-of-the-art many area including speech recognition object recognition language model transfer learning talk summarize advance made breakthrough possible end question major challenge still ahead researcher order continue climb towards ai-level competence
2015,hierarchical recurrent encoder-decoder generative context-aware query suggestion,user may strive formulate adequate textual query information need search engine assist user presenting query suggestion preserve original search intent suggestion context-aware account previous query issued user achieving context awareness challenging due data sparsity present novel hierarchical recurrent encoder-decoder architecture make possible account sequence previous query arbitrary length result suggestion sensitive order query context avoiding data sparsity additionally model suggest rare long-tail query produced suggestion synthetic sampled one word time using computationally cheap decoding technique contrast current synthetic suggestion model relying upon machine learning pipeline hand-engineered feature set  show model outperforms existing context-aware approach next query prediction setting addition query suggestion architecture general enough used variety application
2015,bilbowa fast bilingual distributed representation without word alignment,introduce bilbowa  simple computationally-efficient model learning bilingual distributed representation word scale large monolingual datasets require word-aligned parallel training data instead train directly monolingual data extract bilingual signal smaller set raw-text sentence-aligned data achieved using novel sampled bag-of-words cross-lingual objective used regularize two noise-contrastive language model efficient cross-lingual feature learning show bilingual embeddings learned using proposed model outperforms state-of-the-art method cross-lingual document classification task well lexical translation task wmt data
2015,show attend tell neural image caption generation visual attention,inspired recent work machine translation object detection introduce attention based model automatically learns describe content image describe train model deterministic manner using standard backpropagation technique stochastically maximizing variational lower bound also show visualization model able automatically learn fix gaze salient object generating corresponding word output sequence validate use attention state-of-the-art performance three benchmark datasets flickrk flickrk m coco
2015,gated feedback recurrent neural network,work propose novel recurrent neural network  architecture proposed rnn gated-feedback rnn  extends existing approach stacking multiple recurrent layer allowing controlling signal flowing upper recurrent layer lower layer using global gating unit pair layer recurrent signal exchanged layer gated adaptively based previous hidden state current input evaluated proposed gf-rnn different type recurrent unit tanh long short-term memory gated recurrent unit task character-level language modeling python program evaluation empirical evaluation different rnn unit revealed task gf-rnn outperforms conventional approach build deep stacked rnns suggest improvement arises gf-rnn adaptively assign different layer different timescales layer-to-layer interaction  learning gate interaction
2015,attention-based model speech recognition,recurrent sequence generator conditioned input data attention mechanism recently shown good performance range task including machine translation handwriting synthesis image caption generation extend attention-mechanism feature needed speech recognition show adaptation model used machine translation reach competitive  phoneme error rate  timit phoneme recognition task applied utterance roughly long one trained offer qualitative explanation failure propose novel generic method adding location-awareness attention mechanism alleviate issue new method yield model robust long input achieves  per single utterance  -times longer  utterance finally propose change attention mechanism prevents concentrating much single frame reduces per  level
2015,equilibrated adaptive learning rate non-convex optimization,parameter-specific adaptive learning rate method computationally efficient way reduce ill-conditioning problem encountered training large deep network following recent work strongly suggests thecritical point encountered training network saddle point find considering presence negative eigenvalue hessian could help u design better suited adaptive learning rate scheme show popular jacobi preconditioner undesirable behavior presence positive negative curvature present theoretical empirical evidence so-called equilibration preconditioner comparatively better suited non-convex problem introduce novel adaptive learning rate scheme called esgd based equilibration preconditioner experiment demonstrate scheme yield similar step direction esgd sometimes surpasses rmsprop term convergence speed always clearly improving plain stochastic gradient descent
2015,recurrent latent variable model sequential data,paper explore inclusion latent random variable hidden state recurrent neural network  combining element variational autoencoder argue use high-level latent random variable variational rnn  model kind variability observed highly structured sequential data natural speech empirically evaluate proposed model related sequential model four speech datasets one handwriting dataset  show important role latent random variable play rnn dynamic
2015,binaryconnect training deep neural network binary weight propagation,deep neural network  achieved state-of-the-art  wide range task best  obtained large training set large model past gpus enabled breakthrough greater computational speed future faster computation training test time likely crucial progress consumer application low-power device result much interest research development dedicated hardware deep learning  binary weight ie weight constrained two possible value  would bring great benefit specialized dl hardware replacing many multiply-accumulate operation simple accumulation multiplier space power-hungry component digital implementation neural network introduce binaryconnect method consists training dnn binary weight forward backward propagation retaining precision stored weight gradient accumulated like dropout scheme show binaryconnect act regularizer obtain near state-of-the-art  binaryconnect permutation-invariant mnist cifar- svhn
2015,artificial neural network applied taxi destination prediction,describe first-place solution ecml/pkdd discovery challenge taxi destination prediction task consisted predicting destination taxi based beginning trajectory represented variable-length sequence gps point diverse associated meta-information departure time driver id client information contrary published competitor approach used almost fully automated approach based neural network ranked first  team architecture tried use multi-layer perceptrons bidirectional recurrent neural network model inspired recently introduced memory network approach could easily adapted application goal predict fixed-length output variable-length sequence
2015,difference target propagation,back-propagation workhorse recent success deep learning relies infinitesimal effect  order perform credit assignment could become serious issue one considers deeper non-linear function eg consider extreme case non-linearity relation parameter cost actually discrete inspired biological implausibility back-propagation approach proposed past could play similar credit assignment role spirit explore novel approach credit assignment deep network call target propagation main idea compute target rather gradient layer like gradient propagated backwards way related different previously proposed proxy back-propagation rely backwards network symmetric weight target propagation relies auto-encoders layer unlike back-propagation applied even unit exchange stochastic bit rather real number show linear correction imperfectness auto-encoders called difference target propagation effective make target propagation actually work leading  comparable back-propagation deep network discrete continuous unit denoising auto-encoders achieving state art stochastic network
2015,montreal neural machine translation system wmt15,neural machine translation  system recently achieved  comparable state art translation task including englishfrench englishgerman main purpose montreal institute learning algorithm  submission wmt evaluate new approach greater variety language pair furthermore human evaluation campaign may help u research community better understand behaviour system use rnnsearch architecture add attention mechanism encoderdecoder also leverage recent development nmt including use large vocabulary unknown word replacement limited degree inclusion monolingual language model
2015,neural machine translation jointly learning align translate,neural machine translation recently proposed approach machine translation unlike traditional statistical machine translation neural machine translation aim building single neural network jointly tuned maximize translation performance model proposed recently neural machine translation often belong family encoder-decoders consists encoder encodes source sentence fixed-length vector decoder generates translation paper conjecture use fixed-length vector bottleneck improving performance basic encoder-decoder architecture propose extend allowing model automatically search part source sentence relevant predicting target word without form part hard segment explicitly new approach achieve translation performance comparable existing state-of-the-art phrase-based system task english-to-french translation furthermore qualitative analysis reveals alignment found model agree well intuition
2015,reweighted wake-sleep,training deep directed graphical model many hidden variable performing inference remains major challenge helmholtz machine deep belief network model wake-sleep algorithm proposed train wake-sleep algorithm relies training directed generative model also conditional generative model  run backward visible latent estimating posterior distribution latent given visible propose novel interpretation wake-sleep algorithm suggests better estimator gradient obtained sampling latent variable multiple time inference network view based importance sampling estimator likelihood approximate inference network proposal distribution interpretation confirmed experimentally showing better likelihood achieved reweighted wake-sleep procedure based interpretation propose sigmoidal belief network sufficiently powerful layer inference network order recover good estimator posterior distribution latent variable experiment show using powerful layer model nade yield substantially better generative model
2015,low precision arithmetic deep learning,multiplier space power-hungry arithmetic operator digital implementation deep neural network train set state-of-the-art neural network  three benchmark datasets mnist cifar- svhn trained three distinct format floating point fixed point dynamic fixed point datasets format ass impact precision multiplication final error training find low precision sufficient running trained network also training example possible train maxout network  bit multiplication
2015,nice non-linear independent component estimation,propose deep learning framework modeling complex high-dimensional density called non-linear independent component estimation  based idea good representation one data distribution easy model purpose non-linear deterministic transformation data learned map latent space make transformed data conform factorized distribution ie resulting independent latent variable parametrize transformation computing jacobian determinant inverse transform trivial yet maintain ability learn complex non-linear transformation via composition simple building block based deep neural network training criterion simply exact log-likelihood tractable unbiased ancestral sampling also easy show approach yield good generative model four image datasets used inpainting
2015,embedding word similarity neural machine translation,neural language model learn word representation embeddings capture rich linguistic conceptual information investigate embeddings learned neural machine translation model recently-developed class neural language model show embeddings translation model outperform learned monolingual model task require knowledge conceptual similarity lexical-syntactic role show effect hold translating english french english german argue desirable property translation embeddings emerge largely independently source target language finally apply new method training neural translation model large vocabulary show vocabulary expansion algorithm  minimal degradation embedding quality embedding space queried online demo downloaded web page overall analysis indicate translation-based embeddings used application require concept organised according similarity and/or lexical function monolingual embeddings better suited modelling  inter-word relatedness
2015,target propagation,back-propagation workhorse recent success deep learning relies infinitesimal effect  order perform credit assignment could become serious issue one considers deeper non-linear function eg consider extreme case nonlinearity relation parameter cost actually discrete inspired biological implausibility back-propagation approach proposed past could play similar credit assignment role spirit explore novel approach credit assignment deep network call target propagation main idea compute target rather gradient layer like gradient propagated backwards way related different previously proposed proxy back-propagation rely backwards network symmetric weight target propagation relies auto-encoders layer unlike back-propagation applied even unit exchange stochastic bit rather real number show linear correction imperfectness auto-encoders called difference target propagation effective make target propagation actually work leading  comparable back-propagation deep network discrete continuous unit denoising auto-encoders achieving state art stochastic network
2015,ensemble generative discriminative technique sentiment analysis movie review,sentiment analysis common task natural language processing aim detect polarity text document  simplest setting discriminate positive negative sentiment turning task standard binary classification problem compare several ma- chine learning approach problem combine achieve best possible  show use task standard generative lan- guage model slightly complementary state art technique achieve strong  well-known dataset imdb movie review  easily reproducible publish also code needed repeat experiment simplify advance state art researcher combine technique little effort
2015,fitnets hint thin deep net,depth tends improve network performance also make gradient-based training difficult since deeper network tend non-linear recently proposed knowledge distillation approach aimed obtaining small fast-to-execute model shown student network could imitate soft output larger teacher network ensemble network paper extend idea allow training student deeper thinner teacher using output also intermediate representation learned teacher hint improve training process final performance student student intermediate hidden layer generally smaller teacher intermediate hidden layer additional parameter introduced map student hidden layer prediction teacher hidden layer allows one train deeper student generalize better run faster trade-off controlled chosen student capacity example cifar- deep student network almost  time le parameter outperforms larger state-of-the-art teacher network
2014,conditioning time representation long short-term memory network,dopaminergic model based temporal-difference learning algorithm usually differentiate trace delay conditioning instead use fixed temporal representation elapsed time since conditioned stimulus onset recently new model proposed timing learned within long short-term memory  artificial neural network representing cerebral cortex –  paper model’s ability reproduce explain relevant data well ability make interesting new prediction evaluated model reveals strikingly different temporal representation trace delay conditioning since trace conditioning requires working memory remember past conditioned stimulus delay conditioning hand model predicts important difference da response two condition trained one conditioning paradigm tested model predicts trace conditioning animal timing start conditioned stimulus offset opposed onset classical conditioning predicts conditioned stimulus disappear reward animal may expect second reward finally last simulation reveals buildup activity unit network adapt new delay adjusting rate integration importantly paper show possible proposed architecture acquire discharge pattern similar observed dopaminergic neuron cerebral cortex task simply minimizing predictive cost function
2014,regularized auto-encoders learn data-generating distribution,auto-encoders learn underlying data-generating distribution recent work suggests auto-encoder variant good job capturing local manifold structure data paper clarifies previous observation showing minimizing particular form regularized reconstruction error yield reconstruction function locally characterizes shape data-generating density show auto-encoder capture score  contradicts previous interpretation reconstruction error energy function unlike previous  theorem provided completely generic depend parameterization auto-encoder show auto-encoder would tend given enough capacity example  contractive training criterion show similar denoising auto-encoder training criterion small corruption noise contraction applied whole reconstruction function rather encoder similarly score matching one consider proposed training criterion convenient alternative maximum likelihood involve partition function finally show approximate metropolis-hastings mcmc setup recover sample estimated distribution confirmed sampling experiment
2014,semantic matching energy function learning multi-relational data - application word-sense disambiguation,large-scale relational learning becomes crucial handling huge amount structured data generated daily many application domain ranging computational biology information retrieval natural language processing paper present new neural network architecture designed embed multi-relational graph flexible continuous vector space original data kept enhanced network trained encode semantics graph order assign high probability plausible component empirically show reach competitive performance link prediction standard datasets literature well data real-world knowledge base  addition present method applied perform word-sense disambiguation context open-text semantic parsing goal learn assign structured meaning representation almost sentence free text demonstrating scale ten thousand node thousand type relation
2014,learning semantic representation object part,recently large scale image annotation datasets collected million image thousand possible annotation latent variable model embedding method simultaneously learn semantic representation object label image representation provide tractable solution task work interested jointly learning representation object image part object deeper semantic representation could bring leap forward image retrieval browsing despite size datasets amount annotated data object part costly may available paper propose bypass cost method able learn jointly label object part without requiring exhaustively labeled data design model architecture trained proxy supervision obtained combining standard image annotation  semantic part-based within-label relation  model designed model object image object label similarity object label object part label similarity single joint system experiment conducted combined data precisely annotated evaluation set demonstrate usefulness approach
2014,spike-and-slab rbm extension discrete sparse data distribution,spike-and-slab restricted boltzmann machine  defined real-valued “slab” variable binary “spike” variable associated unit hidden layer model us slab variable model conditional covariance observation-thought important capturing statistical property natural image paper present canonical ssrbm framework together extension extension highlight flexibility spike-and-slab rbm platform exploring sophisticated probabilistic model high dimensional data general natural image data particular introduce subspace-ssrbm focused task learning invariant feature highlight behaviour ssrbm extension experiment mnist digit recognition task cifar- object classification task
2014,challenge physical implementation rbms,restricted boltzmann machine  powerful machine learning model learning kind inference model require sampling-based approximation classical digital computer implemented using expensive mcmc physical computation offer opportunity reduce costof sampling building physical system whose natural dynamic correspond drawing sample desired rbm distribution system avoids burn-in mixing cost markov chain however hardware implementation variety usually entail limitation low-precision limited range parameter restriction size topology rbm conduct software simulation determine harmful restriction simulation based d-wave two computer issue investigate arise form physical computationour finding suggest designer new physical computing hardware algorithm physical computer focus effort overcoming limitation imposed topology restriction currently existing physical computer
2014,learning concept embeddings query expansion quantum entropy minimization,web search user query formulated using term term-matching retrieval function could fail retrieving relevant document given user query technique query expansion  consists selecting related term could enhance likelihood retrieving relevant document selecting expansion term challenging requires computational framework capable encoding complex semantic relationship paper propose novel method learning supervised way semantic representation word phrase embedding query document special matrix model disposes increased representational power respect existing approach adopting vector representation show model produce high-quality query expansion term expansion increase ir measure beyond expansion current word-embeddings model well-established traditional qe method
2014,learning phrase representation using rnn encoder-decoder statistical machine translation,paper propose novel neural network model called rnn encoderdecoder consists two recurrent neural network  one rnn encodes sequence symbol fixedlength vector representation decodes representation another sequence symbol encoder decoder proposed model jointly trained maximize conditional probability target sequence given source sequence performance statistical machine translation system empirically found improve using conditional probability phrase pair computed rnn encoder-decoder additional feature existing log-linear model qualitatively show proposed model learns semantically syntactically meaningful representation linguistic phrase
2014,deep learning cultural evolution,propose theory first experimental test relating difficulty learning deep architecture culture language theory articulated around following hypothesis learning individual human brain hampered presence effective local minimum particularly come learning higher-level abstraction represented composition many level representation ie deep architecture human brain learn high-level abstraction guided signal produced human act hint intermediate higher-level abstraction language recombination optimization mental concept provide efficient evolutionary recombination operator purpose theory grounded experimental observation difficulty training deep artificial neural network empirical test hypothesis regarding need guidance intermediate concept demonstrated done learning task tested machine learning algorithm failed unless provided hint intermediate-level abstraction
2014,deep generative stochastic network trainable backprop,introduce novel training principle probabilistic model alternative maximum likelihood proposed generative stochastic network  framework based learning transition operator markov chain whose stationary distribution estimate data distribution transition distribution conditional distribution generally involving small move fewer dominant mode unimodal limit small move thus easier learn like learning perform supervised function approximation gradient obtained backprop theorem provided generalize recent work probabilistic interpretation denoising autoencoders provide interesting justification dependency network generalized pseudolikelihood  gsns used missing input used sample subset variable given rest successful experiment conducted validating theoretical  two image datasets particular architecture mimic deep boltzmann machine gibbs sampler allows training proceed backprop without need layerwise pretraining
2014,marginalized denoising auto-encoders nonlinear representation,denoising auto-encoders  successfully used learn new representation wide range machine learning task training daes make many pass training dataset reconstruct partial corruption generated pre-specified corrupting distribution process learns robust representation though expense requiring many training epoch data explicitly corrupted paper present marginalized denoising auto-encoder   marginalizes corruption training effectively mdae take account infinitely many corrupted copy training data every epoch therefore able match outperform dae much fewer training epoch analyze proposed algorithm show understood classic auto-encoder special form regularization empirical evaluation show attains - order-of-magnitude speedup training time competing approach
2014,scaling deep learning,deep learning rapidly moved marginal approach machine learning community le ten year ago one strong industrial impact particular high-dimensional perceptual data speech image also natural language demand expert deep learning growing fast  thereby considerably increasing market value deep learning based idea learning multiple level representation higher level computed function lower level corresponding abstract concept automatically discovered learner deep learning arose research artificial neural network graphical model literature subject considerably grown recent year culminating creation dedicated conference  tutorial introduce basic algorithm supervised unsupervised side well discus guideline successfully using practice finally introduce current research question regarding challenge scaling deep learning much larger model successfully extract information huge datasets
2014,iterative neural autoregressive distribution estimator nade-k,training neural autoregressive density estimator  viewed one step probabilistic inference missing value data propose new model extends inference scheme multiple step arguing easier learn improve reconstruction ksteps rather learn reconstruct single inference step proposed model unsupervised building block deep learning combine desirable property nade multi-predictive training  test likelihood computed analytically  easy generate independent sample  us inference engine superset variational inference boltzmann machine proposed nade-k competitive state-of-the-art density estimation two datasets tested
2014,generative adversarial net,propose new framework estimating generative model via adversarial net simultaneously train two model generative model g capture data distribution discriminative model estimate probability sample came training data rather g training procedure g maximize probability making mistake framework corresponds minimax two-player game space arbitrary function g unique solution exists g recovering training data distribution equal / everywhere case g defined multilayer perceptrons entire system trained backpropagation need markov chain unrolled approximate inference network either training generation sample experiment demonstrate potential framework qualitative quantitatively evaluation generated sample
2014,number linear region deep neural network,study complexity function computable deep feedforward neural network piecewise linear activation term symmetry number linear region deep network able sequentially map portion layer input-space output way deep model compute function react equally complicated pattern different input compositional structure function enables re-use piece computation exponentially often term network depth paper investigates complexity compositional map contributes new theoretical  regarding advantage depth neural network piecewise linear activation function particular analysis specific single family model example employ rectifier maxout network improve complexity bound pre-existing work investigate behavior unit higher layer
2014,identifying attacking saddle point problem high-dimensional non-convex optimization,central challenge many field science engineering involves minimizing non-convex error function continuous high dimensional space gradient descent quasi-newton method almost ubiquitously used perform minimization often thought main source difficulty local method find global minimum proliferation local minimum much higher error global minimum argue based  statistical physic random matrix theory neural network theory empirical evidence deeper profound difficulty originates proliferation saddle point local minimum especially high dimensional problem practical interest saddle point surrounded high error plateau dramatically slow learning give illusory impression existence local minimum motivated argument propose new approach second-order optimization saddle-free newton method rapidly escape high dimensional saddle point unlike gradient descent quasi-newton method apply algorithm deep recurrent neural network training provide numerical evidence superior optimization performance
2014,transferable feature deep neural network,many deep neural network trained natural image exhibit curious phenomenon common first layer learn feature similar gabor filter color blob first-layer feature appear specific particular dataset task general applicable many datasets task feature must eventually transition general specific last layer network transition studied extensively paper experimentally quantify generality versus specificity neuron layer deep convolutional neural network report surprising  transferability negatively affected two distinct issue  specialization higher layer neuron original task expense performance target task expected  optimization difficulty related splitting network co-adapted neuron expected example network trained imagenet demonstrate either two issue may dominate depending whether feature transferred bottom middle top network also document transferability feature decrease distance base task target task increase transferring feature even distant task better using random feature final surprising result initializing network transferred feature almost number layer produce boost generalization lingers even fine-tuning target dataset
2014,equivalence deep nade generative stochastic network,neural autoregressive distribution estimator  recently shown successful alternative modeling high dimensional multimodal distribution one issue associated nades rely particular order factorization p issue recently addressed variant nade called orderless nades deeper version deep orderless nade orderless nades trained based criterion stochastically maximizes p possible order factorization unfortunately ancestral sampling deep nade expensive corresponding running neural net separately predicting visible variable given others work make connection criterion training criterion generative stochastic network  show training nades way also train gsn defines markov chain associated nade model based connection show alternative way sample trained orderless nade allows trade-off computing time quality sample  -fold speedup  obtained without noticeably reducing quality sample achieved using novel sampling procedure gsns called annealed gsn sampling similar tempering method combine fast mixing  accurate sample 
2014,learned-norm pooling deep feedforward recurrent neural network,paper propose investigate novel nonlinear unit called l p unit deep neural network proposed l p unit receives signal several projection subset unit layer computes normalized l p norm notice two interesting interpretation l p unit first proposed unit understood generalization number conventional pooling operator average root-mean-square max pooling widely used instance convolutional neural network  hmax model neocognitrons furthermore l p unit certain degree similar recently proposed maxout unit  achieved state-of-the-art object recognition  number benchmark datasets secondly provide geometrical interpretation activation function based argue l p unit efficient representing complex nonlinear separating boundary l p unit defines superelliptic boundary exact shape defined order p claim make possible model arbitrarily shaped curved boundary efficiently combining l p unit different order insight justifies need learning different order unit model empirically evaluate proposed l p unit number datasets show multilayer perceptrons  consisting l p unit achieve state-of-the-art  number benchmark datasets furthermore evaluate proposed l p unit recently proposed deep recurrent neural network 
2014,overcoming curse sentence length neural machine translation using automatic segmentation,author  shown recently introduced neural network translation system suffer significant drop translation quality translating long sentence unlike existing phrase-based translation system paper propose way address issue automatically segmenting input sentence phrase easily translated neural network translation model segment independently translated neural machine translation model translated clause concatenated form final translation empirical  show significant improvement translation quality long sentence
2014,property neural machine translation encoder-decoder approach,neural machine translation relatively new approach statistical machine translation based purely neural network neural machine translation model often consist encoder decoder encoder extract fixed-length representation variable-length input sentence decoder generates correct translation representation paper focus analyzing property neural machine translation using two model rnn encoder--decoder newly proposed gated recursive convolutional neural network show neural machine translation performs relatively well short sentence without unknown word performance degrades rapidly length sentence number unknown word increase furthermore find proposed gated recursive convolutional network learns grammatical structure sentence automatically
2014,bounding test log-likelihood generative model,several interesting generative learning algorithm involve complex probability distribution many random variable involving intractable normalization constant latent variable normalization may even analytic expression unnormalized probability function tractable approximation make difficult estimate quality model trained monitor quality  training previously proposed method based constructing non-parametric density estimator model probability function sample generated model revisit idea propose efficient estimator prove provides lower bound true test log-likelihood unbiased estimator number generated sample go infinity although one incorporates effect poor mixing propose biased variant estimator used reliably finite number sample purpose model comparison
2014,empirical investigation catastrophic forgeting gradient-based neural network,catastrophic forgetting problem faced many machine learning model algorithm trained one task trained second task many machine learning model forget perform first task widely believed serious problem neural network investigate extent catastrophic forgetting problem occurs modern neural network comparing established recent gradient-based training algorithm activation function also examine effect relationship first task second task catastrophic forgetting find always best train using dropout algorithm--the dropout algorithm consistently best adapting new task remembering old task best tradeoff curve two extreme find different task relationship task result different ranking activation function performance suggests choice activation function always cross-validated
2014,multimodal transition generative stochastic network,generative stochastic network  recently introduced alternative traditional probabilistic modeling instead parametrizing data distribution directly one parametrizes transition operator markov chain whose stationary distribution estimator data generating distribution result training therefore machine generates sample markov chain however previously introduced gsn consistency theorem suggest order capture wide class distribution transition operator general multimodal something done paper introduce first time multimodal transition distribution gsns particular using model nade family  output distribution transition operator nade model related rbm  likelihood  computed easily parameter nade obtained learned function previous state learned markov chain experiment clearly illustrate advantage multimodal transition distribution unimodal gsns
2014,construct deep recurrent neural network,paper explore different way extend recurrent neural network  {deep} rnn start arguing concept depth rnn clear feedforward neural network carefully analyzing understanding architecture rnn however find three point rnn may made deeper  input-to-hidden function  hidden-to-hidden transition  hidden-to-output function based observation propose two novel architecture deep rnn orthogonal earlier attempt stacking multiple recurrent layer build deep rnn  provide alternative interpretation deep rnns using novel framework based neural operator proposed deep rnns empirically evaluated task polyphonic music prediction language modeling experimental result support claim proposed deep rnns benefit depth outperform conventional shallow rnns
2014,number inference region deep feed forward network piece-wise linear activation,paper explores complexity deep feedforward network linear pre-synaptic coupling rectified linear activation contribution growing body work contrasting representational power deep shallow network architecture particular offer framework comparing deep shallow model belong family piecewise linear function based computational geometry look deep rectifier multi-layer perceptron  linear output unit compare single layer version model asymptotic regime number input stay constant shallow model hasknhidden unit andninputs number linear region isoknnn aklayer model withnhidden unit layer isω⌊n/n⌋k−nn number⌊n/n⌋k−grows faster thanknwhenntends infinity whenktends infinity andn≥n additionally even whenkis small restrictnto ben show deep model considerably linear region shallow one consider first step towards understanding complexity model specifically towards providing suitable mathematical tool future analysis
2014,empirical analysis dropout piecewise linear network,recently introduced dropout training criterion neural network subject much attention due simplicity remarkable effectiveness regularizer well interpretation training procedure exponentially large ensemble network share parameter work empirically investigate several question related efficacy dropout specifically concern network employing popular rectified linear activation function investigate quality test time weight-scaling inference procedure evaluating geometric average exactly small model well compare performance geometric mean arithmetic mean commonly employed ensemble technique explore effect tied weight ensemble interpretation training ensemble masked network without tied weight finally investigate alternative criterion based biased estimator maximum likelihood ensemble gradient
2014,revisiting natural gradient deep network,evaluate natural gradient algorithm originally proposed amari  learning deep model contribution paper follows show connection natural gradient three recently proposed method training deep model hessian-free  krylov subspace descent  tonga  describe one use unlabeled data improve generalization error obtained natural gradient empirically evaluate robustness algorithm ordering training set compared stochastic gradient descent finally extend natural gradient incorporate second order information alongside manifold information provide benchmark new algorithm using truncated newton approach inverting metric matrix instead using diagonal approximation
2013,learning deep physiological model affect,feature extraction feature selection crucial phase process affective modeling however incorporate substantial limitation hinder development reliable accurate model affect purpose modeling affect manifested physiology paper build recent advance machine learning deep learning  approach efficiency dl algorithm train artificial neural network model tested compared standard feature extraction selection approach followed literature  game data corpus - containing player physiological signal  subjective self-reports affect - reveal dl outperforms manual ad-hoc feature extraction yield significantly accurate affective model moreover appears dl meet even outperforms affective model boosted automatic feature selection several scenario examined dl method generic applicable affective modeling task key finding paper suggest ad-hoc feature extraction selection - lesser degree - could bypassed
2013,representation learning review new perspective,success machine learning algorithm generally depends data representation hypothesize different representation entangle hide le different explanatory factor variation behind data although specific domain knowledge used help design representation learning generic prior also used quest ai motivating design powerful representation-learning algorithm implementing prior paper review recent work area unsupervised feature learning deep learning covering advance probabilistic model autoencoders manifold learning deep network motivates longer term unanswered question appropriate objective learning good representation computing representation  geometrical connection representation learning density estimation manifold learning
2013,scaling spike-and-slab model unsupervised feature learning,describe use two spike-and-slab model modeling real-valued data emphasis application object recognition first model call spike-and-slab sparse coding  preexisting model introduce faster approximate inference algorithm introduce deep variant sc call partially directed deep boltzmann machine  extend sc inference algorithm use model describe learning procedure demonstrate inference procedure sc enables scaling model unprecedented large problem size demonstrate using sc feature extractor  good object recognition performance particularly number labeled example low show pd-dbm generates better sample shallow counterpart unlike dbms dbns pd-dbm may trained successfully without greedy layerwise training
2013,texture modeling convolutional spike-and-slab rbms deep extension,apply spike-and-slab restricted boltzmann machine  texture modeling ssrbm tiled-convolution weight sharing  achieves surpasses state-of-the-art texture synthesis inpainting parametric model also develop novel rbm model spike-and-slab visible layer binary variable hidden layer model designed stacked top ssrbm show resulting deep belief network  powerful generative model improves single-layer model capable modeling single high-resolution challenging texture also multiple texture fixed-size filter bottom layer
2013,stacked calibration off-policy policy evaluation video game matchmaking,consider industrial strength application recommendation system video-game matchmaking off-policy policy evaluation important standard approach hardly applied objective policy sequentially form team player waiting matched way produce well-balanced match unfortunately available training data come policy known perfectly stochastic making impossible use method based importance weight furthermore observe estimated reward function policy obtained training off-policy dataset policy evaluation using estimated reward function biased present simple calibration procedure similar stacked regression remove bias experiment performed data collected beta test ghost recon online first person shooter ubisoft used experiment
2013,high-dimensional sequence transduction,investigate problem transforming input sequence high-dimensional output sequence order transcribe polyphonic audio music symbolic notation introduce probabilistic model based recurrent neural network able learn realistic output distribution given input devise efficient algorithm search global mode distribution resulting method produce musically plausible transcription even high level noise drastically outperforms previous state-of- the-art approach five datasets synthesized sound real recording approximately halving test error rate
2013,advance optimizing recurrent network,decade-long period relatively little research activity area recurrent neural network several new development reviewed allowed substantial progress understanding technical solution towards efficient training recurrent network advance motivated related optimization issue surrounding deep learning although recurrent network extremely powerful principle represent term modeling sequence training plagued two aspect issue regarding learning long-term dependency experiment reported evaluate use clipping gradient spanning longer time range leaky integration advanced momentum technique using powerful output probability model encouraging sparser gradient help symmetry breaking credit assignment experiment performed text music data show combined effect technique generally improving training test error
2013,combining modality specific deep neural network emotion recognition video,paper present technique used university montréals team submission  emotion recognition wild challenge challenge classify emotion expressed primary human subject short video clip extracted feature length movie involves analysis video clip acted scene lasting approximately one-two second including audio track may contain human voice well  music approach combine multiple deep neural network different data modality including  deep convolutional neural network analysis facial expression within video frame  deep belief net capture audio information  deep autoencoder model spatio-temporal information produced human action depicted within entire scene  shallow network architecture focused extracted feature mouth primary human subject scene discus technique performance characteristic different strategy aggregate prediction best single model convolutional neural network trained predict emotion static frame using two large data set toronto face database set face image harvested google image search followed per frame aggregation strategy used challenge training data yielded test set accuracy  using best strategy aggregating top performing model single predictor able produce accuracy  challenge test set compare favorably challenge baseline test set accuracy 
2013,better mixing via deep representation,hypothesized supported experimental evidence deeper representation well trained tend better job disentangling underlying factor variation study following related conjecture better representation sense better disentangling exploited produce markov chain mix faster mode consequently mixing mode would efficient higher level representation better understand propose secondary conjecture higher-level sample fill uniformly space occupy high-density manifold tend unfold represented higher level paper discus hypothesis test experimentally visualization measurement mixing mode interpolating sample
2013,difficulty training recurrent neural network,two widely known issue properly training recurrent neural network vanishing exploding gradient problem detailed bengio et al  paper attempt improve understanding underlying issue exploring problem analytical geometric dynamical system perspective analysis used justify simple yet effective solution propose gradient norm clipping strategy deal exploding gradient soft constraint vanishing gradient problem validate empirically hypothesis proposed solution experimental section
2013,maxout network,consider problem designing model leverage recently introduced approximate model averaging technique called dropout define simple new model called maxout  designed facilitate optimization dropout improve accuracy dropout fast approximate model averaging technique empirically verify model successfully accomplishes task use maxout dropout demonstrate state art classification performance four benchmark datasets mnist cifar- cifar- svhn
2013,unsupervised learning semantics object detection scene categorization,classifying scene  important complicated task nowadays image come variability ambiguity wide range illumination scale condition standard approach build intermediate representation global image learn classifier recently proposed depict image aggregation contained object  representation classifier trained composed many heterogeneous feature vector derived various object detector paper propose study different approach efficiently learn contextual semantics object detection use feature provided object-bank   show several benchmark scene categorization careful combination taking account structure data allows greatly improve original  from++to++ drastically reducing dimensionality representation  %  also show uncertainty relative object detector hamper use external semantic knowledge improve detector combination unlike unsupervised learning approach
2013,unsupervised transfer learning uncertainty - object detection scene categorization,classifying scene  important complicated task nowadays image come variability ambiguity wide range illumination scale condition standard approach build intermediate representation global image learn classifier recently proposed depict image aggregation contained object representation classifier trained composed many heterogeneous approach efficiently combine data extracted detector use feature provided object-bank drastically reducing dimensionality representation 
2013,investigation recurrent-neural-network architecture learning method spoken language understanding,one key problem spoken language understanding  task slot filling light recent success applying deep neural network technology domain detection intent identification carried in-depth investigation use recurrent neural network difficult task slot filling involving sequence discrimination work implemented compared several important recurrent-neural-network architecture including elman-type jordan-type recurrent network variant make  easy reproduce compare implemented network common theano neural network toolkit evaluated atis benchmark also compared  conditional random field  baseline  show task type recurrent network outperform crf baseline substantially bi-directional jordan-type network take account past future dependency among slot work best outperforming crf-based baseline  relative error reduction
2013,audio chord recognition recurrent neural network,paper present audio chord recognition system based recurrent neural network audio feature obtained deep neural network optimized combination chromagram target chord information aggregated different time scale contrarily existing approach system incorporates acoustic musicological model single training objective devise efficient algorithm search global mode output distribution taking long-term dependency account resulting method competitive state-of-the-art approach mirex dataset major/minor prediction task
2013,multi-prediction deep boltzmann machine,introduce multi-prediction deep boltzmann machine  mp-dbm seen single probabilistic model trained maximize variational approximation generalized pseudolikelihood family recurrent net share parameter approximately solve different inference problem prior method training dbms either perform well classification task require initial learning pas train dbm greedily one layer time mp-dbm require greedy layerwise pretraining outperforms standard dbm classification classification missing input mean field prediction task
2013,generalized denoising auto-encoders generative model,recent work shown denoising contractive autoencoders implicitly capture structure data generating density case corruption noise gaussian reconstruction error squared error data continuous-valued led various proposal sampling implicitly learned density function using langevin metropolis-hastings mcmc however remained unclear connect training procedure regularized auto-encoders implicit estimation underlying data generating distribution data discrete using form corruption process reconstruction error another issue mathematical justification valid limit small corruption noise propose different attack problem deal issue arbitrary  corruption arbitrary reconstruction loss  handling discrete continuous-valued variable removing bias due non-infinitesimal corruption noise 
2013,stochastic ratio matching rbms sparse high-dimensional input,sparse high-dimensional data vector common many application domain large number rarely non-zero feature devised unfortunately creates computational bottleneck unsupervised feature learning algorithm based auto-encoders rbms involve reconstruction step whole input vector predicted current feature value algorithm recently developed successfully handle case auto-encoders based importance sampling scheme stochastically selecting input element actually reconstruct training particular example generalize idea rbms propose stochastic ratio-matching algorithm inherits computational advantage unbiasedness importance sampling scheme show stochastic ratio matching good estimator allowing approach beat state-of-the-art two bag-of-word text classification benchmark  keeping computational cost linear number non-zeros
2013,modeling term dependency quantum language model ir,traditional information retrieval  model use bag-of-words basic representation assume form independence hold term representing term dependency defining scoring function capable integrating additional evidence theoretically practically challenging recently quantum theory  proposed possible general framework ir however limited number investigation made potential qt fully explored tested develop new generalized language modeling approach ir adopting probabilistic framework qt particular quantum probability could account single compound term without extend term space artificially previous study naturally allows u avoid weight-normalization problem arises current practice mixing score matching compound term matching single term model first practical application quantum probability show significant improvement robust bag-of-words baseline achieves better performance stronger non bag-of-words baseline
2013,deep learning representation looking forward,deep learning research aim discovering learning algorithm discover multiple level distributed representation higher level representing abstract concept although study deep learning already led impressive theoretical  learning algorithm breakthrough experiment several challenge lie ahead paper proposes examine challenge centering question scaling deep learning algorithm much larger model datasets reducing optimization difficulty due ill-conditioning local minimum designing efficient powerful inference sampling procedure learning disentangle factor variation underlying observed data also proposes forward-looking research direction aimed overcoming challenge
2013,regularized auto-encoders estimate local statistic,auto-encoders learn underlying data generating distribution recent work suggests auto-encoder variant good job capturing local manifold structure data paper clarifies previous observation showing minimizing particular form regularized reconstruction error yield reconstruction function locally characterizes shape data generating density show auto-encoder capture score  contradicts previous interpretation reconstruction error energy function unlike previous  theorem provided completely generic depend parametrization auto-encoder show auto-encoder would tend given enough capacity example  contractive training criterion show similar denoising auto-encoder training criterion small corruption noise contraction applied whole reconstruction function rather encoder similarly score matching one consider proposed training criterion convenient alternative maximum likelihood involve partition function finally show approximate metropolis-hastings mcmc setup recover sample estimated distribution confirmed sampling experiment
2013,semantic matching energy function learning multi-relational data,large-scale relational learning becomes crucial handling huge amount structured data generated daily many application domain ranging computational biology information retrieval natural language processing paper present new neural network architecture designed embed multi-relational graph flexible continuous vector space original data kept enhanced network trained encode semantics graph order assign high probability plausible component empirically show reach competitive performance link prediction standard datasets literature
2013,metric-free natural gradient joint-training boltzmann machine,paper introduces metric-free natural gradient  algorithm training boltzmann machine similar spirit hessian-free method marten  algorithm belongs family truncated newton method exploit efficient matrix-vector product avoid explicitely storing natural gradient metricl metric shown expected second derivative log-partition function  equivalently variance vector partial derivative energy function evaluate method task joint-training -layer deep boltzmann machine show mfng indeed faster per-epoch convergence compared stochastic maximum likelihood centering though wall-clock performance currently competitive
2013,joint training deep boltzmann machine classification,introduce new method training deep boltzmann machine jointly prior method training dbms require initial learning pas train model greedily one layer time perform well classification task approach train layer dbm simultaneously using novel training procedure called multi-prediction training resulting model either interpreted single generative model trained maximize variational approximation generalized pseudolikelihood family recurrent network share parameter may approximately averaged together using novel technique call multi-inference trick show approach performs competitively classification outperforms previous method term accuracy approximate inference classification missing input
2013,big neural network waste capacity,article expose failure big neural network leverage added capacity reduce underfitting past research suggest diminishing return increasing size neural network experiment imagenet lsvrc- show may due fact highly diminishing return capacity term training error leading underfitting suggests optimization method - first order gradient descent - fails regime directly attacking problem either optimization method choice parametrization may allow improve generalization error large datasets large capacity required
2013,natural gradient revisited,aim paper three-fold first show hessian-free  krylov subspace descent  described implementation natural gradient descent due use extended gauss-newton approximation hessian secondly re-derive natural gradient basic principle contrasting difference two version algorithm found neural network literature well highlighting difference natural gradient typical second order method lastly show empirically natural gradient robust overfitting particularly robust order training data presented model
2012,detonation classification acoustic signature restricted boltzmann machine,compare recently proposed discriminative restricted boltzmann machine  classical support vector machine  challenging classification task consisting identifying weapon class audio signal three weapon class considered work  difficult reliably classify standard technique tend similar acoustic signature addition specificity data available study make challenging rigorously compare classifier address methodological issue arising situation experiment show good classification accuracy could make technique suitable fielding autonomous device drbms appear yield better accuracy svms le sensitive choice signal preprocessing model hyperparameters last property especially appealing task lack data make model validation difficult
2012,random search hyper-parameter optimization,grid search manual search widely used strategy hyper-parameter optimization paper show empirically theoretically randomly chosen trial efficient hyper-parameter optimization trial grid empirical evidence come comparison large previous study used grid search manual search configure neural network deep belief network compared neural network configured pure grid search find random search domain able find model good better within small fraction computation time granting random search computational budget random search find better model effectively searching larger le promising configuration space compared deep belief network configured thoughtful combination manual search grid search purely random search -dimensional configuration space found statistically equal performance four seven data set superior performance one seven gaussian process analysis function hyper-parameters validation set performance reveals data set hyper-parameters really matter different hyper-parameters important different data set phenomenon make grid search poor choice configuring algorithm new data set analysis cast light recent high throughput method achieve surprising success--they appear search large number hyper-parameters hyper-parameters matter much anticipate growing interest large hierarchical model place increasing burden technique hyper-parameter optimization work show random search natural baseline judge progress development adaptive  hyper-parameter optimization algorithm
2012,learning algorithm classification restricted boltzmann machine,recent development demonstrated capacity restricted boltzmann machine  powerful generative model able extract useful feature input data construct deep artificial neural network setting rbm yield preprocessing initialization model instead acting complete supervised model right paper argue rbms provide self-contained framework developing competitive classifier study classification rbm  variant rbm adapted classification setting study different strategy training classrbm show competitive classification performance reached appropriately combining discriminative generative training objective since training according generative objective requires computation generally intractable gradient also compare different approach estimating gradient address issue obtaining gradient problem high dimensional input finally describe adapt classrbm two special case classification problem namely semi-supervised multitask learning
2012,beyond skill rating advanced matchmaking ghost recon online,player satisfaction particularly difficult ensure online game due interaction player adversarial multiplayer game matchmaking typically consists trying match together player similar skill level however usually based single-skill value assumes factor “fun” game balance present advanced matchmaking strategy developed ghost recon online upcoming team-focused first-person shooter  ubisoft  first show incorporating information player raw skill lead balanced match also argue balance factor matter present strategy explicitly maximize player fun taking advantage rich player profile includes information player behavior personal preference ultimately goal ask player provide direct feedback match quality in-game survey however data available study rely heuristic tailored specific game experiment data collected ghost recon onlines beta test show neural network effectively used predict balance player enjoyment
2012,disentangling factor variation facial expression recognition,propose semi-supervised approach solve task emotion recognition face image using recent idea deep learning handling factor variation present data emotion classification algorithm robust  remaining variation due pose face image centering alignment  identity morphology face order achieve invariance propose learn hierarchy feature gradually filter factor variation arising   address  using multi-scale contractive convolutional network  order obtain invariance translation facial trait image using feature representation produced ccnet train contractive discriminative analysis  feature extractor novel variant contractive auto-encoder  designed learn representation separating emotion-related factor others  system beat state-of-the-art recently proposed dataset facial expression recognition toronto face database moving state-of-art accuracy   ccnet cda improve accuracy standard cae 
2012,modeling temporal dependency high-dimensional sequence application polyphonic music generation transcription,investigate problem modeling symbolic sequence polyphonic music completely general piano-roll representation introduce probabilistic model based distribution estimator conditioned recurrent neural network able discover temporal dependency high-dimensional sequence approach outperforms many traditional model polyphonic music variety realistic datasets show musical language model serve symbolic prior improve accuracy polyphonic transcription
2012,large-scale feature learning spike-and-slab sparse coding,consider problem object recognition large number class order overcome low amount labeled example available setting introduce new feature learning extraction procedure based factor model call spike-and-slab sparse coding  prior work sc prioritized ability exploit parallel architecture scale sc enormous problem size needed object recognition present novel inference procedure appropriate use gpus allows u dramatically increase training set size amount latent factor sc may trained demonstrate approach improves upon supervised learning capability sparse coding spike-and-slab restricted boltzmann machine  cifar- dataset use cifar- dataset demonstrate method scale large number class better previous method finally use method win nip  workshop challenge learning hierarchical model transfer learning challenge
2012,generative process contractive auto-encoders,contractive auto-encoder learns representation input data capture local manifold structure around data point leading singular vector jacobian transformation input representation corresponding singular value specify much local variation plausible direction associated corresponding singular vector remaining high-density region input space paper proposes procedure generating sample consistent local structure captured contractive auto-encoder associated stochastic process defines distribution one sample experimentally appears converge quickly mix well mode compared restricted boltzmann machine deep belief network intuition behind procedure also used train second layer contraction pool lower-level feature learns invariant local direction variation discovered first layer show help learn represent invariance present data improve classification error
2012,discriminative non-negative matrix factorization multiple pitch estimation,paper present supervised method improve multiple pitch estimation accuracy non-negative matrix factorization  algorithm idea extend sparse nmf framework incorporating pitch information present time-aligned musical score order extract feature enforce separability pitch label introduce two discriminative criterion maximize inter-class scatter quantify predictive potential given decomposition using logistic regressors criterion applied latent variable deterministic autoencoder view nmf devise efficient update rule evaluate method three polyphonic datasets piano recording orchestral instrument mix model greatly enhance quality basis spectrum learned nmf accuracy multiple pitch estimation
2012,building musically-relevant audio feature multiple timescale representation,low-level aspect music audio timbre loudness pitch relatively well modelled feature extracted short-time window higher-level aspect melody harmony phrasing rhythm hand salient larger timescales require better representation time dynamic various music information retrieval task one would benefit modelling low high level aspect unified feature extraction framework combining adaptive feature computed different timescales short-timescale event put context detecting longer timescale feature paper describe method obtain multi-scale feature evaluate effectiveness automatic tag annotation 
2012,deep learning representation unsupervised transfer learning,deep learning algorithm seek exploit unknown structure input distribution order discover good representation often multiple level higher-level learned feature defined term lower-level feature objective make higher-level representation abstract individual feature invariant variation typically present training distribution collectively preserving much possible information input ideally would like representation disentangle unknown factor variation underlie training distribution unsupervised learning representation exploited usefully hypothesis input distributionppis structurally related task interest say predictingpp paper focus context unsupervised transfer learning challenge unsupervised pre-training representation useful exploited transfer learning scenario care prediction example distribution training distribution
2012,unsupervised transfer learning challenge deep learning approach,learning good representation large set unlabeled data particularly challenging task recent work  review show training deep architecture good way extract representation extracting disentangling gradually higher-level factor variation characterizing input distribution paper describe different kind layer trained learning representation setting unsupervised transfer learning challenge strategy team final phase challenge combined stacked different one-layer unsupervised learning algorithm adapted five datasets competition paper describes strategy particular one-layer learning algorithm feeding simple linear classifier tiny number labeled training sample 
2012,joint learning word meaning representation open-text semantic parsing,open-text semantic parser designed interpret statement natural language inferring corresponding meaning representation  unfortunately large scale system cannot easily machine-learned due lack directly supervised data propose method learns assign mr wide range text  thanks training scheme combine learning knowledge base  learning raw text model jointly learns representation word entity mr via multi-task training process operating diverse source data hence system end providing method knowledge acquisition word-sense disambiguation within context semantic parsing single elegant framework experiment various task indicate promise approach
2011,suitability v1 energy model object classification,simulation cortical computation often focused network built simplified neuron model similar rate model hypothesized v simple cell however physiological research revealed even v simple cell surprising complexity computational simulation explore effect complexity visual system ability solve simple task categorization shape digit learning limited number example use recently proposed high-throughput methodology explore ax modeling complexity useful categorization task find complex cell rate model learn categorize object better simple cell model without incurring extra computational expense find squaring linear filter response lead better performance find several component physiologically derived model yield better performance
2011,quickly generating representative sample rbm-derived process,two recently proposed learning algorithm herding fast persistent contrastive divergence  share following interesting characteristic exploit change model parameter sampling order escape mode mix better sampling process part learning algorithm justify approach way escape mode keeping approximately asymptotic distribution markov chain spirit extend fpcd using idea borrowed herding order obtain pure sampling algorithm call rates-fpcd sampler interestingly sampler improve model collect sample since optimizes lower bound log likelihood training data provide empirical evidence new algorithm display substantially better robust mixing gibbs sampling
2011,contextual tag inference,article examines use two kind context improve  content-based music tagger relationship tag clip song tagged show user agree tag applied clip temporally closer one another conditional restricted boltzmann machine model tag accurately predict related tag take context account training data smoothed using context support vector machine better rank clip according original unsmoothed tag accurately three standard multi-label classifier
2011,learning structured embeddings knowledge base,many knowledge base  readily available encompass colossal quantity information thanks either long-term funding effort  collaborative process  however based different rigorous symbolic framework make hard use data system unfortunate rich structured knowledge might lead huge leap forward many area ai like nat- ural language processing  vision  collaborative filtering paper present learning process based innovative neural network architecture designed embed symbolic representation flexible continuous vector space original knowledge kept enhanced learnt embeddings would allow data kb easily used recent machine learning meth- od prediction information retrieval illustrate method wordnet freebase also present way adapt knowledge extraction raw text
2011,expressive power deep architecture,deep architecture family function corresponding deep circuit deep learning algorithm based parametrizing circuit tuning parameter approximately optimize training objective whereas thought difficult train deep architecture several successful algorithm proposed recent year review theoretical motivation deep architecture well practical success propose direction investigation address remaining challenge
2011,domain adaptation large-scale sentiment classification deep learning approach,exponential increase availability online review recommendation make sentiment classification interesting topic academic industrial research review span many different domain difficult gather annotated training data hence paper study problem domain adaptation sentiment classifier hereby system trained labeled review one source domain meant deployed another propose deep learning approach learns extract meaningful representation review unsupervised fashion sentiment classifier trained high-level feature representation clearly outperform state-of-the-art method benchmark composed review  type amazon product furthermore method scale well allowed u successfully perform domain adaptation larger industrial-strength dataset  domain
2011,contractive auto-encoders explicit invariance feature extraction,present paper novel approach training deterministic auto-encoders show adding well chosen penalty term classical reconstruction cost function achieve  equal surpass attained regularized auto-encoders well denoising auto-encoders range datasets penalty term corresponds frobenius norm jacobian matrix encoder activation respect input show penalty term  localized space contraction turn yield robust feature activation layer furthermore show penalty term related regularized auto-encoders denoising auto-encoders seen link deterministic non-deterministic auto-encoders find empirically penalty help carve representation better capture local direction variation dictated data corresponding lower-dimensional non-linear manifold invariant vast majority direction orthogonal manifold finally show using learned feature initialize mlp achieve state art classification error range datasets surpassing method pretraining
2011,large-scale learning embeddings reconstruction sampling,paper present novel method speed learning embeddings large-scale learning task involving sparse data typically case natural language processing task speed-up method developed context denoising auto-encoders trained purely unsupervised way capture input distribution learn embeddings word text similar earlier neural language model main contribution new method approximate reconstruction error sampling procedure show approximation made obtain unbiased estimator training criterion show leveraged make learning much computationally efficient demonstrate effectiveness method amazon rcv nlp datasets instead reducing vocabulary size make learning practical method allows u train using large vocabulary particular reconstruction sampling requires x le training time full amazon dataset
2011,unsupervised model image spike and-slab rbms,spike-and-slab restricted boltzmann machine  defined real valued slab variable binary spike variable associated unit hidden layer paper generalize extend spike-and-slab rbm include non-zero mean conditional distribution observed variable given binary spike variable also introduce term quadratic observed data exploit guarantee conditionals associated model well defined  guarantee absent original spike-and-slab rbm inclusion generalization improves performance spike-and-slab rbm feature learner achieves competitive performance cifar- image classification task spike-and-slab model trained convolutional configuration generate sensible sample demonstrate model captured broad statistical structure natural image
2011,temporal pooling multiscale learning automatic annotation ranking music audio,paper analyzes challenge performing automatic annotation ranking music audio proposes improvement first motivate use principal component analysis mel-scaled spectrum secondly present analysis impact selection pooling function summarization feature time show combining several pooling function improves performance system finally introduce idea multiscale learning incorporating idea model obtained state-of-the-art performance magnatagatune dataset
2011,learning distributed representation semantics,machine learning algorithm try characterize configuration variable plausible  able answer question configuration general approach towards goal learn *representations* configuration help generalize new configuration expose statistical advantage representation *distributed* *deep*  survey advance feature learning algorithm along recent work area natural language processing pattern recognition particular highlight effort towards modeling semantics beyond single-word embeddings capture relation concept produce model -argument relation  seen  used answer question disambiguate text learn free text knowledge base representational space
2011,shallow v deep sum-product network,investigate representational power sum-product network  theoretical analysis compare deep  v shallow  architecture prove exist family function represented much efficiently deep network shallow one ie substantially fewer hidden unit  available contribute motivate recent research involving learning deep sum-product network generally motivate research deep learning
2011,manifold tangent classifier,combine three important idea present previous work building classifier semi-supervised hypothesis  unsupervised manifold hypothesis  manifold hypothesis classification  exploit novel algorithm capturing manifold structure  show build topological atlas chart chart characterized principal singular vector jacobian representation mapping representation learning algorithm stacked yield deep architecture combine domain knowledge-free version tangentprop algorithm encourage classifier insensitive local direction change along manifold record-breaking classification  obtained
2011,tracking partition function,markov random field  proven powerful density estimator feature extractor classification however use often limited inability estimate partition function z paper exploit gradient descent training procedure restricted boltzmann machine  { track} log partition function learning method relies two distinct source information  estimating change  zincurred gradient update  estimating difference zover small set tempered distribution using bridge sampling two source information combined using inference procedure similar kalman filtering learning mrfs tempered stochastic maximum likelihood estimate zusing temperature required learning comparing exact value estimate using annealed importance sampling  show several datasets method able accurately track log partition function contrast ai method provides estimate time-step computational cost similar required training alone
2011,algorithm hyper-parameter optimization,several recent advance state art image classification benchmark come better configuration existing technique rather novel approach feature learning traditionally hyper-parameter optimization job human efficient regime trial possible presently computer cluster gpu processor make possible run trial show algorithmic approach find better  present hyper-parameter optimization  task training neural network deep belief network  optimize hyper-parameters using random search two new greedy sequential method based expected improvement criterion random search shown sufficiently efficient learning neural network several datasets show unreliable training dbns sequential algorithm applied difficult dbn learning problem  find significantly better  best previously reported work contributes novel technique making response surface model p  many element hyper-parameter assignment  known irrelevant given particular value element
2011,higher order contractive auto-encoder,propose novel regularizer training auto-encoder unsupervised feature extraction explicitly encourage latent representation contract input space regularizing norm jacobian  hessian  encoder’s output respect input training point penalty jacobian’s norm ensures robustness tiny corruption sample input space constraining norm hessian extends robustness moving away sample manifold learning perspective balancing regularization auto-encoder’s reconstruction objective yield representation varies moving along data manifold input space insensitive direction orthogonal manifold second order regularization using hessian penalizes curvature thus favor smooth manifold show proposed technique remaining computationally efficient yield representation significantly better suited initializing deep architecture previously proposed approach beating state-of-the-art performance number datasets
2011,discussion neural autoregressive distribution estimator,restricted boltzmann machine  inspired much research recent year particular building block deep architecture  review restricted boltzmann machine  undirected graphical model latent variable exact inference rather simple sampling procedure  several successful learning algorithm based approximation log-likelihood gradient however come actually computing distribution density function intractable except either number input latent variable small  
2011,deep learner benefit out-of-distribution example,recent theoretical empirical work statistical machine learning demonstrated potential learning algorithm deep architecture ie function class obtained composing multiple level representation hypothesis evaluated intermediate level representation shared across task example different related distribution yield even benefit comparative experiment performed large-scale handwritten character recognition setting  class  using multi-task setting perturbed example order obtain out-ofdistribution example  agree hypothesis show deep learner beat previously published  reached human-level performance 
2011,spike slab restricted boltzmann machine,introduce spike slab restricted boltzmann machine characterized real-valued vector slab binary variable spike associated unit hidden layer model posse practical property amenable block gibbs sampling well capable generating similar latent representation data recently introduced mean covariance restricted boltzmann machine illustrate spike slab restricted boltzmann machine achieves competitive performance cifar- object recognition task
2011,deep sparse rectifier neural network,logistic sigmoid neuron biologically plausible hyperbolic tangent neuron latter work better training multi-layer neural network paper show rectifying neuron even better model biological neuron yield equal better performance hyperbolic tangent network spite hard non-linearity non-differentiability 
2010,decision tree generalize new variation,family decision tree learning algorithm among widespread studied motivated desire develop learning algorithm generalize learning highly varying function presumably needed achieve artificial intelligence study theoretical limitation decision tree demonstrate formally seriously hurt curse dimensionality sense bit different nonparametric statistical method importantly cannot generalize variation seen training set decision tree creates partition input space need least one example region associated leaf make sensible prediction region better understanding fundamental reason limitation suggests one use forest even deeper architecture instead tree provide form distributed representation generalize variation encountered training data
2010,alternative time representation dopamine model,dopaminergic neuron activity modeled learning appetitive behavior commonly using temporal-difference  algorithm however proper representation elapsed time exact task usually required model work model use timing element delay-line representation time biologically realistic interval range second interval-timing literature provides several alternative one timing could emerge general network dynamic instead coming dedicated circuit present general rate-based learning model based long short-term memory  network learns time representation needed using nave network learning environment conjunction td reproduce dopamine activity appetitive trace conditioning constant cs-us interval including probe trial unexpected delay proposed model learns representation environment dynamic adaptive biologically plausible framework without recourse delay line special-purpose circuit instead model predicts task-dependent representation time learned experience encoded ramp-like change single-neuron activity distributed across small neural network reflects temporal integration mechanism resulting inherent dynamic recurrent loop within network model also reproduces known finding trace conditioning difficult delay conditioning learned representation task highly dependent type trial experienced training finally suggests phasic dopaminergic signal could facilitate learning cortex
2010,unsupervised pre-training help deep learning,much recent research devoted learning algorithm deep architecture deep belief network stack auto-encoder variant impressive  obtained several area mostly vision language data set best  obtained supervised learning task involve unsupervised learning component usually unsupervised pre-training phase even though new algorithm enabled training deep model many question remain nature difficult learning problem main question investigated following unsupervised pre-training work answering question important learning deep architecture improved propose several explanatory hypothesis test extensive simulation empirically show influence pre-training respect architecture depth model capacity number training example experiment confirm clarify advantage unsupervised pre-training  suggest unsupervised pre-training guide learning towards basin attraction minimum support better generalization training data set evidence  support regularization explanation effect pre-training
2010,stacked denoising autoencoders learning useful representation deep network local denoising criterion,explore original strategy building deep network based stacking layer denoising autoencoders trained locally denoise corrupted version input resulting algorithm straightforward variation stacking ordinary autoencoders however shown benchmark classification problem yield significantly lower classification error thus bridging performance gap deep belief network  several case surpassing higher level representation learnt purely unsupervised fashion also help boost performance subsequent svm classifier qualitative experiment show contrary ordinary autoencoders denoising autoencoders able learn gabor-like edge detector natural image patch larger stroke detector digit image work clearly establishes value using denoising criterion tractable unsupervised objective guide learning useful higher level representation
2010,deep belief network compact universal approximators,deep belief network  generative model many layer hidden causal variable recently introduced hinton osindero teh  along greedy layer-wise unsupervised learning algorithm building le roux bengio  sutskever hinton  show deep narrow generative network require parameter shallow one achieve universal approximation exploiting proof technique prove deep narrow feedforward neural network sigmoidal unit represent boolean expression
2010,tractable multivariate binary density estimation restricted boltzmann forest,investigate problem estimating density function multivariate binary data particular focus model computing estimated probability data point tractable setting previous work mostly concentrated mixture modeling approach argue problem tractable density estimation restricted boltzmann machine  provides competitive framework multivariate binary density modeling mind also generalize rbm framework present restricted boltzmann forest  replaces binary variable hidden layer rbms group tree-structured binary variable extension allows u obtain model modeling capacity remain tractable experiment several data set demonstrate competitiveness approach study property
2010,word representation simple general method semi-supervised learning,take existing supervised nlp system simple general way improve accuracy use unsupervised word representation extra word feature evaluate brown cluster collobert weston  embeddings hlbl  embeddings word ner chunking use near state-of-the-art supervised baseline find three word representation improves accuracy baseline find improvement combining different word representation
2010,learning tag vary within song,paper examines relationship human generated tag describing different part song tag collected using amazon mechanical turk service find agreement different people tag decrease distance part song heard increase model tag relationship describe conditional restricted boltzmann machine using model fill tag probably present given context tag train automatic tag classifier  outperform trained original data
2010,tempered markov chain monte carlo training restricted boltzmann machine,alternating gibbs sampling common scheme used sampling restricted boltzmann machine  crucial component deep architecture deep belief network however find often poor job rendering diversity mode captured trained model suspect hinders advantage could principle brought training algorithm relying gibbs sampling uncovering spurious mode persistent contrastive divergence algorithm alleviate problem explore use tempered markov chain monte-carlo sampling rbms find visualization sample measure likelihood toy dataset help sampling learning
2010,understanding difficulty training deep feedforward neural network,whereas  appears deep multi-layer neural network successfully trained since several algorithm shown successfully train experimental  showing superiority deeper v le deep architecture experimental  obtained new initialization training mechanism objective understand better standard gradient descent random initialization poorly deep neural network better understand recent relative success help design better algorithm future first observe influence non-linear activation function find logistic sigmoid activation unsuited deep network random initialization mean value drive especially top hidden layer saturation surprisingly find saturated unit move saturation albeit slowly explaining plateau sometimes seen training neural network find new non-linearity saturates le often beneficial finally study activation gradient vary across layer training idea training may difficult singular value jacobian associated layer far  based consideration propose new initialization scheme brings substantially faster convergence
2009,learning deep architecture ai,theoretical  strongly suggest order learn kind complicated function represent high-level abstraction  one need deep architecture deep architecture composed multiple level non-linear operation neural net many hidden layer complicated propositional formula re-using many sub-formulae searching parameter space deep architecture difficult optimization task learning algorithm deep belief network recently proposed tackle problem notable success beating state-of-the-art certain area paper discus motivation principle regarding learning algorithm deep architecture particular exploiting building block unsupervised learning single-layer model restricted boltzmann machine used construct deeper model deep belief network
2009,exploring strategy training deep neural network,deep multi-layer neural network many level non-linearities allowing compactly represent highly non-linear highly-varying function however recently clear train deep network since gradient-based optimization starting random initialization often appears get stuck poor solution hinton et al recently proposed greedy layer-wise unsupervised learning procedure relying training algorithm restricted boltzmann machine  initialize parameter deep belief network  generative model many layer hidden causal variable followed proposal another greedy layer-wise procedure relying usage autoassociator network context optimization problem study algorithm empirically better understand success experiment confirm hypothesis greedy layer-wise unsupervised training strategy help optimization initializing weight region near good local minimum also implicitly act sort regularization brings better generalization encourages internal distributed representation high-level abstraction input also present series experiment aimed evaluating link performance deep neural network practical aspect topology example demonstrating case addition depth help finally empirically explore simple variant training algorithm use different rbm input unit distribution simple way combining gradient estimator improve performance well on-line version algorithm
2009,incorporating functional knowledge neural network,incorporating prior knowledge particular task architecture learning algorithm greatly improve generalization performance study case know function learned non-decreasing two argument convex one purpose propose class function similar multi-layer neural network  property  universal approximator lipschitz function property apply new class function task modelling price call option experiment show improvement regressing price call option using new type function class incorporate priori constraint
2009,justifying generalizing contrastive divergence,study expansion log likelihood undirected graphical model restricted boltzmann machine  term expansion associated sample gibbs chain alternating two random variable  particularly interested estimator gradient log likelihood obtained expansion show residual term converges zero justifying use truncationrunning short gibbs chain main idea behind contrastive divergence  estimator log-likelihood gradient truncating even obtain stochastic reconstruction error related mean-field approximation reconstruction error often used train autoassociators stacked autoassociators derivation specific particular parametric form used rbms requires convergence gibbs chain present theoretical empirical evidence linking number gibbs step k magnitude rbm parameter bias cd estimator experiment also suggest sign cd estimator correct time even bias large cd-k good descent direction even small k
2009,hybrid pareto mixture conditional asymmetric fat-tailed distribution,many case observe variable x contain predictive information scalar variable interest  pair observed training set take advantage information estimate conditional density p paper propose conditional mixture model hybrid pareto component estimate p hybrid pareto gaussian whose upper tail replaced generalized pareto tail third parameter addition location spread parameter gaussian control heaviness upper tail using hybrid pareto mixture model  nonparametric estimator adapt multimodality asymmetry heavy tail conditional density estimator built modeling parameter mixture estimator function x use neural network implement function conditional density estimator important application many domain finance insurance show experimentally novel approach better model conditional density term likelihood compared competing algorithm conditional mixture model type component classical kernel-based nonparametric model
2009,workshop summary workshop learning feature hierarchy,abstract available
2009,curriculum learning,human animal learn much better example randomly presented organized meaningful order illustrates gradually concept gradually complex one formalize training strategy context machine learning call curriculum learning context recent research studying difficulty training presence non-convex training criterion  explore curriculum learning various set-ups experiment show significant improvement generalization achieved hypothesize curriculum learning effect speed convergence training process minimum case non-convex criterion quality local minimum obtained curriculum learning seen particular form continuation method 
2009,quadratic feature deep architecture chunking,experiment several chunking model deeper architecture achieve better generalization quadratic filter simplification theoretical model v complex cell reliably increase accuracy fact logistic regression quadratic filter outperforms standard single hidden layer neural network adding quadratic filter logistic regression almost effective feature engineering despite predicting output label independently model competitive one use previous decision
2009,slow decorrelated feature pretraining complex cell-like network,introduce new type neural network activation function based recent physiological rate model complex cell visual area v single-hidden-layer neural network kind model achieves  error mnist also introduce existing criterion learning slow decorrelated feature pretraining strategy image model pretraining strategy  orientation-selective feature similar receptive field complex cell pretraining single-hidden-layer model achieves better generalization error even though pretraining sample distribution different fine-tuning distribution implement pretraining strategy derive fast algorithm online learning decorrelated feature iteration algorithm run linear time respect number feature
2009,infinite factor model hierarchy via noisy-or mechanism,indian buffet process bayesian nonparametric approach model object arising infinite number latent factor extend latent factor model framework two unbounded layer latent factor generative perspective layer defines conditional {factorial} prior distribution binary latent variable layer via noisy-or mechanism explore property model two empirical study one digit recognition task one music tag data experiment
2009,difficulty training deep architecture effect unsupervised pre-training,whereas theoretical work suggests deep architecture might efficient representing highly-varying function training deep architecture unsuccessful recent advent algorithm based unsupervised pretraining even though new algorithm enabled training deep model many question remain nature difficult learning problem answering question important learning deep architecture improved attempt shed light question extensive simulation experiment confirm clarify advantage unsupervised pre-training demonstrate robustness training procedure respect random initialization positive effect pre-training term optimization role kind regularizer show influence architecture depth model capacity number training example
2008,representational power restricted boltzmann machine deep belief network,deep belief network  generative neural network model many layer hidden explanatory factor recently introduced hinton osindero teh  along greedy layer-wise unsupervised learning algorithm building block dbn probabilistic model called restricted boltzmann machine  used represent one layer model restricted boltzmann machine interesting inference easy successfully used building block training deeper model first prove adding hidden unit yield strictly improved modeling power second theorem show rbms universal approximators discrete distribution study question whether dbns layer strictly powerful term representational power suggests new le greedy criterion training rbms within dbns
2008,neural net language model,n/a
2008,adaptive importance sampling accelerate training neural probabilistic language model,previous work statistical language modeling shown possible train feedforward neural network approximate probability sequence word resulting significant error reduction compared standard baseline model based n-grams however training neural network model maximum-likelihood criterion requires computation proportional number word vocabulary paper introduce adaptive importance sampling way accelerate training model idea use adaptive n-gram model track conditional distribution produced neural network show significant speedup obtained standard problem
2008,zero-data learning new task,introduce problem zero-data learning model must generalize class task training data available description class task provided zero-data learning useful problem set class distinguish task solve large entirely covered training data main contribution work lie presentation general formalization zero-data learning experimental analysis property empirical evidence showing generalization possible significant context experimental work paper address two classification problem character recognition multi-task ranking problem context drug discovery finally conclude discussing new framework could lead novel perspective extend machine learning towards ai agent given specification learning problem attempting solve 
2008,classification using discriminative restricted boltzmann machine,recently many application restricted boltzmann machine  developed large variety learning problem however rbms usually used feature extractor another learning algorithm provide good initialization deep feed-forward neural network classifier considered standalone solution classification problem paper argue rbms provide self-contained framework deriving competitive non-linear classifier present evaluation different learning algorithm rbms aim introducing discriminative component rbm training improve performance classifier approach simple rbms used directly build classifier rather stepping stone finally demonstrate discriminative rbms also successfully employed semi-supervised setting
2008,extracting composing robust feature denoising autoencoders,previous work shown difficulty learning deep generative discriminative model overcome initial unsupervised learning step map input useful intermediate representation introduce motivate new training principle unsupervised learning representation based idea making learned representation robust partial corruption input pattern approach used train autoencoders denoising autoencoders stacked initialize deep architecture algorithm motivated manifold learning information theoretic perspective generative model perspective comparative experiment clearly show surprising advantage corrupting input autoencoders pattern classification benchmark suite
2007,noisy k best-paths approximate dynamic programming application portfolio optimization,describe general method transform non-markovian sequential decision problem supervised learning problem using k-best-paths algorithm consider application financial portfolio management train controller directly optimize sharpe ratio  utility function illustrate approach demonstrating experimental  using kernel-based controller architecture would normally considered traditional reinforcement learning approximate dynamic programming show using non-additive criterion  yield noisy k-best-paths extraction problem give substantially improved performance
2007,empirical evaluation deep architecture problem many factor variation,recently several learning algorithm relying model deep architecture proposed though demonstrated impressive performance date evaluated relatively simple problem digit recognition controlled environment many machine learning algorithm already report reasonable  present series experiment indicate model show promise solving harder learning problem exhibit many factor variation model compared well-established algorithm support vector machine single hidden-layer feed-forward neural network
2007,augmented functional time series representation forecasting gaussian process,introduce functional representation time series allows forecast performed unspecified horizon progressively-revealed information set virtue using gaussian process complete covariance matrix forecast several time-steps available information put use application actively trade price spread commodity future contract approach delivers impressive out-of-sample risk-adjusted return transaction cost portfolio  spread 
2007,learning 2-d topology image,study following question two-dimensional structure image strong prior something learned example natural image someone gave u learning task involving image two-dimensional topology pixel known could discover automatically exploit example suppose pixel permuted fixed unknown way could recover relative two-dimensional location pixel image surprising result presented answer yes thousand image enough approximately recover relative location thousand pixel achieved using manifold learning algorithm applied pixel associated measure distributional similarity pixel intensity compare different topology-extraction approach show two-dimensional topology exploited
2007,topmoumoute online natural gradient algorithm,guided goal obtaining optimization algorithm fast yielding good generalization study descent direction maximizing decrease generalization error probability increasing generalization error surprising result bayesian frequentist perspective yield natural gradient direction although direction expensive compute develop efficient general online approximation natural gradient descent suited large scale problem report experimental  showing much faster convergence computation time number iteration tonga  stochastic gradient descent even large datasets
2007,hybrid pareto model conditional density estimation asymmetric fat-tail data,propose estimator conditional density p adapt asymmetric heavy tail might depend x estimator important application nance insurance draw extreme value theory tool build hybrid unimodal density parameter controlling heaviness upper tail hybrid gaussian whose upper tail replaced generalized pareto tail use hybrid multi-modal mixture order obtain nonparametric density estimator easily adapt heavy tailed data obtain conditional density estimator parameter mixture estimator seen function x function learned show experimentally approach better model conditional density term likelihood compared competing algorithm  conditional mixture model type component multivariate nonparametric model
2007,continuous neural network,article extends neural network case uncountable number hidden unit several way first approach proposed finite parametrization possible allowing gradient-based learning number parameter ordinary neural network internal structure suggests represent smooth function much compactly mild assumption also find better error bound ordinary neural network furthermore parametrization may help reducing problem saturation neuron second approach input-to-hidden weight fully nonparametric yielding kernel machine demonstrate simple kernel formula interestingly resulting kernel machine made hyperparameter-free still generalizes spite absence explicit regularization
2006,collaborative filtering family biological target,building qsar model new biological target screening data available statistical challenge however new target may part bigger family screening data collaborative filtering generally multi-task learning machine learning approach improves generalization performance algorithm using information related task inductive bias use collaborative filtering technique building predictive model link multiple target multiple example commonality target better multi-target model built show example multi-target neural network use family information produce predictive model undersampled target evaluate jrank kernel-based method designed collaborative filtering show performance compound prioritization hts campaign underlying shared representation target jrank outperformed neural network single- multi-target model
2006,nonlocal estimation manifold structure,claim present argument effect large class manifold learning algorithm essentially local framed kernel learning algorithm suffer curse dimensionality dimension true underlying manifold observation invite exploration nonlocal manifold learning algorithm attempt discover shared structure tangent plane different position training criterion algorithm proposed experiment estimating tangent plane prediction function presented showing advantage respect local manifold learning algorithm able generalize far training data  local nonparametric method fail
2006,k best-paths approach approximate dynamic programming application portfolio optimization,describe general method transform non-markovian sequential decision problem supervised learning problem using k-best-paths algorithm consider application financial portfolio management train controller directly optimize sharpe ratio  utility function illustrate approach demonstrating experimental  using kernel-based controller architecture would normally considered traditional reinforcement learning approximate dynamic programming
2006,greedy layer-wise training deep network,complexity theory circuit strongly suggests deep architecture much ef cient  shallow architecture term computational element required represent function deep multi-layer neural network many level non-linearities allowing compactly represent highly non-linear highly-varying function however recently clear train deep network since gradient-based optimization starting random initialization appears often get stuck poor solution hinton et al recently introduced greedy layer-wise unsupervised learning algorithm deep belief network  generative model many layer hidden causal variable context optimization problem study algorithm empirically explore variant better understand success extend case input continuous structure input distribution revealing enough variable predicted supervised task experiment also confirm hypothesis greedy layer-wise unsupervised training strategy mostly help optimization initializing weight region near good local minimum giving rise internal distributed representation high-level abstraction input bringing better generalization 
2005,efficient non-parametric function induction semi-supervised learning,increase interest semi-supervised learning recently many datasets large amount unlabeled example labeled one paper follows proposed non-parametric algorithm provide estimated continuous label given unlabelled example first extends function induction algorithm minimize regularization criterion applied out-of-sample example happen form parzen window regressors allows predict test label without solving linear system dimension n cost second function induction procedure give rise efficient approximation training process reducing linear system solved << n unknown using subset example improvement time thus obtained comparative experiment presented showing good performance induction formula approximation algorithm
2005,hierarchical probabilistic neural network language model,recent year variant neural network architecture statistical language modeling proposed successfully applied eg language modeling component speech recognizers main advantage architecture learn embedding word  continuous space help smooth language model provide good generalization even number training example insufficient however model extremely slow comparison commonly used n-gram model training recognition alternative importance sampling method proposed speed-up training introduce hierarchical decomposition conditional probability yield speed-up  training recognition hierarchical decomposition binary hierarchical clustering constrained prior knowledge extracted wordnet semantic hierarchy 
2005,greedy spectral embedding,spectral dimensionality reduction method spectral clustering method require computation principal eigenvectors n  n matrix n number example following previously proposed technique speed-up kernel method focusing subset example study greedy selection procedure subset based featurespace distance candidate example span previously chosen one case kernel pca spectral clustering reduces computation computational complexity also compute feature space projection non-selected example subspace spanned selected example estimate embedding function based data yield considerably better estimation embedding function algorithm formulated online setting bound error approximation gram matrix
2005,semi-supervised learning entropy minimization,consider semi-supervised learning problem decision rule learned labeled unlabeled data framework motivate minimum entropy regularization enables incorporate unlabeled data standard supervised learning approach includes approach semi-supervised problem particular limiting case series experiment illustrates proposed solution benefit unlabeled data method challenge mixture model data sampled distribution class spanned generative model performance definitely favor minimum entropy regularization generative model misspecified weighting unlabeled data provides robustness violation cluster assumption finally also illustrate method also far superior manifold learning high dimension space
2005,curse highly variable function local kernel machine,present series theoretical argument supporting claim large class modern learning algorithm rely solely smoothness prior - similarity example expressed local kernel - sensitive curse dimensionality precisely variability target discussion cover supervised semi-supervised unsupervised learning algorithm algorithm found local sense crucial property learned function x depend mostly neighbor x training set make sensitive curse dimensionality well studied classical non-parametric statistical learning show case gaussian kernel function learned many variation algorithm require number training example proportional number variation could large even though may exist short description target function ie kolmogorov complexity may low suggests exist non-local learning algorithm least potential learn structured apparently complex function  using specific prior domain knowledge
2005,non-local manifold parzen window,escape curse dimensionality claim one learn non-local function sense value shape learned function x must inferred using example may far x objective present non-local non-parametric density estimator build upon previously proposed gaussian mixture model regularized covariance matrix take account local shape manifold also build upon recent work non-local estimator tangent plane manifold able generalize place little training data unlike traditional local non-parametric model
2005,convex neural network,convexity recently received lot attention machine learning community lack convexity seen major disadvantage many learning algorithm multi-layer artificial neural network show training multi-layer neural network number hidden unit learned viewed convex optimization problem problem involves infinite number variable solved incrementally inserting hidden unit time time finding linear classifier minimizes weighted sum error 
2004,locally linear embedding dimensionality reduction qsar,current practice quantitative structure activity relationship  method usually involves generating great number chemical descriptor cutting back variable selection technique variable selection effective method reduce dimensionality may discard valuable information paper introduces locally linear embedding  local non-linear dimensionality reduction technique statistically discover low-dimensional representation chemical data lle shown create stable representation non-linear dimensionality reduction algorithm capable capturing non-linearity chemical data
2004,unbiased estimator variance k-fold cross-validation,machine learning researcher perform quantitative experiment estimate generalization error compare performance different algorithm  order able draw statistically convincing important estimate uncertainty estimate paper study commonly used k-fold cross-validation estimator generalization performance main theorem show exists universal  unbiased estimator variance k-fold cross-validation analysis accompanies result based eigen-decomposition covariance matrix error three different eigenvalue corresponding three degree freedom matrix three component total variance analysis help better understand nature problem make naive estimator  grossly underestimate variance confirmed numerical experiment three component variance compared difficulty learning problem number fold varied
2004,learning eigenfunctions link spectral embedding kernel pca,letter show direct relation spectral embedding method kernel principal component analysis special case general learning problem learning principal eigenfunctions operator defined kernel unknown data-generating density whereas spectral embedding method provided coordinate training point analysis justifies simple extension out-of-sample example  multidimensional scaling  spectral clustering laplacian eigenmaps locally linear embedding  isomap analysis provides spectral embedding method definition loss function whose empirical average minimized traditional algorithm asymptotic expected value loss defines generalization performance clarifies algorithm trying learn experiment lle isomap spectral clustering md show out-of-sample embedding formula generalizes well level error comparable effect small perturbation training set embedding
2004,unsupervised sense disambiguation using bilingual probabilistic model,describe two probabilistic model unsupervised word-sense disambiguation using parallel corpus first model call sense model build work diab resnik  us parallel text sense inventory target language recasts approach probabilistic framework second model call concept model hierarchical model us concept latent variable relate different language specific sense label show model improve performance word sense disambiguation task previous unsupervised approach concept model showing largest improvement furthermore learning concept model by-product learn sense inventory parallel language
2004,non-local manifold tangent learning,claim present argument effect large class manifold learning algorithm essentially local framed kernel learning algorithm suffer curse dimensionality dimension true underlying manifold observation suggests explore non-local manifold learning algorithm attempt discover shared structure tangent plane different position criterion algorithm proposed experiment estimating tangent plane prediction function presented showing advantage respect local manifold learning algorithm able generalize far training data  local non-parametric method fails
2004,brain inspired reinforcement learning,successful application reinforcement learning algorithm often involves considerable hand-crafting necessary non-linear feature reduce complexity value function hence promote convergence algorithm contrast human brain readily autonomously find complex feature provided sufficient training recent work machine learning neurophysiology demonstrated role basal ganglion frontal cortex mammalian reinforcement learning paper develops explores new reinforcement learning algorithm inspired neurological evidence provides potential new approach feature construction problem algorithm compared evaluated acrobot task
2003,scaling large learning problem hard parallel mixture,challenge statistical learning deal large data set eg data mining training time ordinary support vector machine least quadratic raise serious research challenge want deal data set million example propose hard parallelizable mixture methodology yield significantly reduced training time modularization parallelization training data iteratively partitioned gater model way becomes easy learn expert model separately region partition probabilistic extension use set generative model allows representing gater piece model locally trained svms time complexity appears empirically local growth linearly number example generalization performance enhanced probabilistic version algorithm iterative algorithm probably go cost function upper bound negative log-likelihood
2003,neural probabilistic language model,goal statistical language modeling learn joint probability function sequence word language intrinsically difficult curse dimensionality word sequence model tested likely different word sequence seen training traditional successful approach based n-grams obtain generalization concatenating short overlapping sequence seen training set propose fight curse dimensionality learning distributed representation word allows training sentence inform model exponential number semantically neighboring sentence model learns simultaneously  distributed representation word along  probability function word sequence expressed term representation generalization obtained sequence word never seen get high probability made word similar  word forming already seen sentence training large model  within reasonable time significant challenge report experiment using neural network probability function showing two text corpus proposed approach significantly improves state-of-the-art n-gram model proposed approach allows take advantage longer context
2003,extension metric-based model selection,metric-based method recently introduced model selection regularization often yielding significant improvement alternative tried  method require unlabeled data compare function detect gross difference behavior away training point introduce three new extension metric model selection method apply feature selection first extension take advantage particular case time-series data task involves prediction horizon h idea use h unlabeled example precede model selection second extension take advantage different error distribution cross-validation metric method cross-validation tends larger variance unbiased hybrid combining two model selection method rarely beaten two method third extension deal case unlabeled data available using estimated input density experiment described study extension context capacity control feature subset selection
2003,inference generalization error,order compare learning algorithm experimental  reported machine learning literature often use statistical test significance support claim new learning algorithm generalizes better test take account variability due choice training set due test example often case could lead gross underestimation variance cross-validation estimator wrong  new algorithm significantly better perform theoretical investigation variance variant cross-validation estimator generalization error take account variability due randomness training set well test example analysis show variance estimator based  cross-validation experiment must biased analysis allows u propose new estimator variance show via simulation test hypothesis generalization error using new variance estimator better property test involving variance estimator currently use listed dietterich  particular new test correct size good power new test reject null hypothesis often hypothesis true tend frequently reject null hypothesis latter false
2003,bias learning knowledge sharing,biasing properly hypothesis space learner shown improve generalization performance method achieving goal proposed range designing introducing bias learner automatically learning bias multitask learning method fall latter category several related task derived domain available method use domain-related knowledge coded training example task source bias extend idea presented field describe new approach identifies family hypothesis represented manifold hypothesis space embodies domain-related knowledge family learned using training example sampled group related task learning model trained task allowed select hypothesis belong family show new approach encompasses large variety family learned statistical analysis class related task performed show significantly improved performance using approach
2003,quick training probabilistic neural net importance sampling,device invention us electrostatic recording apply original title legend indicia discrete record receiver system includes changeable stencil purpose description constructed counter desired information set voltage applied stencil electrode behind record receiver ionized pigmented particle attracted stencil record receiver record receiver moved fixing station rendering individual item information changeable desired information within range apparatus applied appropriate record receiver
2003,out-of-sample extension lle isomap md eigenmaps spectral clustering,several unsupervised learning algorithm based eigendecomposition provide either embedding clustering given training point straightforward extension out-of-sample example short recomputing eigenvectors paper provides unified framework extending local linear embedding  isomap laplacian eigenmaps multi-dimensional scaling  well spectral clustering framework based seeing algorithm learning eigenfunctions data-dependent kernel numerical experiment show generalization performed level error comparable variability embedding algorithm due choice training data
2002,model selection small sample regression,model selection important ingredient many machine learning algorithm particular sample size small order strike right trade-off overfitting underfitting previous classical  linear regression based asymptotic analysis present new penalization method performing model selection regression appropriate even small sample penalization based accurate estimator ratio expected training error expected generalization error term expected eigenvalue input covariance matrix
2002,kernel matching pursuit,matching pursuit algorithm learn function weighted sum basis function sequentially appending function initially empty basis approximate target function least-squares sense show matching pursuit extended use non-squared error loss function used build kernel-based solution machine learning problem keeping control sparsity solution present version algorithm make optimal choice next basis weight previously chosen base finally link boosting algorithm rbf training procedure well extensive experimental comparison svms classification given showing comparable  typically much sparser model
2002,parallel mixture svms large scale problem,support vector machine  state-of-the-art model many classification problem suffer complexity training algorithm least quadratic respect number example hence hopeless try solve real-life problem hundred thousand example svms article proposes new mixture svms easily implemented parallel svm trained small subset whole data set experiment large benchmark data set  yielded significant time improvement  addition surprisingly significant improvement generalization observed
2002,robust regression asymmetric heavy-tail noise distribution,presence heavy-tail noise distribution regression becomes much difficult traditional robust regression method assume noise distribution symmetric downweight influence so-called outlier noise distribution asymmetric method yield biased regression estimator motivated data-mining problem insurance industry propose new approach robust regression tailored deal asymmetric noise distribution main idea learn parameter model using conditional quantile estimator  learn remaining parameter combine correct estimator minimize average squared error unbiased way theoretical analysis experiment show clear advantage approach  artificial data well insurance data using linear neural network predictor
2002,manifold parzen window,similarity object fundamental element many learning algorithm non-parametric method take similarity fixed much recent work shown advantage learning particular exploit local invariance data capture possibly non-linear manifold data lie propose new non-parametric kernel density estimation method capture local structure underlying manifold leading eigenvectors regularized local covariance matrix experiment density estimation show significant improvement respect parzen density estimator density estimator also used within bayes classifier yielding classification rate similar svms much superior parzen classifier
2002,metric-based model selection time-series forecasting,metric-based method use unlabeled data detect gross difference behavior away training point recently introduced model selection often yielding significant improvement alternative  introduce extension take advantage particular case time-series data task involves prediction horizon h idea  use h unlabeled example precede model selection  take advantage different error distribution cross-validation metric method experimental  establish effectiveness extension context feature subset selection
2001,experiment application iohmms model financial return series,input-output hidden markov model  conditional hidden markov model emission  probability conditioned input sequence example conditional distribution linear logistic nonlinear  compare generalization performance several model special case input-output hidden markov model financial time-series prediction task unconditional gaussian conditional linear gaussian mixture gaussians mixture conditional linear gaussians hidden markov model various iohmms experiment compare model predicting conditional density return market sector index note unconditional gaussian estimate first moment historical average  show although first moment historical average give best  higher moment iohmms yielded significantly better performance estimated out-of-sample likelihood
2001,cost function model combination var-based asset allocation using neural network,introduce asset-allocation framework based active control value-at-risk portfolio within framework compare two paradigm making allocation using neural network first one us network make forecast asset behavior conjunction traditional mean-variance allocator constructing portfolio second paradigm us network directly make portfolio allocation decision consider method performing soft input variable selection show considerable utility use model combination  method systematize choice hyperparameters training show committee using paradigm significantly outperforming benchmark market performance
2001,k-local hyperplane convex distance nearest neighbor algorithm,guided initial idea building complex  decision surface maximal local margin input space give possible geometrical intuition k-nearest neighbor  algorithm often perform poorly svms classification task propose modified k-nearest neighbor algorithm overcome perceived problem approachis similar spirit tangent distance invariance inferred local neighborhood rather prior knowledge experimental  real world classification task suggest modified knn algorithm often give dramatic improvement standard knn perform well better svms
2001,estimating car insurance premia case study high-dimensional data inference,estimating insurance premia data difficult regression problem several reason large number variable many discrete peculiar shape noise distribution asymmetric fat tail large majority zero unreliable large value compare several machine learning method estimating insurance premia test large data base car insurance policy find function approximation method optimize squared loss like support vector machine regression work well context compared method include decision tree generalized linear model best  obtained mixture expert better identifies least risky contract allows reduce median premium charging risky customer
2001,topic segmentation  first stage dialog-based information extraction,study problem topic segmentation manually transcribed speech order facilitate information extraction dialog approach based combination multi-source knowledge modeled hidden markov model experiment different combination linguistic-level cue dialog dealing search rescue mission  show effectiveness multi-source knowledge 
2000,boosting neural network,boosting general method improving performance learning algorithm recently proposed boosting algorithm ada boost applied great success several benchmark machine learning problem using mainly decision tree base classifier article investigate whether ada boost also work well neural network discus advantage drawback different version ada boost algorithm particular compare training method based sampling training set weighting cost function  suggest random resampling training data main explanation success improvement brought ada boost contrast bagging directly aim reducing variance random resampling essential obtain reduction generalization error system achieves  error data set on-line handwritten digit  writer boosted multilayer network achieved  error uci letter  error uci satellite data set significantly better boosted decision tree
2000,gradient-based optimization hyperparameters,many machine learning algorithm formulated minimization training criterion involves hyperparameter hyperparameter usually chosen trial error model selection criterion article present methodology optimize several hyper-parameters based computation gradient model selection criterion respect hyperparameters case quadratic training criterion gradient selection criterion respect hyperparameters efficiently computed backpropagating cholesky decomposition general case show implicit function theorem used derive formula hyper-parameter gradient involving second derivative training criterion
2000,taking curse dimensionality joint distribution using neural network,curse dimensionality severe modeling high-dimensional discrete data number possible combination variable explodes exponentially propose architecture modeling high-dimensional data requires resource  grow square number variable using multilayer neural network represent joint distribution variable product conditional distribution neural network interpreted graphical model without hidden random variable conditional distribution tied hidden unit connectivity neural network pruned using dependency test variable  experiment modeling distribution several discrete data set show statistically significant improvement method naive bayes comparable bayesian network show significant improvement obtained pruning network
2000,probabilistic neural network model sequential data,artificial neural network  incorporated probabilistic model paper review approach proposed incorporate probabilistic model sequential data hidden markov model  also discus new development new idea area particular ann used model high-dimensional discrete continuous data deal curse dimensionality idea proposed model could applied statistical language modeling represent longer-term context allowed trigram model keeping word-order information
2000,neural support vector network architecture adaptive kernel,support vector machine  framework positive-definite kernel seen representing fixed similarity measure two pattern discriminant function obtained taking linear combination kernel computed training example called support vector investigate learning architecture kernel function replaced general similarity measure arbitrary internal parameter training criterion used svms appropriate purpose adopt simple criterion generally used training neural network classification task several experiment performed show neural support vector network perform similarly svms requiring significantly fewer support vector even similarity measure internal parameter
2000,continuous optimization hyper-parameters,many machine learning algorithm formulated minimization training criterion involves hyper-parameter hyper-parameter usually chosen trial error model selection criterion paper present methodology optimize several hyper-parameters based computation gradient model selection criterion respect hyper-parameters case quadratic training criterion gradient selection criterion respect hyper-parameters efficiently computed back-propagating cholesky decomposition general case show implicit function theorem used derive formula hyper-parameter gradient involving second derivative training criterion
2000,incorporating second-order functional knowledge better option pricing,incorporating prior knowledge particular task architecture learning algorithm greatly improve generalization performance study case know function learned non-decreasing two argument convex one purpose propose class function similar multi-layer neural network  property  universal approximator continuous function property apply new class function task modeling price call option experiment show improvement regressing price call option using new type function class incorporate priori constraint 
1999,stochastic learning strategic equilibrium auction,article present new application stochastic adaptive learning algorithm computation strategic equilibrium auction proposed approach address problem tracking moving target balancing exploration  versus exploitation  neural network used represent stochastic decision model bidder experiment confirm correctness usefulness approach 
1999,binary pseudowavelets application bilevel image processing,paper show existence binary pseudowavelets base binary domain exhibit property wavelet multiresolution reconstruction compact support binary pseudowavelets defined b/sup n/  operated upon binary operator logical exclusive forward transform analysis decomposition binary vector constituent binary pseudowavelets binary pseudowavelets allow multiresolution progressive reconstruction binary vector using progressively coefficient inverse transform binary pseudowavelets base sparse matrix also provide fast transforms moreover pseudowavelets rely hardware-friendly operation efficient software hardware implementation
1999,modeling high-dimensional discrete data multi-layer neural network,curse dimensionality severe modeling high-dimensional discrete data number possible combination variable explodes exponentially paper propose new architecture modeling high-dimensional data requires resource  grow square number variable using multi-layer neural network represent joint distribution variable product conditional distribution neural network interpreted graphical model without hidden random variable conditional distribution tied hidden unit connectivity neural network pruned using dependency test variable experiment modeling distribution several discrete data set show statistically significant improvement method naive bayes comparable bayesian network show significant improvement obtained pruning network
1999,object recognition gradient-based learning,finding appropriate set feature essential problem design shape recognition system paper attempt show recognizing simple object high shape variability handwritten character possible even advantageous feed system directly minimally processed image rely learning extract right set feature convolutional neural network shown particularly well suited task also show network used recognize multiple object without requiring explicit segmentation object surrounding second part paper present graph transformer network model extends applicability gradient-based learning system use graph represents feature object combination
1998,gaussian mixture density classification nuclear power plant data,paper concerned application learning algorithm classification reactor state nuclear plant two aspect must considered  type event  appear data set system able detect  classification signal also interpretation important nuclear plant monitoring address issue mixture mixture gaussians parameter shared reflect similar signal observed different state reactor em algorithm shared gaussian mixture presented  experimental  nuclear plant data demonstrate  advantage proposed approach respect two point
1998,high quality document image compression djvu,present new image compression technique called djvu specifically geared towards compression highresolution high-quality image scanned document color enables fast transmission document image low-speed connection faithfully reproducing visual aspect document including color font picture paper texture djvu compressor separate text drawing need high spatial resolution picture smoother coded lower spatial resolution several novel technique used maximize compression ratio bi-level foreground image encoded at&ts proposal new jbig fax standard new wavelet-based compression method used picture technique use new adaptive binary arithmetic coder called zp-coder typical magazine page color  dpi  compressed   kbytes approximately  time better jpeg similar level subjective quality real-time memory efficient version decoder implemented available plug-in popular web browser
1998,browsing high quality document image djvu,present new image compression technique called djvu specifically geared towards compression high-resolution high-quality image scanned document color djvu screen connected internet access display image scanned page faithfully reproducing font color drawing picture paper texture typical magazine page color  dpi compressed   kbytes approximately   time better jpeg similar level subjective quality black-and-white document typically   kbytes  dpi   time better ccitt-g real-time memory-efficient version decoder implemented available plug-in popular web browser
1998,z-coder adaptive binary coder,present z-coder new adaptive data compression coder coding binary data z-coder derived golomb  run-length coder retains speed simplicity earlier coder z-coder also thought multiplication-free approximate arithmetic coder showing close relationship run-length coding arithmetic coding z-coder improves upon existing arithmetic coder speed principled design present derivation z-coder well detail construction adaptive probability estimation table
1998,memory-efficient adaptive huffman coding algorthm large set symbol,summary form given problem computing minimum redundancy code observe symbol one one received lot attention however existing algorithm implicitly assumes either small alphabet arbitrary amount memory disposal creation coding tree real life application one may need encode symbol coming much larger alphabet eg coding integer introduce new algorithm adaptive huffman coding called algorithm us space proportional number frequency class algorithm us tree leaf represent set symbol frequency rather individual symbol code symbol therefore composed prefix  suffix  algorithm us two operation remain close possible optimal set migration rebalancing analyze computational complexity algorithm point advantage term low memory complexity fast decoding comparative experiment performed algorithm calgary corpus static huffman coding well another adaptive huffman coding algorithm algorithm /spl lambda/ vitter experiment show performs comparably better algorithm requires much le memory finally present improved algorithm m/sup +/ non-stationary data model distribution data fixed-size window data sequence
1998,support vector machine improving classification brain pet image,classification brain pet volume carried three main step  registration  feature extraction  classification pet image already smoothed  mm isotropic gaussian kernel registered within talairach tournoux reference system make registration accurate single reference method based optical flow applied feature extraction carried principal component analysis  support vector machine  used classification better controlled neural network  well adapted small sample size problem svm constructed training algorithm maximizes margin training vector decision boundary algorithm simple quadratic programming linear constraint lead global optimum decision boundary expressed linear combination supporting vector subset training vector closest decision boundary registration nn svm trained feature extracted pca training set estimate error rate  svm  nn
1997,using financial training criterion rather prediction criterion,application work decision making financial time series using learning algorithm traditional approach train model using prediction criterion minimizing squared error prediction actual value dependent variable maximizing likelihood conditional model dependent variable find noisy time series better  obtained model directly trained order maximize financial criterion interest gain loss  incurred trading experiment performed portfolio selection  canadian stock
1997,global training document processing system using graph transformer network,propose new machine learning paradigm called graph transformer network extends applicability gradient-based learning algorithm system composed module take graph input produce graph output training performed computing gradient global objective function respect parameter system using kind back-propagation procedure complete check reading system based concept described system us convolutional neural network character recognizers combined global training technique provide record accuracy business personal check presently deployed commercially read million check per month
1997,adaboosting neural network application on-line character recognition,“boosting” general method improving performance weak learning algorithm consistently generates classifier need perform slightly better random guessing recently proposed promising boosting algorithm adaboost  applied great success several benchmark machine learning problem using rather simple learning algorithm  particular decision tree  paper use adaboost improve performance strong learning algorithm neural network based on-line character recognition system particular show used learn automatically great variety writing style even amount training data style varies lot system achieves  % error handwritten digit data base  writer
1997,reading check multilayer graph transformer network,propose new machine learning paradigm called multilayer graph transformer network extends applicability gradient-based learning algorithm system composed module take graph input produce graph output complete check reading system based concept described system combine convolutional neural network character recognizers graph-based stochastic model trained cooperatively document level deployed commercially read million business personal check per month record accuracy
1997,discriminative feature model design automatic speech recognition,system discriminative feature model design presented automatic speech recognition training based minimum classification error using single objective function applied designing set parallel network performing feature transformation set hidden markov model performing speech recognition paper compare use linear non-linear functional transformation applied conventional recognition feature spectrum cepstrum also provides framework integrated feature model training using class-specific transformation experimental  telephone-based connected digit recognition presented
1997,shared context probabilistic transducer,recently model supervised learning probabilistic transducer represented suffix tree introduced however algorithm tends build large tree requiring large amount computer memory paper propose new compact transducer model one share parameter distribution associated context yielding similar conditional output distribution illustrate advantage proposed algorithm comparative experiment inducing noun phrase recognizer
1997,training method adaptive boosting neural network,boosting general method improving performance learning algorithm consistently generates classifier need perform slightly better random guessing recently proposed promising boosting algorithm adaboost  applied great success several benchmark machine learning problem using rather simple learning algorithm  decision tree  paper use adaboost improve performance neural network compare training method based sampling training set weighting cost function system achieves  error data base online handwritten digit  writer adaptive boosting multi-layer network achieved  error uci letter  % error uci satellite data set
1996,input-output hmms sequence processing,consider problem sequence processing propose solution based discrete-state model order represent past context introduce recurrent connectionist architecture modular structure associate subnetwork state model statistical interpretation call input-output hidden markov model  trained estimation-maximization  generalized em  algorithm considering state trajectory missing data decouples temporal credit assignment actual parameter estimation model present similarity hidden markov model  allows u map input sequence output sequence using processing style recurrent neural network iohmms trained using discriminant learning paradigm hmms potentially taking advantage em algorithm demonstrate iohmms well suited solving grammatical inference problem benchmark problem experimental  presented seven tomita grammar showing adaptive model attain excellent generalization
1996,multi-task learning stock selection,artificial neural network used predict future return stock order take financial decision one build separate network stock share network stock paper also explore alternative layer shared others shared prediction future return different stock viewed different task sharing parameter across stock form multi-task learning series experiment canadian stock obtain yearly return  various benchmark
1995,diffusion context credit information markovian model,paper study problem ergodicity transition probability matrix markovian model hidden markov model  make difficult task learning represent long-term context sequential data phenomenon hurt forward propagation long-term context information well learning hidden state representation represent long-term context depends propagating credit information backwards time using  markov chain theory show problem diffusion context credit reduced transition probability approach   ie transition probability matrix sparse model essentially deterministic  found paper apply learning approach based continuous optimization gradient descent baum-welch algorithm
1995,lerec nn/hmm hybrid on-line handwriting recognition,introduce new approach on-line recognition handwritten word written unconstrained mixed style preprocessor performs word-level normalization fitting model word structure using em algorithm word coded low resolution annotated image pixel contains information trajectory direction curvature recognizer convolution network spatially replicated network output hidden markov model produce word score entire system globally trained minimize word-level error
1995,search new learning rule anns,paper present framework learning rule optimized within parametric learning rule space define callparametric learning rule present theoretical study theirgeneralization property estimated set learning task tested another set task corroborate  study practical experiment
1995,recurrent neural network missing asynchronous data,paper propose recurrent neural network feedback input unit handling two type data analysis problem one hand scheme used static data input variable missing hand also used sequential data input variable missing available different frequency unlike case probabilistic model  missing variable network attempt model distribution missing variable given observed variable instead discriminant approach fill missing variable sole purpose minimizing learning criterion  
1995,hierarchical recurrent neural network long-term dependency,already shown extracting long-term dependency sequential data di cult deterministic dynamical system recurrent network probabilistic model hidden markov model  input/output hidden markov model  practice avoid problem researcher used domain speci c a-priori knowledge give meaning hidden state variable representing past context paper propose use general type a-priori knowledge namely temporal dependency structured hierarchically implies long-term dependency represented variable long time scale principle applied recurrent network includes delay multiple time scale experiment con rm advantage structure similar approach proposed hmms iohmms
1994,learning long-term dependency gradient descent difficult,recurrent neural network used map input sequence output sequence recognition production prediction problem however practical difficulty reported training recurrent neural network perform task temporal contingency present input/output sequence span long interval show gradient based learning algorithm face increasingly difficult problem duration dependency captured increase  expose trade-off efficient learning gradient descent latching information long period based understanding problem alternative standard gradient descent considered< >
1994,use genetic programming search new learning rule neural network,previous work explained use standard optimization method simulated annealing gradient descent genetic algorithm optimize parametric function could used learning rule neural network use method choose fixed number parameter rigid form learning rule article propose use genetic programming find value rule parameter also optimal number parameter form rule experiment classification task suggest genetic programming find better learning rule optimization method furthermore best rule found genetic programming outperformed well-known backpropagation algorithm given set tasks< >
1994,word-level training handwritten word recognizer based convolutional neural network,introduce new approach online recognition handwritten word written unconstrained mixed style word represented low resolution annotated image pixel contains information trajectory direction curvature recognizer convolutional network spatially replicated network output hidden markov model produce word score entire system globally trained minimize word-level error
1994,em approach grammatical inference input/output hmms,proposes modular recurrent connectionist architecture adaptive temporal processing model given probabilistic interpretation trained using estimation-maximisation  algorithm model also seen input/output hidden markov model focus paper sequence classification task author demonstrate em supervised learning well suited solving grammatical inference problem experimental benchmark  presented seven tomita grammar showing adaptive model attain excellent generalization
1994,word normalization online handwritten word recognition,introduce new approach normalizing word written electronic stylus applies style handwriting  geometrical model word spatial structure fitted pen trajectory using expectation-maximisation algorithm fitting process maximizes likelihood trajectory given model set prior parameter method evaluated integrated recognition system combine neural network hidden markov model
1994,input output hmm architecture,introduce recurrent architecture modular structure formulate training procedure based em algorithm resulting model similarity hidden markov model support recurrent network processing style allows exploit supervised learning paradigm using maximum likelihood estimation
1994,diffusion credit markovian model,paper study problem diffusion markovian model hidden markov model  make difficult task learning long-term dependency sequence using  markov chain theory show problem diffusion reduced transition probability approach   condition standard hmms limited modeling capability input/output hmms still perform interesting computation 
1994,convergence property k-means algorithm,paper study convergence property well known k-means clustering algorithm k-means algorithm described either gradient descent algorithm slightly extending mathematics em algorithm hard threshold case show k-means algorithm actually minimizes quantization error using fast newton algorithm 
1993,connectionist approach speech recognition,task discussed paper learning map input sequence output sequence particular problem phoneme recognition continuous speech considered discussed technique could applied task recognition sequence handwritten character system considered paper based connectionist model artificial neural network sometimes combined statistical technique recognition sequence pattern stressing integration prior knowledge learning different architecture sequence speech recognition reviewed including recurrent network well hybrid system involving hidden markov model
1993,problem learning long-term dependency recurrent network,author seek train recurrent neural network order map input sequence output sequence application sequence recognition production  presented showing learning long-term dependency recurrent network using gradient descent difficult task shown difficulty arises robustly latching bit information certain attractor derivative output time respect unit activation time zero tend rapidly zero increase input value situation simple gradient descent technique appear inappropriate consideration alternative optimization method architecture suggested< >
1993,credit assignment time alternative backpropagation,learning recognize predict sequence using long-term context many application however practical theoretical problem found training recurrent neural network perform task input/output dependency span long interval starting mathematical analysis problem consider compare alternative algorithm architecture task span input/output dependency controlled  new algorithm show performance qualitatively superior obtained backpropagation
1993,globally trained handwritten word recognizer using spatial representation convolutional neural network hidden markov model,introduce new approach on-line recognition handwritten word written unconstrained mixed style preprocessor performs word-level normalization fitting model word structure using em algorithm word coded low resolution annotated image pixel contains information trajectory direction curvature recognizer convolution network spatially replicated network output hidden markov model produce word score entire system globally trained minimize word-level error
1992,learning dynamic nature speech back-propagation sequence,novel learning algorithm proposed called back-propagation sequence  particular class dynamic neural network unit local feedback network trained respond sequence input pattern seem particularly suited phoneme recognition exhibit forgetting behavior consequently recently past information taken account classification purpose bps permit online weight updating time complexity space requirement back-propagation  applied feedforward network present experimental  problem connected automatic speech recognition
1992,phonetically motivated acoustic parameter continuous speech recognition using artificial neural network,framework ann/hmm hybrid system phone recognition three specialized anns designed evaluated one anns detects manner articulation two anns describe speech signal term place articulation one used plosive nasal classification one used fricative classification design network inspired acoustic-phonetic knowledge input parameter ann topology desired output representation optimized specific task network experiment reported timit database frame classification error  manner ann   plosive nasal ann   fricative ann  obtained set  sentence  new speaker experiment prototype ann/hmm hybrid system also reported developed algorithm global optimization hybrid system network manner articulation one network place articulation merged single ann output modeled hmm globally optimized hybrid system achieved recognition accuracy   class recognition problem zusammenfassungim kontext eines knn/hmm hybridsystems für phonerkennung wurden drei spezialisierte knn entwickelt und ausgewertet eines dieser knn detektiert die artikulationsart die beiden anderen knn beschreiben da sprachsignal mit merkmalen die den artikulationsort charakterisieren eines der beiden dient zur klassifikation von plosiven bzw nasalen da andere zur frikativklassifikation die entwicklung der netzwerke orientierte sich akustisch-phonetischem wissen die eingabeparameter die knn topologie und die repräsentation der ausgabeknoten wurden im hinblick auf die spezialisierte aufgabe jedes netzwerks optimiert experimente mit der timit sprachstichprobe werden vorgestellt bei der klassifikation von frame wurden für  testsätze von  neuen sprechern folgende fehlerraten erzielt mit dem knn für die artikulationsart   für da plosiv/nasal-knn   und für da frikativ-netzwerk   experimente mit einem knn/hmm-hybridsystem da die vorteile von knn und hmm verbindet wurden durchgeführt wir entwickelten ein verfahren zur globalen optimierung eines solchen system da netz für die artikulationsart und eines für den artikulationsort wurden zu einem netz vereinigt dessen ausgabe mit einem hmm postprozessor modelliert wurde mit diesem global optimierten hybridsystem erzielten wir bei der erkennung von  klassen  eine akkuratheit von résumédans le cadre dun décodeur acoustique-phonétique hybride  trois réseaux de neurones  spécialisés ont été développés et évalués un de ce anns détecte le mode darticulation le deux autres anns décrivent le signal en termes du lieu darticulation un réseau est utilisé pour classifier le consonnes nasales et plosive un autre est utilisé pour la classification de fricative le design de ce réseaux est inspiré par de connaissances acoustiques et phonétiques le entrées la topologie et le codage de sortie ont été optimisés pour chacun de réseaux ceux-ci sont évalués sur la base de données timit le erreurs de classification trame par trame sur un test de  locuteurs indépendants sont de  pour le  mode darticulation de  pour le consonnes nasales et plosive  et de  pour le consonnes fricative  évalue aussi la performance dun système hybride ann/hmm pour lequel été développé un algorithme doptimisation globale de tous le paramètres le réseau dédié au mode darticulation et un réseau dédié au lieu darticulation ont été unis dans un seul ann le sortie de ce nouveau réseau sont modélisées par un hmm ce système hybride atteint un taux derreur de  pour la reconnaissance en parole continue de  class de phonèmes 
1992,global optimization neural network-hidden markov model hybrid,integration multilayered recurrent artificial neural network  hidden markov model  addressed anns suitable approximating function compute new acoustic parameter whereas hmms proven successful modeling temporal structure speech signal approach described ann output constitute sequence observation vector hmm algorithm proposed global optimization parameter  speaker-independent recognition experiment using integrated ann-hmm system timit continuous speech database reported< >
1991,comparative study hybrid acoustic phonetic decoder based artificial neural network,paper compare two hybrid acoustic-phonetic decoder based artificial neural network  evaluate task recognizing stop phone continuous speech independently speaker anns well suited perform detailed phonetic distinction general technique based dynamic programming  particular hidden markov model  proven successful modeling temporal structure speech signal approach described ann output constitute sequence observation vector hmm algorithm proposed global optimization parameter ann/hmm decoder comparative experiment using ann/hmm hybrid decoder another ann-dp hybrid reported timit database
1991,neural network - gaussian mixture hybrid speech recognition density estimation,subject paper integration multi-layered artificial neural network  probability density function gaussian mixture found continuous density hidden markov model  first part paper present ann/hmm hybrid parameter system simultaneously optimized respect single criterion second part paper study relationship density input network density output network experiment presented explore perform density estimation anns
1990,efficient recognition immunoglobulin domain amino acid sequence using neural network,neural network trained using back propagation recognize immunoglobulin domain amino acid sequence program designed identify protein exhibiting domain minimal rate false positive false negative national biomedical research foundation new protein sequence database scanned evaluate performance program recognizing mouse immunoglobulin sequence program correctly recognized   mouse immunoglobulin sequence corresponding recognition efficiency  overall false positive rate  data demonstrate neural network-based search program well suited search sequence characterized well-conserved subsequence
1990,phonetically-based multi-layered neural network vowel classification,vowel sub-component speaker-independent phoneme classification system described architecture vowel classifier based ear model followed set multi-layered neural network  mlnns trained learn recognize articulatory feature like place articulation manner articulation related tongue positionexperiments performed  english vowel showing recognition rate higher  new speaker feature used recognition comparable  obtained vowel diphthong used training pronounced new speaker suggests mlnns suitably fed data computed ear model good generalization capability new speaker new soundszusammenfassungbeschrieben wird eine klassifizierungsstufe für vokale al teil eines sprecherunabhängigen phonemklassifizierungssystems die architektur diesis vokalklassifikators basiert auf einem ohrmodell da von einem satz mehrschichtiger neuronaler netze gefolgt wird diese neuronalen netze werden darauf trainiert artikulatorische merkmale wie zb den ort der artikulation oder die art der artikulation — bezogen auf die position der zunge — zu erkennenexperimente mit  englischen vokalen ergeben eine erkennungsrate von mehr al  für neue dem system bisher unbekannte sprecher werden phonetische merkmale für die erkennung herangezogen lassen sich vergleichbare resultate für solche vokale und diphthonge erreichen die für da training nicht verwendet oder von neuen sprechern geäuβert wurden dy legt nahe daβ mehrschichtige neuronale netze auf passende weise mit den ausgangsdaten eines ohrmodells angesteuert sich bei der erweiterung dieser aufgabe auf neue sprecher oder neue laute al gut geeignet erweisenrésuménous présentons un système de classification de phonèmes indépendant du locuteur et appliqué aux voyelles larchitecture du classificateur de voyelles est basée surun modèle doreille suivi dun ensemble de réseaux neuronaux à plusieurs couch  le mlnns apprennent à reconnaître le trait articulatoires par exemple le lieu et le mode darticulation en relation avec la position de la languedes expériences ont été effectuées sur  voyelles anglaises et montrent un taux de reconnaissance supérieur à  sur de nouveaux locuteurs lorsque le trait sont utilisés pour la reconnaissance de résultats comparables sont obtenus pour de voyelles et de dihthongues qui nont pa été utilisées lors de lapprentissage et prononcées par de nouveaux locuteurs ceci suggère que pour de données calculées par un modèle doreille le mlnns présentent un bon pouvoir de généralisation pour de nouveaux locuteurs et de nouveaux son
1989,programmable execution multi-layered network automatic speech recognition,set multi-layered network allows integration information extracted variable resolution time frequency domain keep number link node network small significant generalization learning reasonable training set size
1989,generalization capability multi-layered network extraction speech property,paper describes speech coding system based ear model followed set multilayer network  mlns trained learn recognize articulatory feature like place manner articulation experiment performed  english vowel showing recognition rate higher  new speaker feature used recognition comparable  obtained vowel diphthong used training pronounced new speaker suggests mlns suitably fed data computed ear model good generalization capability new speaker new sound
1989,speaker independent speech recognition neural network speech knowledge,attempt combine neural network knowledge speech science build speaker independent speech recognition system knowledge utilized designing preprocessing input coding output coding output supervision architectural constraint handle temporal aspect speech combine delay copy activation hidden output unit input level back-propagation sequence  learning algorithm network local self-loops strategy demonstrated several experiment particular nasal discrimination task application speech theory hypothesis dramatically improved generalization
1989,neural network detect homology protein,order detect presence location immunoglobulin  domain amino acid sequence built system based neural network one hidden layer trained back propagation program designed efficiently identify protein exhibiting domain characterized localized conserved region low overall homology national biomedical research foundation  new protein sequence database scanned evaluate program performance obtained low rate false negative coupled moderate rate false positive
1988,data-driven execution multi-layered network automatic speech recognition,set multi-layered network  automatic speech recognition  proposed set allows integration information extracted variable resolution time frequency domain keep number link node network small order allow significant generalization learning reasonable training set size subset network executed depending precondition based description time evolution signal energy allowing spectral property significant different acoustic situation learned preliminary experiment speaker-independent recognition letter e-set reported voice  speaker used learning voice  new speaker used test overall error rate  obtained test showing  better previously reported achieved
1988,use multi-layered network coding speech phonetic feature,preliminary  speaker-independant speech recognition reported method combine expertise neural network expertise speech recognition used build recognition system transient sound event-driven property extractor variable resolution time frequency domain used sonorant speech model human auditory system preferred fft front-end module 
