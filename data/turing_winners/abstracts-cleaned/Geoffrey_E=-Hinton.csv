2019,analyzing improving representation soft nearest neighbor loss,explore expand soft nearest neighbor loss measure entanglement class manifold representation space ie close pair point class relative pair point different class demonstrate several use case loss analytical tool provides insight evolution class similarity structure learning surprisingly find maximizing entanglement representation different class hidden layer beneficial discrimination final layer possibly encourages representation identify class-independent similarity structure maximizing soft nearest neighbor loss hidden layer lead better-calibrated estimate uncertainty outlier data also marginally improved generalization data training distribution recognized observing hidden layer fewer normal number neighbor predicted class
2019,similarity neural network representation revisited,recent work sought understand behavior neural network comparing representation layer different trained model examine method comparing neural network representation based canonical correlation analysis  show cca belongs family statistic measuring multivariate similarity neither cca statistic invariant invertible linear transformation measure meaningful similarity representation higher dimension number data point introduce similarity index measure relationship representational similarity matrix suffer limitation similarity index equivalent centered kernel alignment  also closely connected cca unlike cca cka reliably identify correspondence representation network trained different initialization
2018,said modeling individual labelers improves classification,n/a
2018,illustrative language understanding large-scale visual grounding image search,n/a
2018,large scale distributed neural network training online distillation,n/a
2018,matrix capsule em routing,n/a
2018,assessing scalability biologically-motivated deep learning algorithm architecture,backpropagation error algorithm  impossible implement real brain recent success deep network machine learning ai however inspired proposal understanding brain might learn across multiple layer hence might approximate bp yet none proposal rigorously evaluated task bp-guided deep learning proved critical architecture structured simple fully-connected network present  scaling biologically motivated model deep learning datasets need deep network appropriate architecture achieve good performance present  mnist cifar- imagenet datasets explore variant target-propagation  feedback alignment  algorithm explore performance fully- locally-connected architecture also introduce weight-transport-free variant difference target propagation  modified remove backpropagation penultimate layer many algorithm perform well mnist cifar imagenet find tp fa variant perform significantly worse bp especially network composed locally connected unit opening question whether new architecture algorithm required scale approach  implementation detail help establish baseline biologically motivated deep learning scheme going forward
2017,imagenet classification deep convolutional neural network,n/a
2017,distilling neural network soft decision tree,n/a
2017,regularizing neural network penalizing confident output distribution,n/a
2017,outrageously large neural network sparsely-gated mixture-of-experts layer,n/a
2017,dynamic routing capsule,capsule group neuron whose activity vector represents instantiation parameter specific type entity object object part use length activity vector represent probability entity exists orientation represent instantiation parameter active capsule one level make prediction via transformation matrix instantiation parameter higher-level capsule multiple prediction agree higher level capsule becomes active show discrimininatively trained multi-layer capsule system achieves state-of-the-art performance mnist considerably better convolutional net recognizing highly overlapping digit achieve  use iterative routing-by-agreement mechanism lower-level capsule prefers send output higher level capsule whose activity vector big scalar product prediction coming lower-level capsule
2016,attend infer repeat fast scene understanding generative model,present framework efficient inference structured image model explicitly reason object achieve performing probabilistic inference using recurrent neural network attends scene element process one time crucially model learns choose appropriate number inference step use scheme learn perform inference partially specified model  fully specified model  show model learn identify multiple object - counting locating classifying element scene - without supervision eg decomposing image various number object single forward pas neural network unprecedented speed show network produce accurate inference compared supervised counterpart structure lead improved generalization
2016,using fast weight attend recent past,recently research artificial neural network largely restricted system two type variable neural activity represent current recent input weight learn capture regularity among input output payoff good reason restriction synapsis dynamic many different time-scales suggests artificial neural network might benefit variable change slower activity much faster standard weight thesefast weight used store temporary memory recent past provide neurally plausible way implementing type attention past recently proven helpful sequence-to-sequence model using fast weight avoid need store copy neural activity pattern
2015,guest editorial deep learning,n/a
2015,deep learning,n/a
2015,grammar foreign language,syntactic constituency parsing fundamental problem naturallanguage processing subject intensive researchand engineering decade result accurate parsersare domain specific complex inefficient paper showthat domain agnostic attention-enhanced sequence-to-sequence modelachieves state-of-the-art  widely used syntacticconstituency parsing dataset trained large synthetic corpusthat annotated using existing parser also match theperformance standard parser trained smallhuman-annotated dataset show model highlydata-efficient contrast sequence-to-sequence model without theattention mechanism parser also fast processing ahundred sentence per second unoptimized cpu implementation
2014,feature come,n/a
2014,dropout simple way prevent neural network overfitting,n/a
2014,application deep belief network natural language understanding,application deep belief net  various problem subject number recent study ranging image classification speech recognition audio classification study apply dbns natural language understanding problem recent surge activity area largely spurred development greedy layer-wise pretraining method us efficient learning algorithm called contrastive divergence  cd allows dbns learn multi-layer generative model unlabeled data feature discovered model used initialize feed-forward neural network fine-tuned backpropagation compare dbn-initialized neural network three widely used text classification algorithm support vector machine  boosting maximum entropy  plain dbn-based model give call-routing classification accuracy equal best model however using additional unlabeled data dbn pre-training combining dbn-based learned feature original feature provides significant gain svms turn performed better maxent boosting
2014,autoregressive product multi-frame prediction improve accuracy hybrid model,n/a
2013,modeling natural image using gated mrfs,paper describes markov random field real-valued image modeling two set latent variable one set used gate interaction pair pixel second set determines mean intensity pixel powerful model conditional distribution input gaussian mean covariance determined configuration latent variable unlike previous model restricted using gaussians either fixed mean diagonal covariance matrix thanks increased flexibility gated mrf generate realistic sample training unconstrained distribution high-resolution natural image furthermore latent variable model inferred efficiently used effective descriptor recognition task generation discrimination drastically improve layer binary latent variable added model yielding hierarchical model called deep belief network
2013,rectified linear unit speech processing,deep neural network recently become gold standard acoustic modeling speech recognition system key computational unit deep network linear projection followed point-wise non-linearity typically logistic function work show improve generalization make training deep network faster simpler substituting logistic unit rectified linear unit unit linear input positive zero otherwise supervised setting successfully train deep net random initialization large vocabulary speech recognition task achieving lower word error rate using logistic network topology similarly unsupervised setting show learn sparse feature useful discriminative task experiment executed distributed environment using several hundred machine several hundred hour speech data
2013,speech recognition deep recurrent neural network,recurrent neural network  powerful model sequential data end-to-end training method connectionist temporal classification make possible train rnns sequence labelling problem input-output alignment unknown combination method long short-term memory rnn architecture proved particularly fruitful delivering state-of-the-art  cursive handwriting recognition however rnn performance speech recognition far disappointing better  returned deep feedforward network paper investigates deep recurrent neural network combine multiple level representation proved effective deep network flexible use long range context empowers rnns trained end-to-end suitable regularisation find deep long short-term memory rnns achieve test set error  timit phoneme recognition benchmark knowledge best recorded score
2013,new type deep neural network learning speech recognition related application overview,paper provide overview invited contributed paper presented special session icassp- entitled “new type deep neural network learning speech recognition related applications” organized author also describe historical context acoustic model based deep neural network developed technical overview paper presented special session organized five way improving deep learning method  better optimization  better type neural activation function better network architecture  better way determine myriad hyper-parameters deep neural network  appropriate way preprocess speech deep neural network  way leveraging multiple language dialect easily achieved deep neural network gaussian mixture model
2013,improving deep neural network lvcsr using rectified linear unit dropout,recently pre-trained deep neural network  outperformed traditional acoustic model based gaussian mixture model  variety large vocabulary speech recognition benchmark deep neural net also achieved excellent  various computer vision task using random “dropout” procedure drastically improves generalization error randomly omitting fraction hidden unit layer since dropout help avoid over-fitting also successful small-scale phone recognition task using larger neural net however training deep neural net acoustic model large vocabulary speech recognition take long time dropout likely increase training time neural network rectified linear unit  non-linearities highly successful computer vision task proved faster train standard sigmoid unit sometimes also improving discriminative performance work show -hour english broadcast news task modified deep neural network using relus trained dropout frame level training provide  relative improvement dnn trained sigmoid unit  relative improvement strong gmm/hmm system able obtain  minimal human hyper-parameter tuning using publicly available bayesian optimization code
2013,tensor analyzer,factor analysis statistical method seek explain linear variation data using unobserved latent variable due additive nature suitable modeling data generated multiple group latent factor interact multiplicatively paper introduce tensor analyzer multilinear generalization factor analyzer describe efficient way sampling posterior distribution factor value demonstrate sample used em algorithm learning interesting mixture model natural image patch tensor analyzer also accurately recognize face significant pose illumination variation given one previous image face also show tensor analyzer trained unsupervised semi-supervised fully supervised setting
2013,importance initialization momentum deep learning,deep recurrent neural network  powerful model considered almost impossible train using stochastic gradient descent momentum paper show stochastic gradient descent momentum us well-designed random initialization particular type slowly increasing schedule momentum parameter train dnns rnns  level performance previously achievable hessian-free optimization find initialization momentum crucial since poorly initialized network cannot trained momentum well-initialized network perform markedly worse momentum absent poorly tuned success training model suggests previous attempt train deep recurrent neural network random initialization likely failed due poor initialization scheme furthermore carefully tuned momentum method suffice dealing curvature issue deep recurrent network training objective without need sophisticated second-order method
2013,using autoencoder deformable template discover feature automated speech recognition,n/a
2013,modeling document deep boltzmann machine,n/a
2012,visualizing non-metric similarity multiple map,technique multidimensional scaling visualize object point low-dimensional metric map result visualization subject fundamental limitation metric space limitation prevent multidimensional scaling faithfully representing non-metric similarity data word association event co-occurrence particular multidimensional scaling cannot faithfully represent intransitive pairwise similarity visualization cannot faithfully visualize “central” object paper present extension recently proposed multidimensional scaling technique called t-sne extension aim address problem traditional multidimensional scaling technique technique used visualize non-metric similarity new technique called multiple map t-sne alleviates problem constructing collection map reveal complementary structure similarity data apply multiple map t-sne large data set word association data data set nip co-authorships demonstrating ability successfully visualize non-metric similarity
2012,efficient learning procedure deep boltzmann machine,n/a
2012,introduction special section deep learning speech language processing,current speech recognition system example typically use gaussian mixture model  estimate observation  probability hidden markov model  gmms generative model one layer latent variable instead developing powerful model research effort gone finding better way estimating gmm parameter error rate decreased margin different class increased observation hold natural language processing  maximum entropy  model conditional random field  popular last decade approach use shallow model whose success largely depends use carefully handcrafted feature
2012,acoustic modeling using deep belief network,gaussian mixture model currently dominant technique modeling emission distribution hidden markov model speech recognition show better phone recognition timit dataset achieved replacing gaussian mixture model deep neural network contain many layer feature large number parameter network first pre-trained multi-layer generative model window spectral feature vector without making use discriminative information generative pre-training designed feature perform discriminative fine-tuning using backpropagation adjust feature slightly make better predicting probability distribution state monophone hidden markov model
2012,robust boltzmann machine recognition denoising,boltzmann machine successful unsupervised learning density modeling image speech data sensitive noise data paper introduce novel model robust boltzmann machine  allows boltzmann machine robust corruption domain visual recognition robm able accurately deal occlusion noise using multiplicative gating induce scale mixture gaussians pixel image denoising in-painting correspond posterior inference robm model trained unsupervised fashion unlabeled noisy data learn spatial structure occluders compared standard algorithm robm significantly better recognition denoising several face database
2012,understanding deep belief network perform acoustic modelling,deep belief network  competitive alternative gaussian mixture model relating state hidden markov model frame coefficient derived acoustic input competitive three reason dbns fine-tuned neural network dbns many non-linear hidden layer dbns generatively pre-trained paper illustrates three aspect contributes dbns good recognition performance using phone recognition performance timit corpus dimensionally reduced visualization relationship feature vector learned dbns preserve similarity structure feature vector multiple scale two method also used investigate suitable type input representation dbn
2012,learning label aerial image noisy data,n/a
2012,deep mixture factor analyser,n/a
2012,deep lambertian network,n/a
2012,better way pretrain deep boltzmann machine,describe pre-training algorithm deep boltzmann machine  related pre-training algorithm deep belief network show certain condition pre-training procedure improves variational lower bound two-hidden-layer dbm based analysis develop different method pre-training dbms distributes modelling work evenly hidden layer  mnist norb datasets demonstrate new pre-training algorithm allows u learn better generative model
2011,better way learn feature technical perspective,n/a
2011,two distributed-state model generating high-dimensional time series,n/a
2011,discovering binary code document learning deep generative model,n/a
2011,modeling joint density two image variety transformation,describe generative model relationship two image model defined factored three-way boltzmann machine hidden variable collaborate define joint correlation matrix image pair modeling joint distribution pair make possible efficiently match image according learned measure similarity apply model several face matching task show learns represent input image using task-specific basis function matching performance superior previous similar generative model including recent conditional model transformation also show model used plug-in matching score perform invariant classification
2011,deep generative model application recognition,popular way use probabilistic model vision first extract descriptor small image patch object part using well-engineered feature use statistical learning tool model dependency among feature eventual label learning probabilistic model directly raw pixel value proved much difficult typically used regularizing discriminative method work use one best pixel-level generative model natural images-a gated mrf-as lowest level deep belief network  several hidden layer show resulting dbn good coping occlusion predicting expression category face image produce feature perform comparably sift descriptor discriminating different type scene generative ability model also make easy see information captured lost level representation
2011,using deep autoencoders content-based image retrieval,n/a
2011,transforming auto-encoders,artificial neural network used recognize shape typically use one layer learned feature detector produce scalar output contrast computer vision community us complicated hand-engineered feature like sift  produce whole vector output including explicit representation pose feature show neural network used learn feature output whole vector instantiation parameter argue much promising way dealing variation position orientation scale lighting method currently employed neural network community also promising hand-engineered feature currently used computer vision provides efficient way adapting feature domain
2011,deep belief network using discriminative feature phone recognition,deep belief network  multi-layer generative model trained model window coefficient extracted speech discover multiple layer feature capture higher-order statistical structure data feature used initialize hidden unit feed-forward neural network trained predict hmm state central frame window initializing feature good generating speech make neural network perform much better initializing random weight dbns already used successfully phone recognition input coefficient mfccs filterbank output paper demonstrate work even better input speaker adaptive discriminative feature standard timit corpus give phone error rate  using monophone hmms bigram language model  using monophone hmms trigram language model
2011,deep belief net natural language call-routing,paper considers application deep belief net  natural language call routing dbns successfully applied number task including image audio speech classification thanks recent discovery efficient learning technique dbns learn multi-layer generative model unlabeled data feature discovered model used initialize feed-forward neural network fine-tuned backpropagation compare dbn-initialized neural network three widely used text classification algorithm support vector machine  boosting maximum entropy  dbn-based model give call-routing classification accuracy equal best model even though currently us impoverished representation input
2011,learning better representation speech soundwaves using restricted boltzmann machine,state art speech recognition system rely preprocessed speech feature mel cepstrum linear predictive coding coefficient collapse high dimensional speech sound wave low dimensional encoding successfully applied speech recognition system low dimensional encoding may lose relevant information express information way make difficult use discrimination higher dimensional encoding could improve performance recognition task also applied speech synthesis better modeling statistical structure sound wave paper present novel approach modeling speech sound wave using restricted boltzmann machine  novel type hidden variable report initial  demonstrating phoneme recognition performance better current state-of-the-art method based mel cepstrum coefficient
2011,generating text recurrent neural network,n/a
2011,conditional restricted boltzmann machine structured output prediction,n/a
2010,learning represent spatial transformation factored higher-order boltzmann machine,n/a
2010,comparing classification method longitudinal fmri study,n/a
2010,temporal-kernel recurrent neural network,recurrent neural network  powerful connectionist model applied many challenging sequential problem including problem naturally arise language speech however rnns extremely hard train problem long-term dependency necessary remember event many timesteps using make predictionin paper consider problem training rnns predict sequence exhibit significant long-term dependency focusing serial recall task rnn need remember sequence character large number step reconstructing introduce temporal-kernel recurrent neural network  variant rnn cope long-term dependency much easily standard rnn show tkrnn develops short-term memory successfully solves serial recall task representing input string stable state hidden unit
2010,dynamical binary latent variable model 3d human pose tracking,introduce new class probabilistic latent variable model called implicit mixture conditional restricted boltzmann machine  use human pose tracking key property imcrbm follows  learning linear number training exemplar learned large datasets  learns coherent model multiple activity  automatically discovers atomic “movemes”  infer transition activity even transition present training set describe model learned demonstrate use context bayesian filtering multi-view monocular pose tracking model handle difficult scenario including multiple activity transition among activity report state-of-the-art  humaneva dataset
2010,modeling pixel mean covariance using factorized third-order boltzmann machine,learning generative model natural image useful way extracting feature capture interesting regularity previous work learning model focused method latent feature used determine mean variance pixel independently method hidden unit determine covariance matrix zero-mean gaussian distribution work propose probabilistic model combine two approach single framework represent image using one set binary latent feature model image-specific covariance separate set model mean show approach provides probabilistic framework widely used simple-cell complex-cell architecture produce realistic sample natural image extract feature yield state-of-the-art recognition accuracy challenging cifar  dataset
2010,learning detect road high-resolution aerial image,reliably extracting information aerial imagery difficult problem many practical application one specific case problem task automatically detecting road task difficult vision problem occlusion shadow wide variety non-road object despite  year work automatic road detection automatic semi-automatic road detection system currently market published method shown work reliably large datasets urban imagery propose detecting road using neural network million trainable weight look much larger context used previous attempt learning task network trained massive amount data using consumer gpu demonstrate predictive performance substantially improved initializing feature detector using recently developed unsupervised learning method well taking advantage local spatial coherence output label show method work reliably two challenging urban datasets order magnitude larger used evaluate previous approach
2010,phone recognition using restricted boltzmann machine,decade hidden markov model  state-of-the-art technique acoustic modeling despite unrealistic independence assumption limited representational capacity hidden state conditional restricted boltzmann machine  recently proved effective modeling motion capture sequence paper investigates application powerful type generative model acoustic modeling standard timit corpus one type crbm outperforms hmms comparable best method achieving phone error rate   timit core test set
2010,rectified linear unit improve restricted boltzmann machine,n/a
2010,binary coding speech spectrogram using deep auto-encoder,n/a
2010,phone recognition mean-covariance restricted boltzmann machine,straightforward application deep belief net  acoustic modeling produce rich distributed representation speech data useful recognition yield impressive  speaker-independent timit phone recognition task however first-layer gaussian-bernoulli restricted boltzmann machine  important limitation shared mixture diagonal-covariance gaussians grbms treat different component acoustic input vector conditionally independent given hidden state mean-covariance restricted boltzmann machine  first introduced modeling natural image much representationally efficient powerful way modeling covariance structure speech data every configuration precision unit mcrbm specifies different precision matrix conditional distribution acoustic space work use mcrbm learn feature speech data serve input standard dbn mcrbm feature combined dbns allow u achieve phone error rate  superior published  speaker-independent timit date
2010,learning combine foveal glimpse third-order boltzmann machine,describe model based boltzmann machine third-order connection learn accumulate information shape several fixation model us retina enough high resolution pixel cover small area image must decide sequence fixation must combine glimpse fixation location fixation integrating information information glimpse object evaluate model synthetic dataset two image classification datasets showing perform least well model trained whole image
2010,gated softmax classification,describe log-bilinear model computes class probability combining input vector multiplicatively vector binary latent variable even though latent variable take exponentially many possible combination value efficiently compute exact probability class marginalizing latent variable make possible get exact gradient log likelihood bilinear score-functions defined using three-dimensional weight tensor show factorizing tensor allows model encode invariance inherent task learning dictionary invariant basis function experiment set benchmark problem show fully probabilistic model achieve classification performance competitive  svms backpropagation deep belief net
2010,generating realistic image using gated mrfs,probabilistic model natural image usually evaluated measuring performance rather indirect task denoising inpainting direct way evaluate generative model draw sample check whether statistical property sample match statistic natural image method seldom used high-resolution image current model produce sample different natural image assessed even simple visual inspection investigate reason failure show augmenting existing model two set latent variable one set modelling pixel intensity set modelling image-specific pixel covariance able generate high-resolution image look much realistic overall model interpreted gated mrf pair-wise dependency mean intensity pixel modulated state latent variable finally confirm disallow weight-sharing receptive field overlap gated mrf learns efficient internal representation demonstrated several recognition task
2010,factored 3-way restricted boltzmann machine modeling natural image,deep belief net successful modeling handwritten character proved difficult apply real image problem lie restricted boltzmann machine  used module learning deep belief net one layer time gaussian-binary rbms used model real-valued data good way model covariance structure natural image propose factored -way rbm us state hidden unit represent abnormality local covariance structure image provides probabilistic framework widely used simple/complex cell architecture model learns binary feature work well object recognition “tiny images” data set even better feature obtained using standard binary rbm’s learn deeper model
2009,semantic hashing,show learn deep graphical model word-count vector obtained large set document value latent variable deepest layer easy infer give much better representation document latent semantic analysis deepest layer forced use small number binary variable  graphical model performs “semantic hashing” document mapped memory address way semantically similar document located nearby address document similar query document found simply accessing address differ bit address query document way extending efficiency hash-coding approximate matching much faster locality sensitive hashing fastest current method using semantic hashing filter document given tf-idf achieve higher accuracy applying tf-idf entire document set
2009,improving statistical language model non-linear prediction,show improve state-of-the-art neural network language model convert previous “context” word feature vector combine feature vector linearly predict feature vector next word significant improvement predictive accuracy achieved using non-linear subnetwork modulate effect context word produce non-linear correction term predicting feature vector log-bilinear language model incorporates improvement achieves  reduction perplexity best n-gram model fairly large dataset
2009,deep belief network,n/a
2009,learning generative texture model extended fields-of-experts,n/a
2009,modeling pigeon behavior using conditional restricted boltzmann machine,n/a
2009,workshop summary workshop learning feature hierarchy,n/a
2009,factored conditional restricted boltzmann machine modeling motion style,n/a
2009,using fast weight improve persistent contrastive divergence,n/a
2009,3d object recognition deep belief net,introduce new type deep belief net evaluate object recognition task top-level model third-order boltzmann machine trained using hybrid algorithm combine generative discriminative gradient performance evaluated norb database contains stereo-pair image object different lighting condition viewpoint model achieves  error test set close best published result norb  using convolutional neural net built-in knowledge translation invariance substantially outperforms shallow model svms  dbns especially suited semi-supervised learning demonstrate consider modified version norb recognition task additional unlabeled image created applying small translation image database extra unlabeled data  model achieves  error making current best result norb
2009,zero-shot learning semantic output code,consider problem zero-shot learning goal learn classifier f x  ythat must predict novel value ythat omitted training set achieve define notion semantic output code classifier  utilizes knowledge base semantic property yto extrapolate novel class provide formalism type classifier study theoretical property pac framework showing condition classifier accurately predict novel class case study build soc classifier neural decoding task show often predict word people thinking functional magnetic resonance image  neural activity even without training example word
2009,replicated softmax undirected topic model,show model document bag word using family two-layer undirected graphical model member family number binary hidden unit different number ofsoftmax visible unit softmax unit model family share weight binary hidden unit describe efficient inference learning procedure family member family model probability distribution document specific length product topic-specific distribution rather mixture give much better generalization latent dirichlet allocation modeling log probability held-out document low-dimensional topic vector learned undirected family also much better lda topic vector retrieving document similar query document learned topic general found lda precision achieved intersecting many general topic rather selecting single precise topic generate word
2009,product hidden markov model take n>1 tango,n/a
2009,deep boltzmann machine,present new learning algorithm boltzmann machine contain many layer hidden variable data-dependent expectation estimated using variational approximation tends focus single mode data-independent expectation approximated using persistent markov chain use two quite different technique estimating two type expectation enter gradient log-likelihood make practical learn boltzmann machine multiple hidden layer million parameter learning made efficient using layer-by-layer “pre-training” phase allows variational inference initialized single bottom-up pas present  mnist norb datasets showing deep boltzmann machine learn good generative model perform well handwritten digit visual object recognition task
2008,deep narrow sigmoid belief network universal approximators,n/a
2008,improving statistical language model modulating effect context word,n/a
2008,analysis-by-synthesis learning invert generative black box,learning meaningful representation data rich source prior knowledge may come form generative black box eg graphic program generates realistic facial image consider problem learning inverse given generative model data problem non-trivial difficult create labelled training case hand generative mapping black box sense analytic expression gradient describe way training feedforward neural network start one labelled training example us generative black box “breed” training data learning proceeds training set evolves label network assigns unlabelled training data converge correct value demonstrate approach learning invert generative model eye active appearance model face
2008,scalable hierarchical distributed language model,neural probabilistic language model  shown competitive occasionally superior widely-used n-gram language model main drawback nplms extremely long training testing time morin bengio proposed hierarchical language model built around binary tree word two order magnitude faster non-hierarchical language model based however performed considerably worse non-hierarchical counterpart spite using word tree created using expert knowledge introduce fast hierarchical language model along simple feature-based algorithm automatic construction word tree data show resulting model outperform non-hierarchical model achieve state-of-the-art performance
2008,implicit mixture restricted boltzmann machine,present mixture model whose component restricted boltzmann machine  possibility considered computing partition function rbm intractable appears make learning mixture rbms intractable well surprisingly formulated third-order boltzmann machine mixture model learned tractably using contrastive divergence energy function model capture three-way interaction among visible unit hidden unit single hidden multinomial unit represents cluster label distinguishing feature model unlike mixture model mixing proportion explicitly parameterized instead defined implicitly via energy function depend parameter model present  mnist norb datasets showing implicit mixture rbms learns cluster reflect class structure data
2008,generative versus discriminative training rbms classification fmri image, missing
2008,using matrix model symbolic relationship,describe way learning matrix representation object relationship goal learning allow multiplication matrix represent symbolic relationship object symbolic relationship relationship main novelty method demonstrate lead excellent generalization two different domain modular arithmetic family relationship show system learn first-order proposition   +!or  higher-order proposition   plusand   inverseor  higher demonstrate system understands higher-order proposition related first-order one showing correctly answer question first-order proposition involving relation +!or haseven though trained first-order example involving relation
2008,recurrent temporal restricted boltzmann machine,temporal restricted boltzmann machine  probabilistic model sequence able successfully model  several high dimensional sequence motion capture data pixel low resolution video ball bouncing box major disadvantage trbm exact inference extremely hard since even computing gibbs update single variable posterior exponentially expensive difficulty necessitated use heuristic inference procedure nonetheless accurate enough successful learning paper introduce recurrent trbm slight modification trbm exact inference easy exact gradient learning almost tractable demonstrate rtrbm better analogous trbm generating motion capture video bouncing ball
2007,boltzmann machine,n/a
2007,unsupervised learning image transformation,describe probabilistic model learning rich distributed representation image transformation basic model defined gated conditional random field trained predict transformation input using factorial set latent variable inference model consists extracting transformation given pair image performed exactly efficiently show trained natural video model develops domain specific motion feature form field locally transformed edge filter trained affine general transformation still image model develops code transformation subsequently perform recognition task invariant transformation also fantasize new transformation previously unseen image describe several variation basic model provide experimental  demonstrate applicability variety task
2007,three new graphical model statistical language modelling,n/a
2007,restricted boltzmann machine collaborative filtering,n/a
2007,modeling image patch directed hierarchy markov random field,describe efficient learning procedure multilayer generative model combine best aspect markov random field deep directed belief net generative model learned one layer time learning complete fast inference procedure computing good approximation posterior distribution hidden layer hidden layer mrf whose energy function modulated top-down directed connection layer generate model layer turn must settle equilibrium given top-down input show type model good capturing statistic patch natural image
2007,using deep belief net learn covariance kernel gaussian process,show use unlabeled data deep belief net  learn good covariance kernel gaussian process first learn deep generative model unlabeled data using fast greedy algorithm introduced hinton etal data high-dimensional highly-structured gaussian kernel applied top layer feature dbn work much better similar kernel applied raw input performance regression classification improved using backpropagation dbn discriminatively fine-tune covariance kernel
2007,visualizing similarity data mixture map,show visualize set pairwise similarity object using several different two-dimensional map capture different aspect similarity structure object ambiguous word example different sens word occur different map “river” “loan” close “bank” without close aspect map resemble clustering model pair-wise similarity mixture different type similarity also resemble local multi-dimensional scaling model type similarity twodimensional map demonstrate method toy example database human wordassociation data large set image handwritten digit set feature vector represent word
2007,learning nonlinear embedding preserving class neighbourhood structure,show pretrain fine-tune multilayer neural network learn nonlinear transformation input space lowdimensional feature space k-nearest neighbour classification performs well also show non-linear transformation improved using unlabeled data method achieves much lower error rate support vector machine standard backpropagation widely used version mnist handwritten digit recognition task dimension low-dimensional feature space used nearest neighbor classification method us dimension explicitly represent transformation digit affect identity
2007,learning multilevel distributed representation high-dimensional sequence,describe new family non-linear sequence model substantially powerful hidden markov model linear dynamical system model simple approximate inference learning procedure work well practice multilevel representation sequential data learned one hidden layer time adding extra hidden layer improves resulting generative model model trained high-dimensional non-linear data raw pixel sequence performance demonstrated using synthetic video sequence two ball bouncing box
2006,unsupervised discovery nonlinear structure using contrastive backpropagation,n/a
2006,topographic product model applied natural scene statistic,n/a
2006,fast learning algorithm deep belief net,n/a
2006,modeling human motion using binary latent variable, missing
2005,improving dimensionality reduction spectral gradient descent,introduce spectral gradient descent way improving iterative dimensionality reduction technique method us information contained leading eigenvalue data affinity matrix modify step taken gradient-based optimization procedure show approach able speed optimization help dimensionality reduction method find better local minimum objective function also provide interpretation approach term power method finding leading eigenvalue symmetric matrix verify usefulness approach simple experiment
2005,contrastive divergence learning,n/a
2005,learning causally linked markov random field,n/a
2005,kind graphical model brain,n/a
2005,inferring motor program image handwritten digit, missing
2004,reinforcement learning factored state action,n/a
2004,probabilistic sequential independent component analysis,under-complete model derive lower dimensional representation input data valuable domain number input dimension large data consisting temporal sequence image paper present under-complete product expert  expert model one-dimensional projection data maximum-likelihood learning rule model constitute tractable exact algorithm learning under-complete independent component learning rule model coincide approximate learning rule proposed earlier under-complete independent component analysis  model paper also derives efficient sequential learning algorithm model discus relationship sequential independent component analysis  projection pursuit density estimation feature induction algorithm additive random field model paper demonstrates efficacy novel algorithm high-dimensional continuous datasets
2004,neighbourhood component analysis, missing
2004,multiple relational embedding, missing
2004,exponential family harmonium application information retrieval, missing
2003,energy-based model sparse overcomplete representation,n/a
2003,wormhole improve contrastive divergence, missing
2003,efficient parametric projection pursuit density estimation,n/a
2002,memory ray reiter 1939-2002,ray dedicated life research wonder child fearlessness explorer precision mathematician tirelessness researcher found shallowness confusion intolerable leaf legacy groundbreaking deep insight changed course ai
2002,local physical model interactive character animation,n/a
2002,training product expert minimizing contrastive divergence,n/a
2002,recognizing handwritten digit using hierarchical product expert,product expert learning procedure discover set stochastic binary feature constitute nonlinear generative model handwritten image digit quality generative model learned way assessed learning separate model class digit comparing unnormalized probability test image  different class-specific model improve discriminative performance hierarchy separate model learned digit class model hierarchy learns layer binary feature detector model probability distribution vector activity feature detector layer model hierarchy trained sequentially model us layer binary feature detector learn generative model pattern feature activity preceding layer training layer feature detector produce separate unnormalized log probability score three layer feature detector  digit class test image produce  score used input supervised logistic classification network trained separate data
2002,desktop input device interface interactive 3d character animation,n/a
2002,new learning algorithm mean field boltzmann machine,present new learning algorithm mean field boltzmann machine based contrastive divergence optimization criterion addition minimizing divergence data distribution equilibrium distribution maximize divergence one-step reconstruction data equilibrium distribution eliminates need estimate equilibrium statistic need approximate multimodal probability distribution free network unimodal mean field distribution test learning algorithm classification digit
2002,self supervised boosting, missing
2002,stochastic neighbor embedding, missing
2002,learning sparse topographic representation product student-t distribution, missing
2001,learning distributed representation concept using linear relational embedding,introduce linear relational embedding mean learning distributed representation concept data consisting binary relation concept key idea represent concept vector binary relation matrix operation applying relation concept matrix-vector multiplication produce approximation related concept representation concept relation learned maximizing appropriate discriminative goodness function using gradient ascent task involving family relationship learning fast lead good generalization
2001,product hidden markov model,n/a
2001,learning hierarchical structure linear relational embedding, missing
2001,global coordination local linear model, missing
2001,relative density net new way combine backpropagation hmms, missing
2001,discovering multiple constraint frequently approximately satisfied,n/a
2000,variational learning switching state-space model,n/a
2000,smem algorithm mixture model,n/a
2000,split merge em algorithm improving gaussian mixture density estimate,n/a
2000,modeling high-dimensional data combining simple expert,possible combine multiple non-linear probabilistic model data multiplying probability distribution together renormalizing product expert efficient way model data simultaneously satisfies many different constraint difficult fit product expert data using maximum likelihood gradient log likelihood intractable efficient way optimizing different objective function produce good model high-dimensional data
2000,learning distributed representation mapping concept relation linear space,n/a
2000,extracting distributed representation concept relation positive negative proposition,linear relational embedding  introduced previously author  mean extracting distributed representation concept relational data original formulation cannot use negative information cannot properly handle data multiple correct answer paper propose extended formulation lre solves problem present  two simple domain show learning lead good generalization
2000,rate-coded restricted boltzmann machine face recognition, missing
2000,recognizing hand-written digit using hierarchical product expert, missing
2000,using free energy represent q-values multiagent reinforcement learning task, missing
1999,variational learning nonlinear gaussian belief network,n/a
1999,spiking boltzmann machine, missing
1999,learning parse image, missing
1998,coaching variable regression classification,regression classification setting wish predict xx xp suppose additional set ‘coaching’ variable zz zm available training sample might variable difficult measure available predict xx xp future consider two method making use coaching variable order improve prediction xx xp relative merit approach discussed compared number example
1998,glove-talkii-a neural-network interface map gesture parallel formant speech synthesizer control,glove-talkii system translates hand gesture speech adaptive interface hand gesture mapped continuously ten control parameter parallel formant speech synthesizer mapping allows hand act artificial vocal tract produce speech real time give unlimited vocabulary addition direct control fundamental frequency volume currently best version glove-talkii us several input device  parallel formant speech synthesizer three neural network gesture-to-speech task divided vowel consonant production using gating network weight output vowel consonant neural network gating network consonant network trained example user vowel network implement fixed user-defined relationship hand position vowel sound require training example user volume fundamental frequency stop consonant produced fixed mapping input device one subject trained speak intelligibly glove-talkii speaks slowly far natural sounding pitch variation text-to-speech synthesizer
1998,fast neural network emulation dynamical system computer animation, missing
1998,neuroanimator fast neural network emulation control physics-based model,n/a
1997,efficient stochastic source coding application bayesian network source model,n/a
1997,instantiating deformable model neural net,deformable model attractive approach recognizing object considerable within-class variability handwritten character however severe search problem associated fitting model data could reduced better starting point search available show training neural network predict deformable model instantiated input image improved starting point obtained method implemented system recognizes handwritten digit using deformable model  show search time significantly reduced without compromising recognition performance
1997,using expectation-maximization reinforcement learning,n/a
1997,mobile robot learns place,n/a
1997,modeling manifold image handwritten digit,paper describes two new method modeling manifold digitized image handwritten digit model allow priori information structure manifold combined empirical data accurate modeling manifold allows digit discriminated using relative probability density alternative model one method grounded principal component analysis factor analysis method based locally linear low-dimensional approximation underlying data manifold link method model manifold discussed
1997,hierarchical non-linear factor analysis topographic map, missing
1997,learning fast neural network emulator physics-based model,n/a
1996,variety helmholtz machine,helmholtz machine new unsupervised learning architecture us top-down connection build probability density model input bottom-up connection build inverse model wake-sleep learning algorithm machine involves purely local delta rule paper suggests number different variety helmholtz machine strength weakness relates cortical information processing copyright ©  elsevier science ltd
1996,using generative model handwritten digit recognition,describe method recognizing handwritten digit fitting generative model built deformable b-splines gaussian ink generator spaced along length spline spline adjusted using novel elastic matching procedure based expectation maximization algorithm maximizes likelihood model generating data approach many advantage  system produce classification digit also rich description instantiation parameter yield information writing style  generative model perform recognition driven segmentation  method involves relatively small number parameter hence training relatively easy fast  unlike many recognition scheme rely form pre-normalization input image handle arbitrary scaling translation limited degree image rotation demonstrated method fitting model image get trapped poor local minimum main disadvantage method requires much computation standard ocr technique
1996,free energy coding,introduce new approach problem optimal compression source code produce multiple codewords given symbol may seem sensible codeword use case shortest one however proposed free energy approach random codeword selection yield effective codeword length le shortest codeword length random choice boltzmann distributed effective length optimal given source code expectation-maximization parameter estimation algorithm minimize effective codeword length illustrate performance free energy coding simple problem compression factor two gained using new method
1995,learning population code minimizing description length,n/a
1995,helmholtz machine,n/a
1995,glovetalkii adaptive gesture-to-formant interface,n/a
1995,using pair data-points define split decision tree, missing
1995,wake-sleep algorithm produce good density estimator, missing
1994,alternative model mixture expert, missing
1994,glove-talkii mapping hand gesture speech using neural network, missing
1994,using neural net instantiate deformable model, missing
1994,recognizing handwritten digit using mixture linear model, missing
1993,learning mixture model spatial coherence,n/a
1993,soft decision-directed lm algorithm blind equalization,adaptation algorithm equalizer operating distorted channel presented algorithm based idea adjusting equalizer tap gain maximize likelihood equalizer output would generated mixture two gaussians known mean decision-directed least-mean-square algorithm shown approximation maximizing likelihood equalizer output come independently identically distributed source algorithm developed context binary pulse-amplitude-modulation channel simulation demonstrate algorithm converges channel decision-directed lm algorithm converge< >
1993,glove-talk neural network interface data-glove speech synthesizer,illustrate potential multilayer neural network adaptive interface vpl data-glove connected dectalk speech synthesizer via five neural network used implement hand-gesture speech system using minor variation standard backpropagation learning procedure complex mapping hand movement speech learned using data obtained single speaker simple training phase  gesture-to-word vocabulary wrong word produced le  time word produced  time adaptive control speaking rate word stress also available training time final performance speed improved using small separate network naturally defined subtask system demonstrates neural network used develop complex mapping required high bandwidth interface adapts individual user< >
1993,keeping neural network simple minimizing description length weight,n/a
1993,autoencoders minimum description length helmholtz free energy, missing
1993,developing population code minimizing description length, missing
1992,simplifying neural network soft weight-sharing,n/a
1992,feudal reinforcement learning, missing
1991,adaptive mixture local expert,n/a
1991,learning make coherent prediction domain discontinuity, missing
1991,adaptive elastic model hand-printed character recognition, missing
1991,adaptive soft weight tying using gaussian mixture, missing
1990,connectionist symbol processing - preface,n/a
1990,mapping part-whole hierarchy connectionist network,three different way mapping part-whole hierarchy connectionist network described simplest scheme us fixed mapping inadequate task fails share unit connection different piece part-whole hierarchy two alternative scheme described involves different method time-sharing connection unit scheme finally arrive suggests neural network two quite different method performing inference simple “intuitive” inference performed single settling network without changing way world mapped network complex “rational” inference involve sequence settling mapping change settling
1990,bootstrap widrow-hoff rule cluster-formation algorithm,n/a
1990,time-delay neural network architecture isolated word recognition,translation-invariant back-propagation network described performs better sophisticated continuous acoustic parameter hidden markov model noisy -speaker confusable vocabulary isolated word recognition task network replicated architecture permit extract precise information unaligned training pattern selected naive segmentation rule
1990,building adaptive interface neural network glove-talk pilot study,n/a
1990,discovering viewpoint-invariant relationship characterize object, missing
1990,evaluation adaptive mixture competing expert, missing
1989,connectionist learning procedure,major goal research network neuron-like processing unit discover efficient learning procedure allow network construct complex internal representation environment learning procedure must capable modifying connection strength way internal unit part input output come represent important feature task domain several interesting gradient-descent procedure recently discovered connection computes derivative respect connection strength global measure error performance network strength adjusted direction decrease error relatively simple gradient-descent learning procedure work well small task new challenge find way improving convergence rate generalization ability applied larger realistic task
1989,deterministic boltzmann learning performs steepest descent weight-space,n/a
1989,phoneme recognition using time-delay neural network,author present time-delay neural network  approach phoneme recognition characterized two important property  using three-layer arrangement simple computing unit hierarchy constructed allows formation arbitrary nonlinear decision surface tdnn learns automatically using error backpropagation  time-delay arrangement enables network discover acoustic-phonetic feature temporal relationship independently position time therefore blurred temporal shift input recognition task speaker-dependent recognition phoneme b g varying phonetic context chosen comparison several discrete hidden markov model  trained perform task performance evaluation  testing token three speaker showed tdnn achieves recognition rate  correct rate obtained best hmms < >
1989,dimensionality reduction prior knowledge e-set recognition, missing
1989,traffic recognizing object using hierarchical reference frame transformation, missing
1989,discovering high order feature mean field module, missing
1988,distributed connectionist production system,n/a
1988,gemini gradient estimation matrix inversion noise injection, missing
1987,learning guide evolution,n/a
1987,connectionist architecture artificial intelligence
1987,learning representation recirculation, missing
1987,learning translation invariant recognition massively parallel network,one major goal research massively parallel network neuron-like processing element discover efficient method recognizing pattern another goal discover general learning procedure allow network construct internal representation required complex task paper describes recently developed procedure learn perform recognition task network trained example input vector represents instance pattern particular position required output vector represents name prolonged training network develops canonical internal representation pattern us canonical representation identify familiar pattern novel position
1986,learning massively parallel net panel,human brain different conventional digital computer relies massive parallelism rather raw speed store long-term knowledge modifying way processing element interact rather setting bit passive general purpose memory robust minor physical damage learns experience instead explicitly programmed yet know brain us activity neuron represent complex articulated structure perceptual system turn raw input useful internal representation rapidly know brain learns new representational scheme past year lot new interesting theory issue much theorizing motivated belief brain using computational principle could also applied massively parallel artificial system knew principle
1985,learning algorithm boltzmann machine,n/a
1985,symbol among neuron detail connectionist inference architecture,n/a
1985,shape recognition illusory conjunction,n/a
1983,massively parallel architecture ai netl thistle boltzmann machine,becoming increasingly apparent aspect intelligent behavior require enormous computational power sort massively parallel computing architecture plausible way deliver power parallelism rather raw speed computing element seems way brain get job done even need massive parallelism admitted still question kind parallel architecture best fit need various ai task paper attempt isolate number basic computational task intelligent system must perform describe several family massively parallel computing architecture see computational task handled family particular describe new architecture call boltzmann machine whose ability appear include number task inefficient impossible architecture
1981,parallel computation assigns canonical object-based frame reference,n/a
1981,shape representation parallel system,n/a
1979,demonstration effect structural description mental imagery,n/a
1978,representation control vision,n/a
1976,using relaxation find puppet,n/a
