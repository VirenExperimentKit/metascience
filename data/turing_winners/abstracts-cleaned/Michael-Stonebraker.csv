2020,role latency task complexity predicting visual search behavior,latency visualization system widely believed affect user behavior measurable way requiring user wait visualization system respond leading interruption analytic flow effect frequently observed widely accepted precisely latency affect different analysis scenario le well understood paper examine role latency context visual search essential task data foraging exploration using visualization conduct series study amazon mechanical turk find certain condition latency statistically significant predictor visual search behavior consistent previous study however  also suggest task type task complexity factor modulate effect latency case rendering latency statistically insignificant predicting user behavior suggests nuanced view role latency previously reported building  finding prior study propose design guideline measuring interpreting effect latency evaluating performance visual search task
2019,kyrix interactive pan/zoom visualization scale,pan zoom basic yet powerful interaction technique exploring large datasets however existing zoomable ui toolkits pad++ zvtm provide backend database support datadriven primitive necessary creating largescale visualization limitation existing generalpurpose toolkits led many purposebuilt solution  address issue scalability cannot easily extended support visualization beyond intended data type usage scenario paper introduce kyrix ease process creating general largescale webbased pan/zoom visualization kyrix integrated system provides developer concise expressive declarative language along backend support performance optimization largescale data evaluate scalability kyrix conducted set benchmarked experiment show kyrix support high interactivity  pan/zoom visualization  million data point demonstrate accessibility kyrix observational study  developer  indicate developer quickly learn kyrixs underlying declarative model create scalable pan/zoom visualization finally provide gallery visualization show kyrix expressive flexible support developer creating wide range customized visualization across different application domain data type
2019,rethinking database high availability rdma network,highly available database system rely data replication tolerate machine failure class existing replication algorithm active-passive active-active designed time network dominant performance bottleneck essence technique aim minimize network communication replica cost incurring processing redundancy trade-off suitably fitted conventional wisdom distributed database design however emergence next-generation network high throughput low latency call revisiting assumption paper first make case modern rdmaenabled network bottleneck shifted cpu therefore existing network-optimized replication technique longer optimal present active-memory replication new high availability scheme efficiently leverage rdma completely eliminate processing redundancy replication using activememory replica dedicate processing power executing new transaction opposed performing redundant computation active-memory maintains high availability correctness presence failure efficient rdma-based undologging scheme evaluation active-passive activeactive scheme show active-memory factor  faster second-best protocol rdma-based network
2019,data civilizer 20 holistic framework data preparation analytics,data scientist spend  time  parameter-tuning machine learning model  iterating data cleaning machine learning model execution existing effort support first requirement currently integrated workflow system couple data cleaning machine learning development previous version data civilizer geared towards data cleaning discovery using set pre-defined tool paper introduce data civilizer  end-to-end workflow system satisfying requirement addition system also support sophisticated data debugger workflow visualization system demo show used data civilizer  help scientist massachusetts general hospital build cleaning machine learning pipeline tb brain activity dataset
2019,choosing cloud dbms architecture tradeoff,analytic  application move cloud dbms shifted employing pure shared-nothing design locally attached storage hybrid design combine use shared-storage  use shared-nothing query execution mechanism paper shed light resulting tradeoff properly identified previous work end evaluates tpc-h benchmark across variety dbms offering running cloud environment  fast gb+ network specifically database-as-a-service offering  query engine  traditional cloud agnostic olap database  comparison cannot apples-to-apples case due cloud configuration restriction nonetheless identify pattern design choice advantageous include prioritizing low-cost object store like data storage using system agnostic yet still performant columnar format like orc allow easy switching system different workload making feature benefit subsequent run like query precompilation caching remote data faster storage optional rather required disadvantage ad hoc query
2019,smile system support machine learning eeg data scale,order reduce possibility neural injury seizure sidestep need neurologist spend hour manually reviewing eeg recording critical automatically detect classify interictal-ictal continuum  pattern eeg data however existing iic classification technique shown accurate robust enough clinical use lack high quality label eeg segment training data obtaining high-quality labeled data traditionally manual process trained clinician tedious time-consuming errorprone work propose smile industrial scale system provides end-to-end solution iic pattern classification problem core component smile include visualizationbased time series labeling module deep-learning based active learning module labeling module enables user explore label  million eeg segment  interactive speed multiple coordinated view allow user examine eeg signal time domain frequency domain simultaneously active learning module first train deep neural network automatically extract local feature respect segment long term dynamic eeg signal classify iic pattern leveraging output deep learning model eeg segment best improve model selected prompted clinician label process iterated clinician model show high degree agreement initial experimental  show smile system allows clinician label eeg segment response time  m accuracy model progressively improved high quality label acquired time
2019,kyrix interactive visual data exploration scale,scalable interactive visual data exploration crucial many domain due increasingly large datasets generated rapid rate details-on-demand provides useful interaction paradigm exploring large datasets user start overview find region interest zoom see detailed view zoom repeat paradigm primary user interaction mode widely-used system google map aperture tile forecache earlier system however highly customized hardcoded visual representation optimization general framework needed facilitate development visual data exploration system scale paper present kyrix end-to-end system developing scalable details-on-demand data exploration application kyrix provides developer declarative model easy specification general visualization behind scene kyrix utilizes suite performance optimization technique achieve response time within m various user interaction also report  performance study show novel dynamic fetching scheme adopted kyrix outperforms tile-based fetching used earlier system
2019,unsupervised string transformation learning entity consolidation,data integration long-standing challenge data management many application key step data integration entity consolidation take collection cluster duplicate record input produce single golden record cluster contains canonical value attribute truth discovery data fusion method well master data management  system used entity consolidation however achieve better  variant value  cluster need consolidated applying method purpose propose data-driven method standardize variant value based two observation  variant value usually transformed representation   transformation often appears repeatedly across different cluster  approach first us unsupervised method generate group value pair transformed way group presented human verification approved one used standardize data real-world dataset  record method achieved  recall  precision standardizing variant value asking human  yes/no question completely outperformed state art data wrangling tool
2019,learned stop worrying love re-optimization,cost-based query optimizers remain one important component database management system analytic workload though modern optimizers select plan close optimal performance common case small number query order magnitude slower could paper investigate still case despite decade improvement cost model plan enumeration cardinality estimation demonstrate believe re-optimization mechanism likely cost-effective way improve end-to-end query performance find even simple re-optimization scheme improve latency many poorly performing query demonstrate re-optimization improves end-to-end latency top  longest running query join order benchmark  realizing benefit perfect cardinality estimation
2019,towards end-to-end human-centric data cleaning framework,data cleaning refers process detecting fixing error data human involvement instrumental several stage process providing rule validating computed repair plethora data cleaning algorithm addressing wide range data error  many algorithm involve human loop however latter usually coupled underlying cleaning algorithm real data cleaning pipeline several data cleaning operation performed using different tool high-level reasoning tool combined repair data potential unlock useful use case involve human cleaning process additionally believe opportunity benefit recent advance active learning method minimize effort human spend verify data item produced tool human currently end-to-end data cleaning framework systematically involves human cleaning pipeline regardless underlying cleaning algorithm paper present opportunity framework could offer highlight key challenge need addressed realize vision present design vision discus scenario motivate need framework judiciously assist human cleaning process
2019,raha configuration-free error detection system,detecting erroneous value key step data cleaning error detection algorithm usually require user provide input configuration form rule statistical parameter however providing complete yet correct set configuration new dataset trivial user know dataset error detection algorithm upfront paper present raha new configuration-free error detection system generating limited number configuration error detection algorithm cover various type data error generate expressive feature vector tuple value leveraging feature vector propose novel sampling classification scheme effectively chooses representative value training furthermore system exploit historical data filter irrelevant error detection algorithm configuration experiment raha outperforms state-of-the-art error detection technique  labeled tuples dataset
2019,anmat automatic knowledge discovery error detection pattern functional dependency,knowledge discovery critical successful data analytics propose new type meta-knowledge namely pattern functional dependency  combine pattern  integrity constraint  model dependency  partial value  across different attribute table pfds go beyond classical functional dependency extension instance employee table id f-- f determines finance department moreover key application pfds use identify erroneous data tuples violate pfds demonstration attendee experience following feature  pfd discovery -- automatically discover pfds  data different domain  error detection pfds -- show error detected pfds cannot captured existing approach
2019,schengendb data protection database proposal,gdpr europe similar regulation california ccpa require new level privacy support consumer challenging department “right forgotten” hence enterprise must ensure information specific consumer deleted enterprise storage requested since enterprise internally heavily “siloed” sharing information usually accomplished copying data system make finding deleting copy data particular consumer difficultgdpr also requires notion purpose access control model orthogonal one customarily sql herein sketch implementation purpose show fit within conventional access control frameworkwe propose two solution supporting gdpr dbms “green field” environment present propose solution directly support process ensuring gdpr compliance enterprise-scale specifically designed store every fact consumer exactly therefore right forgotten readily supported deleting fact hand dealing legacy system enterprise propose second solution track copy personal information deleted request course solution entail additional overhead dbmsonce data leaf dbms application propose “sandboxing” application novel way prevent leaking data outside world inappropriate lastly discus challenge associated auditing logging data paper sketch design gdpr compliant facility collectively term schengendb
2019,wip - skod framework situational knowledge demand,extracting relevant pattern heterogeneous data stream pose significant computational analytical challenge identifying pattern pushing analogous content interested party according mission need real-time difficult problem paper present design skod novel situational knowledge query engine continuously build multi-modal relational knowledge base using sql query skod push dynamic content relevant user trigger based modeling users’ interest skod scalable real-time on-demand situational knowledge extraction dissemination framework process stream multi-modal data utilizing publish/subscribe stream engine initial prototype skod us deep neural network natural language processing technique extract model relevant object video stream topic entity event unstructured text resource twitter news article extensible architecture skod aim provide high-performance generic framework situational knowledge demand supporting effective information retrieval evolving mission
2018,data integration current status way forward,n/a
2018,beagle automated extraction interpretation visualization web,common interactive visualization web popular visualization design prevalent pie chart really question intimate role interactive visualization real  world paper present approach  answering question first introduce beagle mine web svg-based visualization automatically classifies type  beagle extract  visualization across five different tool repository classify  accuracy across  visualization type given visualization collection study usage across tool find visualization fall four type bar chart line chart scatter chart geographic map though controversial pie chart relatively rare visualization tool studied finding also suggest total visualization type supported given tool could factor ease use however effect appears mitigated providing variety diverse expert visualization example user
2018,tabularosa tabular operating system architecture massively parallel heterogeneous compute engine,rise computing hardware choice driving reevaluation operating system traditional role operating system controlling execution hardware evolving toward model whereby controlling processor distinct compute engine performing computation context operating system viewed software broker track resource compute engine akin database management system explore idea using database operating system role work defines key operating system function term rigorous mathematical semantics  directly translatable database operation operation posse number mathematical property ideal parallel operating system guaranteeing correctness wide range parallel operation resulting operating system equation provide mathematical specification tabular operating system architecture  implemented platform simulation forking tabularrosa performed using associative array implementation compared linux + core supercomputer using  forkers managing  process simulation show tabularosa potential perform operating system function massively parallel scale tabularosa simulation show x higher performance compared linux managing x process fully searchable table
2018,top ten fear dbms field,paper present top ten fear future dbms field apology david letterman three ”big fears” discus first five additional fear result ”big three” conclude ”the big enchilada” pair fear case indicate think best way deal current situation
2018,seeping semantics linking datasets using word embeddings data discovery,employee spend time finding relevant data analyzing suffer data discovery problem large volume data enterprise sometimes lack knowledge schema aggravates problem similar navigate web propose identify semantic link assist analyst discovery task link relate table facilitate navigating schema also relate data external data source ontology dictionary help explain schema meaning materialize link enterprise knowledge graph become available analyst main challenge find pair object semantically related propose semprop dag different component find link based syntactic semantic similarity semprop commanded semantic matcher leverage word embeddings find object semantically related introduce coherent group technique combine word embeddings work better state art combination alternative implement semprop part aurum data discovery system building conduct user study real deployment quantitative evaluation understand benefit link data discovery task well benefit semprop coherent group find link
2018,aurum data discovery system,organization face data discovery problem analyst spend time looking relevant data analyzing problem become commonplace modern organization data stored across multiple storage system database data lake cloud ii data scientist operate within limit well-defined schema small number data sources-instead answer complex question must access data spread across thousand data source address problem capture relationship datasets enterprise knowledge graph  help user navigate among disparate source contribution paper aurum system build maintain query ekg build ekg introduce two-step process scale large datasets requires one-pass data avoiding overloading source system maintain ekg without re-reading data every time introduce resource-efficient sampling signature  method work using small sample data finally query ekg introduce collection composable primitive thus allowing user define many different type discovery query describe experience using aurum three corporate scenario performance evaluation component
2018,building data civilizer pipeline advanced workflow engine,order enterprise gain insight internal business changing outside environment essential provide relevant data in-depth analysis enterprise data usually scattered across department geographic region often inconsistent data scientist spend majority time finding preparing integrating cleaning relevant data set data civilizer end-to-end data preparation system paper present complete system focusing new workflow engine superior system entity matching consolidation new cleaning tool workflow engine allows data scientist author execute retrofit data preparation pipeline different data discovery cleaning service end-to-end demo scenario based data mit data warehouse e-commerce data set
2018,p-store elastic database system predictive provisioning,oltp database system critical part operation many enterprise system often configured statically sufficient capacity peak load many oltp application however maximum load order magnitude larger minimum load varies repeating daily pattern thus prudent allocate computing resource dynamically match demand one allocate resource reactively load increase detected place additional burden already-overloaded system reconfigure predictive allocation advance load increase clearly preferablewe present p-store first elastic oltp dbms use prediction apply workload bw digital  large online retailer study show p-store outperforms reactive system bw workload causing  fewer latency violation achieves performance comparable static allocation peak demand using  fewer server
2018,fastdawg improving data migration bigdawg polystore system,problem data integration around decade yet satisfactory solution yet emerged new type system called polystore surfaced partially address integration problem based experience polystore called bigdawg identify three major roadblock acceptable commercial solution offer new architecture inspired three problem trade generality usability architecture also exploit modern hardware  gain performance paper concludes promising experimental 
2017,database decay,n/a
2017,evaluation distributed concurrency control,increasing transaction volume led resurgence interest distributed transaction processing particular partitioning data across several server improve throughput allowing server process transaction parallel executing transaction across server limit scalability performance systemsin paper quantify effect distribution concurrency control protocol distributed environment evaluate six classic modern protocol in-memory distributed database evaluation framework called deneva providing apples-to-apples comparison  expose severe limitation distributed transaction processing engine moreover analysis identify several protocol-specific scalability bottleneck conclude achieve truly scalable operation distributed concurrency control solution must seek tighter coupling either novel network hardware  application 
2017,silkmoth efficient method finding related set maximum matching constraint,determining two set related - similar value one set contains -- important problem many application data cleaning data integration information retrieval example set relatedness useful tool discover whether column two different database joinable enough value column match may make sense join common metric measure relatedness two set treating element vertex bipartite graph calculating score maximum matching pairing element compared metric require exact matchings element metric us similarity function compare element two set making robust small dissimilarity element useful real-world dirty data unfortunately metric suffers expensive computational cost taking time n number element set set-to-set comparison thus application try search pairing related set brute-force manner runtime becomes unacceptably largeto address challenge developed silkmoth system capable rapidly discovering related set pair collection set internally silkmoth creates signature set property set related must match signature silkmoth us signature prune search space set match signature left candidate finally silkmoth applies maximum matching metric remaining candidate verify candidate truly related set important property silkmoth guaranteed output exactly related set pairing brute-force method unlike approximate technique thus contribution paper characterization space signature enable property show selecting optimal signature space np-complete based insight characterization space propose two novel filter help prune candidate verification addition introduce simple optimization calculation maximum matching metric based triangle inequality compared related approach silkmoth much general handling larger space similarity function relatedness metric order magnitude efficient real datasets
2017,exploring big volume sensor data vroom,state art sensor within single autonomous vehicle  produce video lidar data rate greater  gb/hour unsurprisingly even small av research team accumulate ten terabyte sensor data multiple trip multiple vehicle av practitioner would like extract information specific location specific situation study often unable query av sensor data different generic analytics spatial query demand reasoning field view well heavy computation extract feature scene article demo present vroom system ad-hoc query av sensor database vroom combine domain specific property av datasets selective indexing multi-query optimization address challenge posed av sensor data
2017,approximate string join abbreviation,string join wide application data integration cleaning inconsistency data caused data error term variation missing value led need approximate string join  paper study asj abbreviation frequent type term variation although prior work studied asj given user-inputted dictionary synonym rule three common limitation first suffer low precision presence abbreviation multiple full form second join algorithm scalable due exponential time complexity third dictionary may exist since abbreviation highly domain-dependentwe propose end-to-end workflow address limitation three main component workflow  new similarity measure taking abbreviation account handle abbreviation multiple full form  efficient join algorithm following filter-verification framework  unsupervised approach learn dictionary abbreviation rule input string evaluate workflow four real-world datasets show workflow output accurate join  scale well input size grows greatly outperforms state-of-the-art approach accuracy efficiency
2017,data civilizer system,many organization often challenging user find relevant data specific task since data usually scattered across enterprise often inconsistent fact data scientist routinely report majority effort spent finding cleaning integrating accessing data interest task hand order decrease grunt work needed facilitate analysis data wild present data civilizer end-to-end big data management system data civilizer linkage graph computation module build linkage graph data data discovery module utilizes linkage graph help identify data relevant user task also us linkage graph discover possible join path used query actual query execution use polystore dbms federates query processing across disparate system addition data civilizer integrates data cleaning operation query processing different user need invoke task different order data civilizer embeds workflow engine enables arbitrary composition different module well handling data update deployed preliminary data civilizer system two institution mit merck describe initial positive experience show system shortens time effort required find prepare analyze data
2017,position statement case visualization performance benchmark,visualization invaluable tool data analysis process enable scientist explore interpret billion datapoints quickly rendered image however many visualization system unable keep unprecedented accumulation data remote sensor field sensor medical personal device social network due certain assumption many tool rely assumption system store entire datasets directly main memory many datasets massive datasets available ranging nasa modis satellite imagery dataset internet movie database  twitter stream  assumption longer match reality
2017,bigdawg version 01,polystore system database management system composed integrated heterogeneous database engine multiple programming language matching data storage engine best suited need complex analytics run faster flexible storage choice help improve data organization bigdawg  prototype implementation polystore system paper describe current bigdawg software release support postgresql accumulo scidb describe overall architecture api initial  applying bigdawg mimic ii medical dataset
2017,database engine integration performance analysis bigdawg polystore system,bigdawg polystore database system aim address workload dealing large heterogeneous datasets need system motivated increase big data application dealing disparate type data large scale analytics realtime data stream text-based record suited different storage engine application often perform cross-engine query correlated data resulting complex query planning data migration execution one application medical application built intel science technology center  data collected intensive care unit  present work done add support two commonly used database engine vertica mysql bigdawg system well  analysis performance evaluation system using tpc-h benchmark
2017,demo data civilizer system,finding relevant data specific task numerous data source available organization daunting task number possible data source data interest resides also due data scattered enterprise typically dirty inconsistent practice data scientist routinely reporting majority  effort spent finding cleaning integrating accessing data interest task hand propose demonstrate data civilizer ease pain faced analyzing data wild data civilizer end-to-end big data management system component data discovery data integration stitching data cleaning querying data large variety storage engine running large enterprise
2016,land shark squawk box,turn riding across america handy metaphor building system software
2016,beckman report database research,database researcher paint big data defining challenge make enormous opportunity hand require focusing five research area
2016,detecting data error need done,data cleaning played critical role ensuring data quality enterprise application naturally extensive research area many data cleaning algorithm translated tool detect possibly repair certain class error outlier duplicate missing value violation integrity constraint since different type error may coexist data set often need run one kind tool paper investigate two pragmatic question  tool robust enough capture error real-world data set  best strategy holistically run multiple tool optimize detection effort answer two question obtained multiple data cleaning tool utilize variety error detection technique also collected five real-world data set could obtain raw data ground truth existing error paper report experimental finding error detected tool tested first show coverage tool well  second show order multiple tool run make big difference hence propose holistic multi-tool strategy order invocation available tool maximize benefit minimizing human effort verifying  third since holistic approach still lead acceptable error coverage discus two simple strategy potential improve situation namely domain specific tool data enrichment close paper reasoning error detectable tool tested
2016,clay fine-grained adaptive partitioning general database schema,transaction processing database management system  critical today data-intensive application enable organization quickly ingest query new information many application exceed capability single server thus database deployed distributed dbms key factor affecting system performance database partitioned database partitioned incorrectly number distributed transaction high transaction synchronize operation network considerably slower lead poor performance previous work elastic database repartitioning focused certain class application whose database schema represented hierarchical tree structure many application cannot partitioned manner thus subject distributed transaction impede performance scalabilityin paper present new on-line partitioning approach called clay support tree-based schema complex general schema arbitrary foreign key relationship clay dynamically creates block tuples migrate among server repartitioning placing constraint schema taking care balance load reduce amount data migrated clay achieves goal including block set hot tuples tuples co-accessed hot tuples evaluate approach integrate clay distributed main-memory dbms show generate partitioning scheme enable system achieve  better throughput  lower latency existing approach
2016,database decay avoid,traditional wisdom designing database schema use design tool  construct initial data model one data one satisfied result tool automatically construct collection rd normal form relation model application coded relational schema business circumstance change  one run tool produce new data model new resulting collection table new schema populated old schema application altered work new schema using relational view whenever possible ease migration way database remains rd normal form represents “good” schema defined dbms researcher “in wild” schema often change quarter often traditional wisdom repeat exercise alteration paper report traditional wisdom appears rarely-to-never followed large multi-department application instead dba appear attempt minimize application maintenance  instead maximizing schema quality lead schema quickly diverge e-r uml model actual database semantics tend drift farther farther rd normal form term divergence reality rd normal form principle database decay obviously undesirable state affair avoided possible paper continues tactic slow database decay argue traditional development methodology coding application odbc jdbc least partly blame decay hence propose alternate methodology resilient decay
2016,step scalable tenant placement managing database-as-a-service deployment,public cloud provider database-as-a-service offering must efficiently allocate computing resource customer effective assignment tenant reduces number physical server use meet customer expectation price point competitive cloud market public cloud vendor like microsoft amazon mean packing million user database onto hundred thousand serversthis paper study tenant placement examining publicly released dataset anonymized customer resource usage statistic microsofts azure sql database production system three-month period implemented step framework ingest analyze large dataset step allowed u use production dataset evaluate several new algorithm packing database tenant onto server technique produce highly efficient packing collocating tenant compatible resource usage pattern evaluation show production-sourced customer workload technique robust variation number node keeping performance objective violation minimum even high-density tenant packing comparison algorithm used production time data collection algorithm produce  fewer performance objective violation save  total operational cost cloud provider
2016,bigdawg monitoring framework,bigdawg polystore database system designed work heterogenous data may stored disparate database storage engine central component bigdawg polystore system ability submit query may executed different data engine paper present monitoring framework bigdawg federated database system maintains performance information benchmark query environmental condition change monitoring framework update existing performance information match current condition using information monitoring system determine optimal query execution plan similar incoming query also describe series test query run ass whether system correctly determines optimal plan query
2016,data transformation migration polystores,ever increasing data size new requirement data processing fostered development many new database system result many data-intensive application underpinned different engine enable data mobility need transfer data system easily efficiently analyze state-of-the-art data migration outline research opportunity rapid data transfer experiment explore data migration diverse set database including postgresql scidb s-store accumulo system excels specific application requirement transactional processing numerical computation streaming data large scale text processing providing efficient data migration tool essential take advantage superior processing specialized database goal build data migration framework take advantage recent advancement hardware software
2016,bigdawg polystore system architecture,organization often faced challenge providing data management solution large heterogenous datasets may different underlying data programming model example medical dataset may unstructured text relational data time series waveform imagery trying fit datasets single data management system adverse performance efficiency effect part intel science technology center big data developing polystore system designed problem bigdawg  polystore system designed work complex problem naturally span across different processing storage engine bigdawg provides architecture support diverse database system working different data model support competing notion location transparency semantic completeness via island middleware provides uniform multi-island interface initial  prototype bigdawg system applied medical dataset validate polystore concept article describe polystore database current bigdawg architecture application mimic ii medical dataset initial performance  future development plan
2016,cross-engine query execution federated database system,developed reference implementation bigdawg system new architecture future big data application guided philosophy “one size fit all” application call large-scale analytics also real-time streaming support smaller analytics interactive speed data visualization cross-storage-system query importance effectiveness system demonstrated hospital application using data intensive care unit  article describe implementation evaluation cross-system query executor particular focus cross-engine shuffle join within bigdawg system evaluate various strategy computing faced varying degree data skew
2016,dataxformer robust transformation discovery system,data integration data curation data analysis task user spend considerable amount time converting data one representation another example u date european date airport code city name previous vision paper presented initial design dataxformer system us web resource assist transformation discovery specifically dataxformer discovers possible transformation web table web form involves human feedback appropriate paper present full fledged system along several extension particular present algorithm find  transformation entail multiple column input data  indirect transformation composition transformation  transformation function rather relationship  transformation knowledge base public data report experiment collection  transformation task show enhanced system automatically cover  using openly available resource
2016,towards large-scale data discovery position paper,thousand data source spread across multiple database data lake modern organization face data discovery challenge analyst spend time finding relevant data answer question hand analyzing itin paper introduce data discovery system facilitates locating relevant data among thousand data source represent data source succinctly signature create search path permit quick execution set data discovery primitive used finding relevant data built prototype used solve data discovery challenge two big organization
2016,dynamic prefetching data tile interactive visualization,paper present forecache general-purpose tool exploratory browsing large datasets forecache utilizes client-server architecture user interacts lightweight client-side interface browse datasets data browsed retrieved dbms running back-end server assume detail-on-demand browsing paradigm optimize back-end support paradigm inserting separate middleware layer front dbms improve response time middleware layer fetch data ahead user explores datasetwe consider two different mechanism prefetching  learning fetch user recent movement  using data characteristic  find data similar user viewed past incorporate mechanism single prediction engine adjusts prediction strategy time based change user behavior evaluated prediction engine user study found dynamic prefetching strategy provides  significant improvement overall latency compared non-prefetching system   substantial improvement prediction accuracy  latency  relative existing prefetching technique
2015,handling shared mutable state stream processing correctness guarantee,s-store next-generation stream processing system developed brown intel mit portland state university designed achieve high throughput maintaining number correctness guarantee required handle shared mutable state streaming application paper explores correctness criterion describes s-store achieves including new model stream processing provides support acid transaction 
2015,demonstration bigdawg polystore system,paper present bigdawg reference implementation new architecture big data application application call large-scale analytics also real-time streaming support smaller analytics interactive speed data visualization cross-storage-system query guided principle one size fit build top variety storage engine designed specialized use case illustrate promise approach demonstrate effectiveness hospital application using data intensive care unit  complex application serf need doctor researcher provides real-time support stream patient data showcase novel approach querying across multiple storage engine data visualization scalable real-time analytics
2015,s-store streaming meet transaction processing,stream processing address need real-time application transaction processing address coordination safety short atomic computation heretofore two mode operation existed separate stove-piped system work attempt fuse two computational paradigm single system called s-store way s-store simultaneously accommodate oltp streaming application present simple transaction model stream integrates seamlessly traditional oltp system provides acid stream-oriented guarantee chose build s-store extension h-store - open-source in-memory distributed oltp database system implementing s-store way make use transaction processing facility h-store already provides concentrate additional feature needed support streaming similar implementation could done using main-memory oltp platform show actually achieve higher throughput streaming workload s-store equivalent deployment h-store alone also show achieved within h-store addition modest amount new functionality furthermore compare s-store two state-of-the-art streaming system esper apache storm show s-store sometimes exceed performance time providing stronger correctness guarantee
2015,temporal rule discovery web data cleaning,declarative rule functional dependency widely used cleaning data several system take input detecting error computing clean version data support domain expert specifying rule several tool proposed profile data mine rule however existing discovery technique traditionally ignored time dimension recurrent event person reported location duration valid duration part rule cleaning process would simply failin work study rule discovery problem temporal web data discovery process challenging nature web data extracted fact  sparse time  reported delay  often reported error value inaccurate source non robust extractor handle challenge new discovery approach robust noise solution us machine learning method association measure outlier detection discovery rule together aggressive repair data mining step experimental evaluation real-world data recorded future intelligence company monitor k web source show temporal rule improve quality data increase average precision cleaning process    relative increase average f-measure
2015,bigdawg polystore system,paper present new view federated database address growing need managing information span multiple data model trend fueled proliferation storage engine query language based observation  one size fit  address shift propose polystore architecture designed unify querying multiple data model consider challenge opportunity associated polystores open question space revolve around query optimization assignment object storage engine introduce approach topic discus prototype context intel science technology center big data
2015,dataxformer leveraging web semantic transformation,data transformation crucial step data integration transformation liter gallon easily performed applying formula program input value others zip code city require sifting repository containing explicit value mapping already powerful system provide formula algorithm transformation however automated identication reference datasets support value mapping remains largely unresolved web home million table many containing explicit value mapping addition value mapping hidden behind web form paper present dataxformer transformation engine leverage web table web form perform transformation task particular describe inductive lter-rene approach identifying explicit transformation corpus web table approach dynamically retrieve wrap web form experiment show combination resource type cover  transformation query formulated real-world user
2015,fruit shall ye know data analyst perspective massively parallel system design,increasingly parallel system promise remedy current stagnation single-core performance however battle find appropriate architecture resulting massively parallel system still ongoing currently two active contender massively parallel single instruction multiple thread  system gpgpus many core single instruction multiple data  system intels xeon phi former versatile latter efficient time-tested technology clear migration path study provide data management perspective debate study implementation performance set common data management operation simt device  compare many core simd system  interpret  pinpoint architectural decision tradeoff lead suboptimal performance point potential area improvement next generation device
2015,skew-aware join optimization array database,science application accumulating ever-increasing amount multidimensional data although processed relational database much better suited array-based engine important optimize query processing system paper focus efficient query processing join operation within array database engine invariably ``chunk data multidimensional tile use efficiently process spatial query traditional relational algorithm need substantially modified take advantage array tile moreover n-dimensional science data unevenly distributed array space underlying observation rarely follow uniform pattern crucial optimization array join skew-aware addition owing scale science application query processing usually span multiple node complicates planning array joinsin paper introduce join optimization framework skew-aware distributed join optimization consists two phase first logical planner selects query algorithm  granularity tile reorganization operation needed align data second phase implement logical plan assigning tile cluster node using analytical cost model experimental  synthetic real-world data demonstrate optimization framework speed array join x comparison baseline
2015,dataxformer interactive data transformation tool,syntactic transformation require application formula input value unit conversion date format conversion semantic transformation zip code city require look-up reference data recently presented dataxformer system leverage web table web form expert sourcing cover wide range transformation demonstration present user-interaction dataxformer show scenario used transform data explore effectiveness efficiency several approach transformation discovery leveraging  million table online source
2014,s-store streaming newsql system big velocity application,first-generation streaming system pay much attention state management via acid transaction  s-store data management system combine oltp transaction stream processing create s-store begin h-store main-memory transaction processing engine add primitive support streaming includes trigger transaction workflow implement push-based processing window provide way bound computation table hidden state implement scoping proper isolation demo explores benefit approach showing naive implementation benchmark using h-store yield incorrect  also show exploiting push-based semantics implementation trigger achieve significant improvement transaction throughput demo two modern application  leaderboard maintenance version american idol  city-scale bicycle rental scenario
2014,vertexica relational friend graph analytics!,paper present vertexica graph analytics tool top relational database user friendly yet highly efficient instead constraining programmer sql vertexica offer popular vertex-centric query interface natural analyst express many graph query programmer simply provide vertex-compute function vertexica take care efficiently executing standard sql engine advantage using vertexica ability leverage relational feature enable much sophisticated graph analysis include expressing graph algorithm difficult vertex-centric straightforward sql ability compose end-to-end data processing pipeline including pre- post- processing graph well combining multiple algorithm deeper insight vertexica graphical user interface outline several demonstration scenario including interactive graph analysis complex graph analysis continuous time series analysis
2014,staring abyss evaluation concurrency control one thousand core,computer architecture moving towards eradominated many-core machine dozen even hundred ofcores single chip unprecedented level on-chipparallelism introduces new dimension scalability currentdatabase management system  designed inparticular number core increase problem ofconcurrency control becomes extremely challenging hundred ofthreads running parallel complexity coordinatingcompeting access data likely diminish gain fromincreased core count better understand unpreparedcurrent dbms future cpu architecture performed anevaluation concurrency control on-line transactionprocessing  workload many-core chip implementedseven concurrency control algorithm main-memory dbms andusing computer simulation scaled system  core ouranalysis show algorithm fail scale magnitudebut different reason case identify fundamentalbottlenecks independent particular databaseimplementation argue even state-of-the-art dbms sufferfrom limitation conclude rather pursuingincremental solution many-core chip may require completelyredesigned dbms architecture built ground istightly coupled hardware 
2014,e-store fine-grained elastic partitioning distributed transaction processing,on-line transaction processing  database management system  often serve time-varying workload due daily weekly seasonal fluctuation demand rapid growth demand due company business success addition many oltp workload heavily skewed hot tuples range tuples example majority nyse volume involves  stock deal fluctuation oltp dbms need elastic must able expand contract resource response load fluctuation dynamically balance load hot tuples vary timethis paper present e-store elastic partitioning framework distributed oltp dbms automatically scale resource response demand spike periodic event gradual change application workload e-store address localized bottleneck two-tier data placement strategy cold data distributed large chunk smaller range hot tuples assigned explicitly individual node contrast traditional single-tier hash range partitioning strategy experimental evaluation e-store show viability approach efficacy variation load across cluster machine compared single-tier approach e-store improves throughput  reducing latency 
2014,large-scale semantic profile extraction,web-search engine usually outperformed specialized system optimized specific domain type data halevy et al  demonstrate use case specialized spatial search google fusion table whereby user search bike trail san francisco bay area see result google map query submitted general-purpose google web-search engine return many irrelevant search  relevance returned search  key property search-engine hence important appreciated problem database information retrieval websearch  user search-engine strongly prefer get relevant search  first otherwise spend time curating search- content provider web setting usually exhibit specific focus posting example information http//wwwnasdaqcom usually financial domain britney spear mostly tweeting music business wire often publishes acquisition rare source cover wide variety topic paper introduce demonstrate data structure designed capture semantic sketch data source along algorithm similarity measure used extract populate match similar profile example newspaper business wire often publishes acquisition therefore named entity type highly ranked profile  run experiment corpus  million web page webm provided web aggregator recorded future  extract   million profile leverage outperform general-purpose web-search certain type query paper organized follows section  defines semantic profile describes algorithm extract semantic profile section  describes large-scale storage engine used run experiment section  introduces similarity measure useful match find information source alike section  demonstrates profile used improve general-purpose web-search engine well expert mining finish section  discussing related future work
2014,enterprise database application cloud difficult road ahead,considerable interest moving dbms application inside enterprise data center cloud reduce cost increase flexibility elasticity application green field project  others existing legacy system must migrated cloud another dimension decision support application others update-oriented paper discus technical political challenge various enterprise application face considering cloud deployment addition requirement quality-of-service  guarantee generate additional disruptive issue circumstance achieving good dbms performance current cloud architecture future hardware technology non-trivial summary difficult road ahead enterprise database application
2014,rethinking main memory oltp recovery,fine-grained record-oriented write-ahead logging exemplified system like aries gold standard relational database recovery paper show modern high-throughput transaction processing system longer optimal way recover database system particular transaction throughput get higher aries-style logging start represent non-trivial fraction overall transaction execution time propose lighter weight coarse-grained command logging technique record transaction executed database recovery starting transactionally consistent checkpoint replaying command log new transaction avoiding overhead fine-grained logging image  command logging yield significantly higher throughput run-time recovery time command logging higher compared aries-style physiological logging approach advent high-availability technique mask outage recovering node recovery speed become secondary importance run-time performance application evaluated approach implementation tpcc main memory database system  found command logging offer  x higher throughput main-memory optimized implementation aries-style physiological logging
2014,text structured data fusion data tamer scale,large-scale text data research recently started regain momentum - wealth date information communicated unstructured format example new information online medium  becomes instantly available refreshed regularly broad coverage valuable property unusual data source format therefore many enterprise individual interested integrating using unstructured text addition structured data
2014,genbase complex analytics genomics benchmark,paper introduces new benchmark designed test database management system  performance mix data management task  complex analytics  mixed workload prevalent number application area including science workload web analytics specific use case chosen genomics data benchmark constructed collection typical task domain addition representative mixed data management analytics workload benchmark also meant scale large dataset size multiple node across cluster besides presenting benchmark run variety storage system including traditional row store newer column store hadoop array dbms present performance number system single multiple node show performance differs order magnitude various solution addition demonstrate platform scalability issue also test offloading analytics onto coprocessor intent benchmark focus research interest area end data data generator script available web site
2014,incremental elasticity array database,relational database benefit significantly elasticity whereby execute set changing hardware resource provisioned match storage processing requirement flexibility especially attractive scientific database user often no-overwrite storage model delete data available space exhausted  database regularly growing expanding hardware proportionally also scientific database frequently store data multidimensional array optimized spatial querying brings several novel challenge clustered skew-aware data placement elastic shared-nothing database work design implement elasticity array database address challenge two front determining expand database cluster partition data within step propose incremental approach affecting minimum set data node maintaining high performance introduce algorithm gradually augmenting array database hardware using closed-loop control system cluster add node optimize data placement n-dimensional array many elastic partitioners incrementally reorganize array redistributing data new node combining two tool scientific database efficiently seamlessly manages monotonically increasing hardware resource
2014,prolegomenon oltp database system non-volatile memory,design database management system  architecture predicated target storage hierarchy traditional diskoriented system use two-level hierarchy fast volatile memory used caching slower durable device used primary storage system use buffer pool complex concurrency control scheme mask disk latency compare main memory dbms assume data reside dram thus need component emerging non-volatile memory  technology require u rethink dichotomy memory device slightly slower dram writes persistent even power loss explore two possible use case nvm on-line transaction processing  dbms first nvm completely replaces dram nvm dram coexist system case compare performance disk-oriented dbms memory-oriented dbms using two oltp benchmark also evaluate performance different recovery algorithm nvm device evaluation show storage hierarchy memory-oriented system able outperform disk-oriented counterpart however skew decrease performance two architecture converge showing neither architecture ideally suited nvm-based storage hierarchy
2013,scidb database management system application complex analytics,description discussion scidb database management system focus lesson learned application area performance comparison solution additional approach managing data complex analytics
2013,voltdb main memory dbms,paper describes voltdb dbms exists mid  focus design decision especially concurrency control high availability well application area system found acceptance run-time performance number also included 
2013,scidb dbms research mit,paper present snapshot scientific dbms research mit part intel science technology center big data focus effort primarily scidb although work used backend dbms summarize work making scidb elastic providing skew-aware join strategy producing scalable visualization scientific data 
2013,anti-caching new approach database management system architecture,traditional wisdom building disk-based relational database management system  organize data heavily-encoded block stored disk main memory block cache order improve performance given high disk latency system use multi-threaded architecture dynamic record-level locking allows multiple transaction access database time previous research shown  substantial overhead on-line transaction processing  application next generation dbms seek overcome limitation architecture based main memory resident data overcome restriction data fit main memory propose new technique called anti-caching cold data moved disk transactionally-safe manner database grows size data initially resides memory anti-caching architecture revers traditional storage hierarchy disk-based system main memory primary storage devicewe implemented prototype anti-caching proposal high-performance main memory oltp dbms performed series experiment across range database size workload skews read/write mix compared performance open-source disk-based dbms optionally fronted distributed main memory cache  show higher skewed workload anti-caching architecture performance advantage either architecture tested  data size  larger memory
2013,intel big data science technology center vision execution plan,intel moved collaboration model university consisting science technology center  located hub university participation university contain embedded intel personnel focused research theme intel held national competition th science technology center  selected proposal mit theme big data paper present big data vision technology center execution plan first year
2013,dynamic reduction query result set interactive visualizaton,modern database management system  designed efficiently store manage perform computation massive amount data contrast many existing visualization system scale seamlessly small data set enormous one designed three-tiered visualization system called scalar deal issue scalar dynamically performs resolution reduction expected result dbms query large effectively rendered existing screen real estate instead running original query scalar insert aggregation sampling filtering operation reduce size result paper present design implementation scalar show  example application displaying satellite imagery data stored scidb back-end dbms
2013,data curation scale data tamer system,data curation act discovering data source interest cleaning transforming new data semantically integrating local data source deduplicating resulting composite much research various component curation  however little work collecting curation component integrated end-to-end system addition previous work scale size problem nding eld example one web aggregator requires curation  url second biotech company problem curating  spreadsheet scale data curation cannot manual  eort must entail machine learning approach human assist necessary paper describes data tamer end-to-end curation system built mit brandeis qatar computing research institute  expects input sequence data source add composite constructed time new source subjected machine learning algorithm perform attribute identication grouping attribute table transformation incoming data deduplication necessary human asked guidance also data tamer includes data visualization component human examine data source specify manual transformation run data tamer three real world enterprise curation problem shown lower curation cost  relative currently deployed production software
2013,standard graph algorithm primitive,view state art constructing large collection graph algorithm term linear algebraic operation mature enough support emergence standard set primitive building block paper position paper defining problem announcing intention launch open effort define standard
2013,subzero fine-grained lineage system scientific database,data lineage key component provenance help scientist track query relationship input output data current system readily support lineage relationship file data array level finer-grained support array-cell level impractical due lack support user defined operator high runtime storage overhead store lineage interviewed scientist several domain identify set common semantics leveraged efficiently store fine-grained lineage use insight define lineage representation efficiently capture common locality property lineage data set apis operator developer easily export lineage information user defined operator finally introduce two benchmark derived astronomy genomics show technique reduce lineage query cost × incuring substantially le impact workflow runtime storage
2013,drowning sea least publishable unit lpus,field drowning sea conference submission assert sheer number paper begun seriously hurt quality work field field going implode unless take action remedy situation order improve quality paper published must reduce number submitted require change culture field equated better hiring promotion committee panel explore idea correcting situation
2012,demonstration dbwipes clean query,data analytics becomes mainstream complexity underlying data computation grows increasingly important provide tool help analyst understand underlying reason encounter error result data provenance large step providing tool help debug complex workflow current form limited utility debugging aggregation operator compute single output large collection input traditional provenance return entire input collection low precision contrast user seeking precise description input caused error propose ranked provenance system identifies subset input influenced output error describes subset human readable predicate order contribution error demonstration present dbwipes novel data cleaning system allows user execute aggregate query interactively detect understand clean error query  conference attendee explore anomaly campaign donation current u presidential election reading -node sensor deployment 
2012,earthdb scalable analysis modis data using scidb,arth scientist increasingly experiencing difficulty analyzing rapidly growing volume complex data must perform analysis directly low-level national aeronautics space administration  moderate resolution imaging spectroradiometer  level b calibrated geolocated data example encounter arcane high-volume data set burdensome make use instead earth scientist typically opt use higher-level canned product provided nasa however higher-level product fail meet requirement particular project cruel dilemma arises cope data product dont exactly meet project need spend enormous amount resource extracting needed unadulterated low-level data paper present earthdb system eliminates dilemma offering following contribution enabling painless importing modis level b data scidb highly scalable science-oriented database platform abstract away complexity distributed storage analysis complex multi-dimensional data defining schema unifies storage representation modis level b data regardless source file supporting fast filtering analysis modis data use intuitive high-level query language rather complex procedural programming providing ability easily define reconfigure entire analysis pipeline within scidb database allowing rapid ad-hoc analysis demonstrate ability provide sample benchmark construction true-color  normalized difference vegetative index  image raw modis level b data using relatively simple query scalable performance
2012,future scientific data base,many decade user scientific field  resorted either home-grown tool legacy software management data technological advancement nowadays necessitate many property data independence scalability functionality found roadmap dbms technology dbms product however yet ready address scientific application user need recent effort toward building science dbms indicate long way ahead u paved research agenda rich interesting challenging problem
2012,efficient versioning scientific array database,paper describe versioned database storage manager developing scidb scientific database system designed efficiently store retrieve array-oriented data exposing no-overwrite storage model update creates new version array make possible perform comparison version produced different time different algorithm create complex chain tree version present algorithm efficiently encode version minimizing storage space still providing efficient access data additionally present optimal algorithm given long sequence version determines version encode term  minimize total storage space query execution cost compare performance algorithm real world data set national oceanic atmospheric administration  open street map several source show algorithm provide better performance existing version control system optimized array data term storage size access time delta-compression algorithm able substantially reduce total storage space version exist high degree similarity
2011,10 rule scalable performance simple operation datastores,partition data operation keep administration simple assume one size fit 
2011,architecture scidb,scidb open-source analytical database oriented toward data management need scientist mix statistical linear algebra operation data management one using natural nested multidimensional array data model working code two year recently help venture capital backing release   downloadable website paper present main design decision scidb focus decision concerning high-level sql-like query language issue facing query optimizer executor efficient storage management array paper also discus implementation feature usually present dbms including version control uncertainty provenance
2010,mapreduce parallel dbms friend foe,mapreduce complement dbms since database designed extract-transform-load task mapreduce specialty 
2009,claremont report database research,database research expanding major effort system architecture new language cloud service mobile virtual world interplay structure text
2009,demonstration scidb science-oriented dbms,cidr  presented collection requirement scidb dbms would meet need scientific user included nested-array data model science-specific operation regrid support uncertainty lineage named version paper present overview scidbs key feature outline demonstration first version scidb data operation one lighthouse user large synoptic survey telescope  
2009,requirement science data base scidb,past year assembling requirement collection scientific data base user astronomy particle physic fusion remote sensing oceanography biology intent specify common set requirement new science data base system call scidb addition discovered complex business analytics share requirement big science also constructed partnership company fund development scidb including ebay large synoptic survey telescope  microsoft stanford linear accelerator center  vertica lastly identified two lighthouse customer  run initial system constructed paper report requirement identified briefly sketch scidb design
2009,comparison approach large-scale data analysis,n/a
2009,new direction tpc,paper give author’s opinion concerning contribution transaction processing council  made past viewed present colleague offer suggestion go future short tpc become vendor-dominated time tpc reinvent serve customer community
2008,technical perspective - one size fit idea whose time come gone,last  year commercial dbms development summed single phrase one size fit phrase refers fact traditional dbms architecture  used support many data-centric application widely varying characteristic requirement paper argue concept longer applicable database market commercial world fracture collection independent database engine may unified common front-end parser use example stream-processing market datawarehouse market bolster claim also briefly discus market traditional architecture poor fit argue critical rethinking current factoring system service product
2008,h-store high-performance distributed main memory transaction processing system,previous work shown architectural application shift resulted modern oltp database increasingly falling short optimal performance  particular availability multiple-cores abundance main memory lack user stall dominant use stored procedure factor portend clean-slate redesign rdbmss previous work showed redesign potential outperform legacy oltp database significant factor  however obtained using bare-bones prototype developed demonstrate potential system since set design complete execution platform implement idea presented original paper demonstration presented provides insight development distributed main memory oltp database allows study challenge inherent operating environment
2008,jim gray win turing award,short paper intended describe layman jim gray many award culminating selected receive  acm turing award arguably nobel prize computer science briefly summarizes main contribution field 
2008,fault-tolerance borealis distributed stream processing system,past year stream processing engine  emerged new class software system enabling low latency processing stream data arriving high rate spes mature get used monitoring application must continuously run  significant challenge arises spes must able handle various software hardware fault occur masking provide high availability  article develop implement evaluate dpc  protocol handle crash failure processing node network failure distributed spelike previous approach ha dpc us replication mask many type node network failure presence network partition designer replication system face choice providing availability data consistency across replica dpc choice made explicit user specifies availability bound  dpc attempt minimize resulting inconsistency replica  meeting given delay threshold although conceptually simple dpc protocol tolerates occurrence multiple simultaneous failure well failure occur recoverythis article describes dpc implementation borealis spe show dpc enables distributed spe maintain low-latency processing time also achieving eventual consistency application eventually receive complete correct output stream furthermore show independent system size failure location possible handle failure almost up-to user-specified bound manner meet required availability without introducing inconsistency
2008,oltp looking glass found,online transaction processing  database include suite feature - disk-resident b-trees heap file locking-based concurrency control support multi-threading - optimized computer technology late advance modern processor memory network mean today computer vastly different  year ago many oltp database fit main memory oltp transaction processed millisecond le yet database architecture changed littlebased observation look interesting variant conventional database system one might build exploit recent hardware trend speculate performance detailed instruction-level breakdown major component involved transaction processing database system  running subset tpc-c rather simply profiling shore progressively modified every feature removal optimization  working system fully ran workload overall identify overhead optimization explain total difference factor x raw performance also show single high pole tent modern  database system substantial time spent logging latching locking b-tree buffer management operation
2007,architecture database system,n/a
2007,one size fit part 2 benchmarking study,two year ago u wrote paper predicting demise one size fit  paper examined stream processing data warehouse market gave reason substantial performance advantage specialized architecture market herein make three additional contribution first present reason performance advantage enjoyed specialized implementation text processing market second major contribution paper show apple apple performance number commercial implementation specialized architecture relational dbms stream processing data warehouse finally also show comparison number academic prototype specialized architecture scientific intelligence application relational dbms widely used mathematical computation tool summary appear least four market specialized architecture enjoy overwhelming performance advantage
2007,end architectural era time complete rewrite,previous paper  u predicted end one size fit commercial relational dbms paradigm paper presented reason experimental evidence showed major rdbms vendor outperformed -- order magnitude specialized engine data warehouse stream processing text scientific database marketsassuming specialized engine dominate market time current relational dbms code line left business data processing  market hybrid market one kind capability required paper show current rdbmss beaten nearly two order magnitude oltp market well experimental evidence come comparing new oltp prototype h-store built mit popular rdbms standard transactional benchmark tpc-cwe conclude current rdbms code line attempting one size fit solution fact excel nothing hence  year old legacy code line retired favor collection scratch specialized engine dbms vendor  start clean sheet paper design system tomorrow requirement continue push code line architecture designed yesterday need
2006,data integration transform reuse morpheus project,discus morpheus data transformation construction tool associated repository architecture morpheus motivated goal reuse  previously written transformation solve data integration problem finding relevant one repository modifying repurposing addition morpheus integrated dbms leverage existing capability including runtime environment transforms discus architecture morpheus illustrate usage help simple transform construction scenario
2005,lowell database research self-assessment,database need changing driven internet increasing amount scientific sensor data article author propose research several important new direction database management system
2005,8 requirement real-time stream processing,application require real-time processing high-volume data steam pushing limit traditional data processing infrastructure stream-based application include market feed processing electronic trading wall street network infrastructure monitoring fraud detection command control military environment furthermore sea change caused cheap micro-sensor technology take hold expect see everything material significance planet get sensor-tagged report state location real time sensorization real world lead green field novel monitoring control application high-volume low-latency processing requirementsrecently several technology emerged---including off-the-shelf stream processing engines---specifically address challenge processing high-volume real-time data without requiring use custom code time existing software technology main memory dbms rule engine also repurposed marketing department address applicationsin paper outline eight requirement system software meet excel variety real-time stream processing application goal provide high-level guidance information technologist know look evaluation alternative stream processing solution paper serf purpose comparable requirement paper relational dbms on-line analytical processing also briefly review alternative system software technology context requirementsthe paper attempt vendor neutral specific commercial product mentioned
2005,one size fit idea whose time come gone abstract,last  year commercial dbms development summed single phrase one size fit phrase refers fact traditional dbms architecture  used support many data-centric application widely varying characteristic requirement paper argue concept longer applicable database market commercial world fracture collection independent database engine may unified common front-end parser use example stream-processing market data-warehouse market bolster claim also briefly discus market traditional architecture poor fit argue critical rethinking current factoring system service product
2005,thalia test harness assessment legacy information integration approach,introduce new publicly available testbed benchmark called thalia  testing evaluating integration technology thalia provides researcher collection  downloadable data source representing university course catalog computer science department worldwide addition thalia currently provides set twelve challenge query well scoring function ranking performance integration system second contribution systematic classification type syntactic semantic heterogeneity directly lead twelve challenge chosen course information domain discourse well known easy understand furthermore abundance data source publicly available allowed u develop testbed exhibiting syntactic semantic heterogeneity identified
2005,high-availability algorithm distributed stream processing,stream-processing system designed support emerging class application require sophisticated timely processing high-volume data stream often originating distributed environment unlike traditional data-processing application require precise recovery correctness many stream-processing application tolerate benefit weaker recovery guarantee paper study various recovery guarantee pertinent recovery technique meet correctness performance requirement stream-processing application discus design algorithmic challenge associated proposed recovery technique describe provide different guarantee proper combination redundant processing checkpointing remote logging using analysis simulation quantify cost recovery guarantee examine performance applicability recovery technique also analyze knowledge query network property help decrease cost high availability
2005,c-store column-oriented dbms,paper present design read-optimized relational dbms contrast sharply current system write-optimized among many difference design storage data column rather row careful coding packing object storage including main memory query processing storing overlapping collection column-oriented projection rather current fare table index non-traditional implementation transaction includes high availability snapshot isolation read-only transaction extensive use bitmap index complement b-tree structureswe present preliminary performance data subset tpc-h show system building c-store substantially faster popular commercial product hence architecture look encouraging
2004,retrospective aurora,experience paper summarizes key lesson learned throughout design implementation aurora stream-processing engine past  year built five stream-based application using aurora first describe detail application implementation aurora reflect design aurora based experience finally discus initial idea follow-on project called borealis whose goal eliminate limitation aurora well address new key challenge application stream-processing domain
2004,contract-based load management federated distributed system,paper focus load management loosely-coupled federated distributed system present distributed mechanism moving load autonomous participant using bilateral contract negotiated offline set bounded price moving load show mechanism good incentive property efficiently redistributes excess load low overhead practiceour load management mechanism especially well-suited distributed stream-processing application emerging class data-intensive application employ continuous query processing model model stream data processed composed continuously arrive rather indexed stored implemented mechanism medusa distributed stream processing system demonstrate property using simulation experiment
2004,load management high availability medusa distributed stream processing system,medusa  distributed stream processing system based aurora single-site stream processing engine  demonstrate medusa handle time-varying load spike provides high availability face network partition demonstrate medusa context borealis second generation stream processing engine based aurora medusa
2004,linear road stream data management benchmark,paper specifies linear road benchmark stream data management system  stream data management system process streaming data executing continuous historical query producing query  real-time benchmark make possible compare performance characteristic sdms relative alternative  system linear road endorsed sdms benchmark developer aurora   stream   stream systemslinear road simulates toll system motor vehicle expressway large metropolitan area tolling system us variable tolling  increasingly prevalent tolling technique us dynamic factor traffic congestion accident proximity calculate toll charge linear road specifies variable tolling system fictional urban area including feature accident detection alert traffic congestion measurement toll calculation historical query specifying benchmark describe experimental  involving two implementation one using commercially available relational database using aurora  show dedicated stream data management system outperform relational database least factor  streaming data application
2003,aurora medusa project,n/a
2003,aurora new model architecture data stream management,paper describes basic processing model architecture aurora new system manage data stream monitoring application monitoring application differ substantially conventional business data processing fact software system must process react continual input many source  rather human operator requires one rethink fundamental architecture dbms application area paper present aurora new dbms currently construction brandeis university brown university mit first provide overview basic aurora model architecture describe detail stream-oriented set operator
2003,visionary next generation visualization system database,n/a
2003,aurora data stream management system,aurora system  experimental data stream management system fully functional prototype includes graphical development environment runtime system propose demonstrate aurora system development environment runtime system several example monitoring application developed consultation defense financial natural science community also demonstrate effect various system alternative various workload example show different scheduling algorithm affect tuple latency internal queue length use visualization tool accomplish data stream management aurora data stream management system monitoring application stream continuous data feed source sensor satellite stock feed monitoring application track data numerous stream filtering sign abnormal activity processing purpose aggregation reduction correlation management requirement monitoring application differ profoundly satisfied traditional dbms traditional dbms assumes passive model data processing  human issuing transaction query data stream management requires active approach monitoring data feed unpredictable external source  alerting human abnormal activity detected traditional dbms manages data currently table data stream management often requires processing data bounded finite window value unbounded past traditional dbms provides exact answer exact query blind real-time deadline data stream management often must respond real-time deadline  therefore must often provide reasonable approximation query traditional query processor optimizes query way  stream data manager benefit application specific optimization criterion  traditional dbms assumes pull-based query norm push-based data processing norm data stream management system brief summary aurora aurora designed deal large number data stream user build query small set operator  current implementation provides user interface tapping pre-existing input network flow wiring box together produce answer output certainly possible accept input declarative query feel large number query process common sub-expression elimination difficult example aurora network given screen shot  simple stream potentially infinite sequence tuples stream id arc carry multiple simple stream important simple stream added deleted system without modify basic network query sub-network end single output includes arbitrary number input box connect multiple downstream box path split carry identical tuples multiple stream merged since box type accept one input  allow cycle operator network output supplied quality service  specification currently qos captured three function  latency graph  value-based graph  loss-tolerance graph latency graph indicates utility drop answer delayed value-based graph show value output space important loss-tolerance graph simple way describe averse application approximate answer tuples arrive input queued processing scheduler selects box waiting tuples executes box one input tuples output tuples box queued input next box sequence way tuples make way input output system overloaded qos adversely affected case invoke load shedder strategically eliminate aurora support persistent storage two different way first box queue consume storage available ram system spill tuples le likely needed soon secondary storage second ad hoc query connected  arc connection point defined connection point store historical portion stream flowed arc example one could define connection point last hour worth data seen given arc ad hoc query connects connection point access full stored history well additional data flow past query connected
2003,load shedding data stream manager,data stream manager accepts push-based input set data source process input respect set standing query produce output based quality-of-service  specification input rate exceed system capacity system become overloaded latency deteriorate condition system shed load thus degrading answer order improve observed latency  paper examines technique dynamically inserting removing drop operator query plan required current load examine two type drop first drop fraction tuples randomized fashion second drop tuples based importance content address problem determining load shedding needed query plan insert drop much load shed point plan describe efficient solution present experimental evidence bring system back useful operating range minimal degradation answer quality
2003,operator scheduling data stream manager,many stream-based application sophisticated data processing requirement real-time performance expectation need met high-volume time-varying data stream order address challenge propose novel operator scheduling approach specify  operator schedule  order schedule operator  many tuples process execution step study approach context aurora data stream managerwe argue fine-grained scheduling approach combination various scheduling technique  significantly improve system efficiency reducing various system overhead also discus application-aware extension make scheduling decision according per-application quality service  specification finally present prototype-based experimental  characterize efficiency effectiveness approach various stream workload processing scenario
2002,much middleware,movement client-server computing multi-tier computing created potpourri so-called middleware system including application server workflow product eai system etl system federated data system paper argue explosion middleware created myriad poorly integrated system overlapping functionality world would well served considerable consolidation present way might happen point covered paper previously explored 
2002,monitoring stream - new class data management application,paper introduces monitoring application show differ substantially conventional business data processing fact software system must process react continual input many source  rather human operator requires one rethink fundamental architecture dbms application area paper present aurora new dbms currently construction brandeis university brown university mit describe basic system architecture stream-oriented set operator optimization tactic support real-time operation
2001,datasplash direct manipulation environment programming semantic zoom visualization tabular data,describe datasplash direct manipulation system creating semantic zoom visualization tabular  data datasplash make contribution three area key construction visualization first datasplash help user graphically specify visual appearance group object second system help user visually program way appearance group object change user browse visualization third datasplash allows user create group graphical link canvas direct manipulation facility simplify process constructing semantic zoom application particularly one display large data set
2001,semantic web perfection seeking view drug terminology,date semantic web viewed formal terminology ontology either immutable something change past future -- present change process -- perfection seeking outside scope proposed semantics except far represented attribute contrast current u government effort formalize drug  terminology driven need manage change terminology asynchronously longitudinally example year fda  approves  new drug thousand change label existing drug vha  must manage new drug label change ten thousand drug packaging change nlm  must maintain current index reference proposed approved medication world biomedical literature propose emerging multi-federal-agency reference terminology model medication mrt used drive development necessary repertoire semantic change management mechanism semantic web process mechanism organized ontology change
2001,content integration e-business,define problem content integration e-business show differs fundamental way traditional issue surrounding data integration application integration data warehousing oltp content integration includes catalog integration special case encompasses broader set application challenge explore characteristic content integration required service solution addition explore architectural alternative discus use xml arena
1999,independent open enterprise data integration,database researcher practitioner long espoused virtue data independence logical physical representation data separated multiple user see different view data flexibility usage evolution perfor mance database system maximized tenet lost marketing crush data warehousin g prescribes tight coupling physical representation high-level usability paper describe cohera federated dbms ntroduces data independence heterogeneous database present today enterprise c oheras physical independence feature provide scalable spectrum solution physical design ente rprise-wide data logical independence feature remove distinction data transformati querying using industry-standard sql unified open conversion framework
1999,vida visual information density adjuster,multiple study shown clutter sparsity visual representation negative effect ranging decreased user performance diminished visual appeal developed system assist user construction navigation visualization appropriate visual information density system vida  applies cartographic principle minimize clutter sparsity visual display information
1998,interoperability distributed application distributed database virtual table interface,user distributed database distributed application framework require interoperation heterogeneous data component respectively paper examine extensible objectrelational database system integrate mode interoperability describe virtual table interface facility informix-dynamic server universal data option provides simplified access heterogeneous component discus benefit integrating data application component
1998,asilomar report database research,database research community rightly proud success basic research remarkable record technology transfer field need radically broaden research focus attack issue capturing storing analyzing presenting vast array online data database research community embrace broader research agenda  broadening definition database management embrace content web online data store rethinking fundamental assumption light technology shift accelerate transition recommend changing way research  evaluated presented particular advocate encouraging speculative long-range work moving conference poster format publishing research literature web
1998,constant information density zoomable interface,introduce system help user construct interactive visualization constant information density work extension datasplash database visulaization environment datasplash direct manipulation system user construct navigate visualization object appearance change user zoom closer away visualization user specify graphically point change occurour experience datasplash indicates user find difficult construct visualization display appropriate amount detail paper introduce extension datasplash based principle constant information density extension give user feedback density visualization create also introduce extension suggests improvement existing visualizationswe performed informal study user navigation application without constant information density suggest designer take density account designing application avoid biasing user navigation unexpected way
1998,goal-directed zoom,introduce novel zoom method goal-directed zoom goal-directed zoom system user specify representation object wish see system automatically zoom elevation representation appears appropriate detail extended database visualization environment support end-user construction visualization goaldirected zoom present sample visualization constructed using environment 
1998,datasplash,database visualization area growing importance database system become larger accessible datasplash easy-to-use integrated environment navigating creating querying visual representation data demonstrate three main component make datasplash environment navigation system direct-manipulation interface creating modifying visualization direct-manipulation visual query system
1998,constant density visualization non-uniform distribution data,cartographic principle constant information density suggests amount information interactive visualization remain constant user pan zoom previous work presented system vida  help user manually construct application overall display density remains constant context semantic zoom system approach ensures uniformity z dimension extend naturally ensuring uniformity x dimension paper present new approach automatically creates display uniform x z dimension new system user express constraint visual representation appear display system applies constraint subdivision display subdivision meet target density value implemented technique datasplash/vida database visualization environment describe algorithm implementation advantage disadvantage approach
1998,viqing visual interactive querying,paper present viqing environment expressing query via direct manipulation data visualization viqing provides simple graphical interface connecting visualization expressive power basic relational operator select project join viqing implemented tioga datasplash visualization system provide seamless integration querying browsing resulting system unique providing unified visual interface developing database application encompassing querying data visualization
1997,supporting fine-grained data lineage database visualization environment,lineage datum record processing history information used trace source anomaly error processed data set valuable user variety application including investigation anomaly debugging traditional data lineage approach rely metadata however metadata scale well fine-grained lineage especially large data set example feasible store information necessary trace specific floating-point value processed data set particular satellite image pixel source data set paper propose novel method support fine-grained data lineage rather relying metadata approach lazily computes lineage using limited amount information processing operator base data introduce notion weak inversion verification system perfectly invert data us weak inversion verification provide number guarantee lineage generates propose design implementation weak inversion verification object-relational database management system
1997,esmdis earth system model data information system,goal development earth system model data information system  provide earth scientist  output management system earth system model  browse metadata retrieve desired subset esm output  analysis system esm output related datasets  automated pipelining system esm data processing  visualization system  web based user interface utilize system esmdis based dbms centric approach built upon bigsur earth science data schema developed using object relational dbms built prototype esmdis present  development
1996,query processing parallel object-relational database system,n/a
1996,mariposa wide-area distributed database system,requirement wide-area distributed database system differ dramatically local-area network system wide-area network  configuration individual site usually report different system administrator different access charging algorithm install site-specific data type extension different constraint servicing remote request typical last point production transaction environment fully engaged normal business hour cannot take additional load finally may many site participating wan distributed dbms world single program performing global query optimization using cost-based optimizer work well cost-based optimization respond well site-specific type extension access constraint charging algorithm time-of-day constraint furthermore traditional cost-based distributed optimizers scale well large number possible processing site since traditional distributed dbms used cost-based optimizers appropriate wan environment new architecture required proposed implemented economic paradigm solution issue new distributed dbms called mariposa paper present architecture implementation mariposa discus early feedback operating characteristic
1996,tioga-2 direct manipulation database visualization environment,paper report user experience tioga dbms centric visualization tool developed berkeley based experience designed tioga- direct manipulation system powerful much easier program detailed design revised system presented together extensive example application
1996,data replication mariposa,mariposa distributed data manager us economic model managing allocation storage object query server present extension economic model support replica management well mechanism propagating update among replica show replica control mechanism used provide consistent although potentially stale view data across many machine without expensive per-transaction synchronization present rule-based conflict resolution mechanism used enhance traditional time-stamp serialization discus effect replica system query processing read-only read-write query demonstrate replication model mechanism naturally support name service mariposa
1996,reordering query execution tertiary memory database,relational model order fetching data affect query correctness flexibility exploited query optimization statically reordering data access however query optimized executed fixed order system result data request made fixed order limited form runtime reordering provided low-level device manager aggressive reordering strategy essential scenario latency access data object varies widely dynamically tertiary device paper present strategy key innovation exploit dynamic reordering match execution order optimal data fetch order part plan-tree demonstrate practicality approach impact optimization report prototype implementation based postgres using system typical i/o cost query tertiary memory database much order magnitude smaller conventional query processing technique
1995,chabot retrieval relational database image,selecting large expanding collection image requires carefully chosen search criterion present approach integrates relational database retrieval system color analysis technique chabot project initiated university study storage retrieval vast collection digitized image image state california department water resource goal integrate relational database retrieval system content analysis technique would give querying system better method handling image simple color analysis method used conjunction search criterion improves ability retrieve image efficiently best result obtained text-based search criterion combined content-based criterion coarse granularity used content analysis< >
1995,overview sequoia 2000 project,author describes sequoia  research program university california project refinement computing-specifically involving storage networking file system extensible database management visualization-will applied specific global change application planet earth sequoia  organized around interconnected collection hardware file system dbms  networking visualization repository project author discus turn
1995,tioga-2 database visualization environment,paper report user experience tioga dbms-centric visualization tool developed berkeley based experience designed tioga- direct manipulation system powerful much easier program present detailed design revised system together extensive example application also give progress report tioga- implementation
1995,navigation coordination primitive multidimensional visual browser,paper describes extension tioga flight-simulator browsing protocol presented stonebraker et al  extension allow user navigate multidimensional data space using sophisticated zooming capability design also allows user move easily different multidimensional space tunneling different data space shown substantial generalization hyperlink hypermedia system finally design provides coordination multiple browser preserve context allows user explore multiple path simultaneously
1995,buffering intermediate result dataflow diagram,buffering intermediate  dataflow diagram significantly reduce latency user browse  re-executes diagram slightly different input define optimal buffer allocation problem determining buffer content minimize average response time user request show problem several characteristic render traditional latency reduction technique ineffective since optimal buffer allocation np-hard propose heuristic method buffer management intermediate  present simulation behavior heuristic variety condition varying graph structure access pattern argue history mechanism track user access pattern used improve performance show graph structure access pattern determine factor improvement possible performance enhancement describe applied minimize query response time visual dataflow language examine strategy buffering intermediate  dataflow diagram context tioga graphical application development tool
1995,bigsur system management earth science data,paper present prototype system management earth science data novel take dbms centric view task prototype -called bigsur -is shown context use two geographically distributed scientific group demanding data storage processing requirement bigsur currently store  terabyte data one thousandth volume eosdis must store claim design principle embodied bigsur provide sufficient flexibility achieve difficult scientific technical objective mission planet earth
1994,sequoia 2000 metadata schema satellite image,sequoia  schema development based emerging geospatial standard accelerate development facilitate data exchange paper focus metadata schema digital satellite image examine satellite metadata defined used maintained discus geospatial standard using describe sql prototype based spatial archive interchange format  standard implemented illustra object-relational database 
1994,mariposa new architecture distributed data,describe design mariposa experimental distributed data management system provides high performance environment high data mobility heterogeneous host capability mariposa design unifies approach taken distributed file system distributed database addition mariposa provides general flexible platform development new algorithm distributed query optimization storage management scalable data storage structure flexibility primarily due unique rule-based design permit autonomous local-knowledge decision made regarding data placement query execution location storage management< >
1994,implementing calendar temporal rule next generation database,application like financial trading scheduling manufacturing process control time based predicate query rule important also need define list time point interval author refer list calendar author present system calendar allow specification natural-language time-based expression maintenance valid time database specification temporal condition database query rule user-defined semantics date manipulation simple list based language proposed define manipulate query calendar design parser algorithm efficient evaluation calendar expression also described paper also describes implementation time-based rule postgres using proposed system calendars< >
1994,efficient organization large multidimensional array,large multidimensional array widely used scientific engineering database application author present method organizing array make access secondary tertiary memory device fast efficient developed four technique  storing array multidimensional chunk minimize number block fetched  reordering chunked array minimize seek distance accessed block  maintaining redundant copy array organized different chunk size ordering  partitioning array onto platter tertiary memory device minimize number platter switch measurement real data obtained global change scientist show access array organized using technique often order magnitude faster unoptimized data< >
1994,sequoia 2000 next-generation information system study global change,better data management crucial success scientific investigation global change new mode research earth especially synergistic interaction observation model require massive amount diverse data stored organized accessed distributed visualized analyzed address technical issue better data management participant sequoia  collaborative effort computer scientist earth scientist several campus university california digital equipment corporation  apply refinement computing specific application software architecture includes layer common device interface file system database management system  application network early prototype application software include global-change data schema integration general circulation model  remote sensing data system climate study longer range effort include transfer protocol moving element database controller secondary tertiary storage distributed file system distributed dbms< >
1994,economic paradigm query processing data migration mariposa,many new database application require large volume data mariposa database system construction berkeley responding need system combine best feature traditional distributed database system object-oriented dbms tertiary memory file system distributed file system mariposa object stored thousand autonomous site memory hierarchy large capacity scale system lead complex query execution storage management issue unsolvable practice traditional technique propose economic paradigm solution query receives budge spends obtain answer site attempt maximize income buying selling storage object processing query locally stored object present protocol underlie mariposa economy< >
1994,sequoia 2000 reflection first three year,purpose sequoia  project build better computing environment global change researcher researcher investigate issue global warming ozone depletion environment toxification specie extinction member earth science department university national laboratory sequoia  digital equipment corporation flagship research project describe sequoia  project implementation effort first three year included objective chose address lesson learned endeavour< >
1994,zooming tunneling tioga supporting navigation multimedia space,tioga system applies box arrow programming notation allow nonexpert user graphically construct database application user connect database procedure using dataflow model browser used visualize resulting data paper describes extension tioga browser protocol extension allow sophisticated flight-simulator navigation multidimensional data space design also incorporates wormhole allow tunneling different multidimensional space wormhole shown substantial generalization hyperlink hypertext system powerful mechanism relating data provide user great flexibility example user create magnifying glass provide enhanced view underlying data< >
1993,sequoia 2000 project,paper describes objective sequoia  project software development done achieve objective addition several lesson relevant geographic information system  learned project explained 
1993,optimization parallel query execution plan xprs,author describe approach optimization query execution plan xprs multi-user parallel database machine based shared-memory multi-processor disk array main difficulty optimization problem compile-time unknown parameter available buffer size number free processor enormous search space possible parallel plan author deal problem novel two phase optimization strategy dramatically reduces search space allows run time parameter without significantly compromising plan optimality present two phase strategy give experimental evidence xprs benchmark indicate almost always produce optimal plan
1993,large object support postgres,four implementation support large object postgres database system presented four implementation offer varying level support security transaction compression time travel implemented using postgres abstract data type paradigm support user-defined operator function allow file-oriented access large object database support user-defined storage manager available postgres also detailed performance four large object implementation two different storage device described< >
1993,highlight file system tertiary storage,highlight file system combining secondary disk storage tertiary robotic storage developed part sequoia  project described highlight extension bsd log-structured file system  provides hierarchical storage management without requiring special support application author present highlight design various policy automatic migration file data hierarchy level performance highlight compared bsd lf implementation initial  indicate highlight performance comparable bsd lf disk-resident data overhead associated accessing data tertiary cache negligible< >
1993,sequoia 2000 benchmark,paper present benchmark concisely capture data base requirement collection earth scientist working sequoia  project various aspect global change research benchmark novel characteristic us real data set real query representative earth science task appears earth science problem typical problem engineering scientific dbms user claim benchmark represents need general community also included paper benchmark  three example dbms grass ipw postgres
1993,predicate migration optimizing query expensive predicate,traditional focus relational query optimization scheme choice join method join order restriction typically handled query optimizers predicate pushdown rule apply restriction random order many join possible rule work assumption restriction essentially zero-time operation however today extensible object-oriented database system allow user define time-consuming function may used query restriction join predicate furthermore sql long supported subquery predicate may arbitrarily time-consuming check thus restriction considered zero-time operation model query optimization must enhancedin paper develop theory moving expensive predicate query plan total cost plan  including cost join restriction  minimal present algorithm implement theory well  implementation postgres experience newly enhanced postgres query optimizer demonstrates correctly optimizing query expensive predicate often produce plan order magnitude faster plan generated traditional query optimizer additional complexity considering expensive predicate optimization found manageably small
1993,miro dbms,short paper explains key object-relational  dbms technology used miro dbms
1993,highlight using log-structured file system tertiary storage management,robotic storage device offer huge storage capacity low cost per byte large access time integrating device storage hierarchy present challenge file system designer log-structured file system  developed reduce latency involved accessing disk device sequential write pattern match well tertiary storage characteristic unfortunately existing version manage memory cache disk support broader storage hierarchy highlight extends bsd lf incorporate secondary storage device  tertiary storage device  providing hierarchy within file system require application support paper present design highlight proposes various policy automatic migration file data hierarchy level present initial migration mechanism performance figure
1993,tioga database-oriented visualization tool,work present new architecture visualization system based data base management system  technology building mechanism present next-generation dbms rather merely capability standard file manager show simpler powerful visualization system constructed retain popular box arrow programming notation constructing visualization program add flight simulator model movement navigate output program addition provide mean specify hierarchy abstract data different type resolution zoom capability supported underlying dbms support system tioga briefly described well current state implementation< >
1993,tioga providing data management support scientific visualization application,present user interface paradigm database management system motivated scientific visualization application graphical user interface includes box arrow notation database access flight simulator model movement information space also provide mean specify hierarchy abstract data different type resolution zoom capability supported underlying dbms support system described includes compilation query plan megaplans new algorithm data buffering provision guaranteed rate data delivery current state tioga implementation also described
1992,integration rule system database system,integration rule system database management system explored research activity area past decade surveyed focus prototype system completely specified implementation issue encountered research agenda addressed research community next year presented< >
1991,postgres next generation database management system,n/a
1991,database system achievement opportunity,history database system research u one exceptional productivity startling economic impact barely twenty year old basic science research field database research conducted federal support nation university industrial research laboratory fueled information service industry estimated $ billion per year u alone industry grown average rate  percent per year since  continuing expand rate achievement database research underpin fundamental advance communication system transportation logistics financial management knowledge-based system accessibility scientific literature host civilian defense application also serve foundation considerable progress basic science various field ranging computing biology
1991,read optimized file system design performance evaluation,performance comparison presented several file system allocation policy file system designed provide high bandwidth disk main memory taking advantage parallelism underlying disk array catering large unit transfer minimizing bandwidth dedicated transfer metadata file system described use multiblock allocation strategy allows large small file allocated efficiently simulation  show multiblock policy result system able utilize large percentage underlying disk bandwidth  general-purpose system called upon support data intensive application database supercomputing policy offer opportunity provide superior performance larger class users< >
1991,managing persistent object multi-level store,paper present architecture persistent object store multi-level storage explicitly included traditionally dbms assumed accessible data resides magnetic disk recently several researcher begun consider possibility significant amount data occupy space main memory cache feel object base time critical object reside main memory object disk resident remainder occupy tertiary memory moreover possible three level present level remote hardware paper contains architectural proposal addressing need along sketch required query optimizer
1991,segment index dynamic indexing technique multi-dimensional interval data,propose new indexing technique interval data k  dimension consisting set extension class database indexing structure technique useful improving index search performance spatial data composed multi-dimensional interval non-uniform length distributiona interval data collection non-uniform length distribution likely occur practice may typical historical data collection tuples represent interval time dimension present indexing technique illustrate may applied r-tree index end provide  performance experiment
1991,using write protected data structure improve software fault tolerance highly available database management system,paper describes database management system  modified use hardware write protection guard critical dbms data structure software error guarding  dbms data improves software reliability providing quick detection corrupted pointer array bound overrun guarding especially helpful extensible dbms since limit power extension code corrupt unrelated part system read-write data structure guarded long correct software able temporarily unprotect data structure update paper discus effect three different update model performance software complexity error protection measurement dbms us guarding protect buffer pool show two eleven percent performance degradation debit/credit benchmark
1990,architecture future data base system,paper first discus three typical programming interface future dbms system may support interface non-procedural set-oriented   navigational   access method briefly comparing language level provided interface four data base system architecture described support high level interface top one two interface believe architecture reasonable candidate future dbms package
1990,third-generation database system manifesto - committee advanced dbms function,preparation beehive synthetic resinous material acceptable bee attacked vermin exhibit requisite physical property provide desirable beehive accomplished assembling hive molded urethane foam panel urethane foam formulated produce product rejected bee make bee nervous otherwise interfere normal habit secreting honey said beehive
1990,data base research berkeley,data base research berkeley decomposed four category first postgres project led michael stonebraker building next-generation dbms novel object management knowledge management time travel capability second larry rowe lead picasso project constructing advanced application development tool kit use postgres dbms third xprs project  led four berkeley faculty randy katz john ousterhout dave patterson michael stonebraker xprs exploring high performance i/o system efficient utilization operating system data manager last project one aimed improving reliability dbms software dealing effectively software error also led michael stonebraker
1990,implementation postgres,design implementation decision made three-dimensional data manager postgres discussed attention restricted dbms backend function postgres data model query language rule system storage system postgres implementation current status performance discussed< >
1990,third-generation database manifesto brief retrospection,n/a
1990,alternative complex object representation performance perspective,database system finding wider use cad office information system logic programming application importance efficiently representing manipulating complex object growing study classification alternative representing complex object examined consideration given performance aspect one representation technique based object identifier shown clustering subobjects referencing object rarely good idea contrast shown caching intermediate  query processing yield large benefits< >
1990,distributed raid - new multiple copy algorithm,new multicopy algorithm proposed potentially attractive property much le space required equal performance provided normal operation hand failure new algorithm offer lower performance conventional scheme algorithm may attractive various multicopy environment well disaster recovery algorithm presented compared various multicopy disaster recovery techniques< >
1990,rule procedure caching view data base system,paper demonstrates simple rule system constructed support powerful view system available current commercial system view specified using rule also special semantics resolving ambiguous view update simply additional rule moreover procedural data type proposed postgres also efficiently simulated rule system lastly caching action part certain rule possible performance enhancement applied materialize view well cache procedural data item hence conclude rule system fundamental concept next generation dbms subsumes view procedure special case
1990,postgres dbms,n/a
1990,committee advanced dbms function third generation data base system manifesto,n/a
1990,transaction support read optimizied write optimized file system,paper provides comparative analysis five implementation transaction support first method traditional approach implementing transaction processing within data manager top read optimized file system second also assumes traditional file system embeds transaction support inside file system third model considers traditional data manager top write optimized file system last two model embed transaction support inside write optimized file system using different logging mechanism  show transaction processing environment write optimized file system often yield better performance one optimized read addition show file system embedded transaction manager perform well data manager transaction throughput limited i/o bandwidth finally even cpu critical resource difference performance data manager embedded system much smaller previous work shown
1990,highly redundant management distributed data,algorithm redundant management distributed data using minimal amount data replication described analyzed main  storage space utilization i/o  performance reliability outlined recently introduced raid  concept extended distributed computing system resulting distributed storage architecture called radd  shown support redundant copy data across computer network space cost raid local data thus radd increase data availability presence temporary permanent failure  local site normal operation radd scheme offer performance comparable two-copies system radds considered possible alternative traditional multiple-copy technique well high-availability scheme
1989,commentary postgres rule system,paper suggests modification postgres rule system  increase usability function specifically suggest changing rule syntax powerful one propose additional keywords introduce notion rulesets whose purpose increase user control rule activation process expand versioning facility support broader range application currently possible 
1989,case partial index,current data manager support secondary and/or primary index column relation paper suggest advantage result index contain possible value column relation 
1989,future trend database system,author discus likely evolution commercial data manager next several year topic covered include following sql  become universal standard benefit sql standardization current sql standard chance lasting database system distributed soon new technology likely commercialized vendor independence may achievable< >
1989,performance consideration operating system transaction manager, previous comparison study  conventional transaction manager operating system  transaction manager indicated o transaction manager incurs severe performance penalty appears feasible special circumstance three approach enhancing performance o transaction manager considered first strategy improve performance reducing cost lock acquisition compressing log second strategy explores possibility still improvement additional semantics built o transaction system last strategy use modified index structure make update operation le expensive perform  show o implement essentially specialized tactic transaction management currently used database management system  order match dbms performance< >
1989,indexing technique historical database,two indexing structure based r-trees proposed historical data span magnetic disk optical disk medium performance index compared two indexing candidate contained entirely one medium test  indicate proposed index perform well compared index contained entirely optical disk< >
1988,project high performance i/o subsystem,n/a
1988,postgres rule manager,rule subsystem implemented postgres dbms explained novel several way first give user capability defining rule well data moreover depending scope rule defined optimization handled differently lead good performance many rule small scope rule large scope addition rule provide either forward-chaining backward-chaining control flow system chooses control mechanism optimizes performance whenever possible priority rule defined allowing user specify rule system conflict use exception seems necessary many application database service view protection integrity constraint referential integrity obtained simply applying rule system appropriate way consequently special-purpose code need included postgres handle tasks< >
1988,future trend expert data base system,paper discus see capability dbms evolving next several year meet need expert data base application also present research thrust see important appear receiving insufficient attention research community 
1988,performance comparison two architecture fast transaction processing,investigates issue involved using multiprocessor transaction processing author use simulation model study behavior two different architecture namely shared everything shared nothing shared everything processor access disk memory shared shared nothing neither disk memory shared study effect data contention resource contention architecture quantify effect intraquery parallelism architecture different operating conditions< >
1988,semantics based transaction management technique replicated data,data often replicated distributed database application improve availability response time conventional multi-copy algorithm deliver fast response time high availability read-only transaction sacrificing goal update paper propose multi-copy algorithm work well retrieval update environment exploiting special application semantics subdividing transaction various category utilizing commutativity property demonstrate cheaper technique show guarantee correctness performance comparison technique conventional one quantifies extent saving
1988,extended user-defined indexing application textual database,number application-specific searching mechanism including keyword searching textual database implemented naturally relational dbms using abstract datatypes userdefined operator query efficiency operator abstract datatypes must supported index new indexing scheme proposed allows large class query predicate evaluated using index including many key operator textual database indexing scheme also significantly reduces space required store indexed textual data relational database system
1988,design xprs,paper present overview technique using build dbms berkeley simultaneously provide high performance high availability transaction processing environment application complex ad-hoc query application large object image cad layout plan achieve goal using general purpose dbms operating system shared memory multiprocessor hardware software tactic using accomplish goal described paper include novel fast path feature special purpose concurrency control scheme twodimensional file system exploitation parallelism novel method efficiently mirror disk strive high performance three different application area  transaction processing  complex ad-hoc query  management large object
1987,extendability postgres,n/a
1987,effect join selectivity optimal nesting order,heuristic query optimizer must choose best way process incoming query choice based comparing expected cost many  way command might processed expected cost calculation determined statistic size relation involved selectivity operation performed course estimate subject error paper investigate sensitivity best query plan error selectivity estimate treat common case join query show optimal plan query insensitive selectivity inaccuracy hence little reason data manager spend lot effort making accurate estimate join selectivity
1987,extending database system procedure,paper suggests powerful database system  built supporting database procedure full-fledged database object particular allowing field database collection query query language system shown allow natural expression complex data relationship moreover many feature present object-oriented system semantic data model supported facilityin order implement construct extension typical relational query language must made considerable work execution engine underlying dbms must accomplished paper report extension one particular query language data manager give performance figure prototype implementation even though performance prototype competitive conventional system suggestion improvement presented
1987,performance issue high performance transaction processing architecture,paper investigate issue involved using multiprocessor high performance transaction processing application use simulation model compare performance two different architecture namely shared everything shared nothing shared everything processor access disk memory shared shared nothing neither disk memory shared study effect response time constraint architecture compare performance presence load imbalance addition study intra-query parallelism affect performance architecture different operating condition
1987,design postgres rule system,paper explains rule subsystem implemented postgres dbms novel several way first give user capability defining rule well data dbms moreover depending scope rule defined optimization handled differently lead good performance case many rule small scope rule large scope addition rule provide either forward chaining control flow backward chaining one system choose control mechanism optimizes performance case possible furthermore priority rule defined thereby allowing user specify rule system conflict use exception seems necessary many application lastly rule system support implementation view protection integrity control simply applying rule system particular way consequently special purpose code need included handle task
1987,postgres data model,paper describes data model postgres next-generation extensible database management system developed university california str data model relational model extended abstract data type data type procedure attribute procedure inheritance mechanism used simulate wide variety semantic object-oriented data modeling construct including aggregation generalization complex object shared subobjects attribute reference tuples relation
1987,design postgres storage system,paper present design storage system postgres data base system construction berkeley novel several way first storage manager support transaction management without using conventional write ahead log  fact code run recovery time consequently recovery crash essentially instantaneous second storage manager allows user optionally keep entire past history data base object closely integrating archival storage system historical record spooled lastly storage manager consciously constructed collection asynchronous process hence large monolithic body code avoided opportunity parallelism exploited paper concludes analysis storage system suggests performance competitive wal system many situation
1987,performance evaluation operating system transaction manager,conventional transaction manager implemented database management system  compared one implemented within operating system  variety simulated situation model concurrency control crash recovery constructed environment  collection experiment presented paper  indicate o transaction manager incurs severe performance disadvantage appears feasible special circumstance 
1986,case shared nothing,three dominent theme building high transaction rate multiprocessor system namely shared memory  shared disk  shared nothing  paper argues shared nothing preferred approach 
1986,operating system support data management,several operating system service examined view toward applicability support database management function service include buffer pool management file system scheduling process management interprocess communication consistency control 
1986,object management relational data base system,paper first present collection capability area object management desired non business data processing application three approach providing function application specific system semantic data model high leverage extension relational model examined advantage latter approach described 
1986,analysis rule indexing implementation data base system,paper discus several alternate implementation scheme rule indexing data base system two proposal much common predicate locking four others resemble version physical locking performance analysis conducted based abstract model rule indexing problem 
1986,inclusion new type relational data base system,paper explores mechanism support user-defined data type column relational data base system previous work suggested support new operator new data type contribution work suggest way allow query optimization command include new data type operator way allow access method used new data type
1986,object management postgres using procedure,paper present object management facility designed next-generation data manager postgres system unique invent new data model support object chooses instead extend relational model powerful abstract data typing capability procedure full-fledged data base object reason remain relational model indicated paper along postgres relational extension 
1986,design postgres,paper present preliminary design new database management system called postgres successor ingres relational database system main design goal new system toprovide better support complex objectsprovide user extendibility data type operator access methodsprovide facility active database  inferencing including forward- backward-chainingsimplify dbms code crash recoveryproduce design take advantage optical disk workstation composed multiple tightly-coupled processor custom designed vlsi chip andmake change possible  relational modelthe paper describes query language programming language interface system architecture query processing strategy storage system new system
1985,tip benchmarking data base system,n/a
1985,problem supporting data base transaction operating system transaction manager,paper report experience author attempting support data base transaction top existing operating system transaction manager seen significant modification example data base system example operating system required support concept  drawn operating system transaction manager designed generally suggested application program  participate transaction management process
1985,trigger inference data base system,collection database application  may best accomplished using collection trigger paradigm initial action recursively trigger dependent action often called forward chaining addition database support large knowledge base requires least simple inferencing capability retrieve command cannot satisfied using stored data data manager must determine rule knowledge base used reformulate query way one work desired data toward database fact must ascertained using backward chaining
1985,expert database systems/bases de données et systèmes expert,n/a
1984,using relational database management system computer aided design data - update,n/a
1984,performance analysis distributed data base system,paper briefly present design distributed relational data base system discus experimental observation performance system executing short long command also drawn concerning metric distributed query processing heuristic attempt minimize lastly comment architecture appear viable distributed data base application 
1984,implementation data abstraction relational database system ingres,paper discus design implementation abstract data type  facility added ingres database manager implementation adts allows user register adts adt operator run-time database manager declare column value relation instance adts formulate query containing reference adts adt operator user view implementation performance possible extension new facility described
1984,virtual memory transaction management,paper examine consequence operating system providing transaction management environment file bound user address space discussion focus inherent limitation providing concurrency control crash recovery service environment hardware extension needed overcome deficiency
1984,heuristic search data base system,n/a
1984,implementation technique main memory database system,availability large relatively inexpensive main memory becoming possible keep large database resident main memory paper consider change necessary permit relational database system take advantage large amount main memory evaluate avl v b+-tree access method main memory database hash-based query processing strategy v sort-merge study recovery issue database fit main memory expected b+-trees preferred storage mechanism unless -- database fit main memory somewhat surprising result hash based query processing strategy advantageous large memory situation 
1984,quel data type,paper explores use command query language abstract data type  data base management system basically adt facility allows new data type polygon line money time array floating point number bit vector etc supplement built-in data type data base system paper demonstrate power adding data type corresponding command query language also propose three extension query language quel enhance power augmented environment
1984,database portal new application program interface,paper describes design proposed implementation new application program interface database management system program browse database making ad-hoc update well served conventional embedding dbms command programming language new embedding suggested overcomes deficiency construct called portal allows program request collection tuples support novel concurrency control scheme 
1984,performance concurrency control algorithm database management system,paper describes study performance centralized concurrency control algorithm algorithm-independent simulation framework developed order support comparative study various concurrency control algorithm describe framework detail present performance  obtained believe representative cross-section many proposed algorithm basic algorithm studied include four locking algorithm two timestamp algorithm one optimistic algorithm also briefly summarize study several multiple version algorithm several hierarchical algorithm show general locking algorithm provide best performance
1983,implementation rule relational data base system,paper contains proposed implementation rule system relational data base system rule system provide data base service including integrity control protection alerters trigger view processing moreover used user specified rule proposed implementation make efficient use abstract data type facility introducing new data type assist rule specification enforcement 
1983,performance enhancement relational database system,paper examine four performance enhancement database management system dynamic compilation microcoded routine special-purpose file system special-purpose operating system examined context ingres database management system benchmark timing included suggest attractiveness dynamic compilation special-purpose file system microcode special-purpose operating system analyzed appear limited utility ingres context 
1983,document processing relational database system,paper contains proposal enhance relational database manager support document processing basically suggests support data item variable-length string support ordered relation support substring operation support new operator concatenate break apart string field
1983,formal model crash recovery distributed system,formal model atomic commit protocol distributed database system introduced model used prove existence  resilient protocol site failure partition network partitioned network site failure pessimistic recovery technique called independent recovery introduced class failure resilient protocol exist identified partitioned network two case studied pessimistic case message lost optimistic case message lost case fundamental limitation resiliency protocol derived
1983,application abstract data type abstract index cad data base,n/a
1983,dbms ai common point view,recent workshop explored commonality among point view al dbms programming language researcher surface would appear dbms researcher building smarter data base system al researcher building expert system containing knowledge base however appears minimal commonality point view expressed community therefore author dubious significant cross fertilisation illustrates difference point view two community  example
1983,implementation hypothetical relation,paper develop different approach implementing hypothetical relation previously proposed design borrows idea tactic based view differential file offer several advantage scheme actual implementation described performance statistic presented 
1982,observation evolution software system,n/a
1982,using relational database management system computer aided design data,n/a
1982,implementation time expert data base system,paper report design implementation time expert relational data base system demonstrates sophisticated expert written easily cause minimal performance degradation result extending data base system support user defined data type easy operation
1982,adding semantic knowledge relational database system,chapter suggests two mechanism adding semantic knowledge data manager namely inclusion ai oriented rule system particular use abstract data type topic explored context ingres relational database system 
1982,database perspective,n/a
1982,rule system relational data base management system,paper present specification proposed implementation rule system relational data base manager motivation proposal fact integrity constraint protection trigger alerters view example special purpose rule system suggest five service obtained one unified way single rule system 
1982,timber sophisticated relation browser invited paper,paper discus function present sophisticated relation browser support icon map text normal fixed format relation discussion required database extension support browser also presented
1981,operating system support database management,several operating system service examined view toward applicability support database management function service include buffer pool management file system scheduling process management interprocess communication consistency control
1981,hypothetical data base view,paper show hypothetical data base effectively supported slight extension conventional view support mechanism moreover argue resulting structure may well quite efficient advantage making hypothetical data base central operation dbms 
1980,retrospection database system,paper describes implementation history ingres database system focus mistake made progress rather eventual correction attention also given role structured design database system implementation problem supporting nontrivial user lastly miscellaneous impression unix pdp- data model given 
1980,embedding expert knowledge hypothetical data base data base system,paper concerned adding knowledge data base management system suggests two appropriate mechanism namely hypothetical data base  expert herein indicate need hdbs define extension needed data base system support hdbsin addition suggest notion expert appropriate way add semantic knowledge data base system unlike proposal extend underlying data model capture meaning proposal require extension schema moreover dbms even know expert function paper define expert indicate would added one existing data base system
1980,analysis distributed data base processing strategy,paper report query processing experiment performed one distributed data base environment environment compared strategy produced collection algorithm basis number byte moved among found limited search algorithm perform well compared algorithm exhaust possible processing plan 
1979,locking granularity revisited,locking granularity refers size hence number lock used ensure consistency database multiple concurrent update earlier simulation study concluded coarse granularity area file locking preferred fine granularity individual page record lockinghowever alternate assumption used original paper change  first modified assumption concerning placement lock database respect accessing transaction original model lock assumed well placed worse case random placement assumption small transaction access database fine granularity preferablesecond extended simulation model lock hierarchy large transaction use large lock small transaction use small lock scenario random worse case lock placement assumption fine granularity preferable transaction accessing  percent database use large locksfinally simulation extended model claim needed locking strategy together resultant possibility deadlock original study lock claimed one atomic operation beginning transaction claim needed strategy change concerning desired granularity 
1979,concurrency control consistency multiple copy data distributed ingres,paper contains algorithm ensuring consistency distributed relational data base subject multiple concurrent update also included mechanism correctly update multiple copy object continue operation le machine network operational together   paper constitutes significant portion design distributed data base version ingres
1979,performance analysis relational data base management system,effect performance data management system use extended storage device multiple processor prefetching data block analyzed respect one system ingres benchmark query stream derived user query run ingres system cpu usage data reference pattern traced  show performance characteristic two query type data-intensive query overhead-intensive query different may difficult design single architecture optimize performance type shown random access model data reference hold overhead-intensive query reference system catalog considered data reference significant sequentiality reference found data-intensive query shown back-end data management machine distribute processing toward data may cost effective data-intensive query proposed best method distributing processing overhead-intensive query use intelligent terminal third benchmark set multi-relation query devised proposal made taking advantage locality reference found
1978,b-trees re-examined,b-tree variant increasing frequency proposed basic storage structure multiuser database application three potential problem must dealt structure arise traditional static directory structure indicated one problem possible performance penalty 
1978,distributed query processing relational data base system,paper present new algorithm retrieving updating data distributed relational data base within data base number relation distributed number site moreover user supplied distribution criterion optionally used specify site tuple belongs tothe algorithm efficient way process query breaking qualification separate piece using simple heuristic cost criterion considered minimum response time minimum communication traffic addition algorithm optimize separately two model communication network representing respectively arpanet ethernet like network algorithm implemented part ingres data base system
1977,effect locking granularity database management system,many database system guarantee form integrity control upon multiple concurrent update form locking granule database chosen unit individually locked lock management algorithm used ensure integrity using simulation model paper explores desired size granule wide variety seemingly realistic condition surprisingly coarse granularity called paper concludes implication  concerning viability so-called predicate locking
1977,distributed database version ingres,n/a
1977,geo-ouel system manipulation display geographic data,paper briefly summarizes implementation geo-ouel special purpose geographic information retrieval display system basically rather small front end powerful general purpose relational data base system ingres implemented berkeley also discussed problem discovered implementation original proposal  corrective step taken lastly experiment described indicate performance penalty paid front end approach saving development time realized
1977,study effect locking granularity data base management system abstract,many data base system guarantee form integrity control upon multiple concurrent update form locking granule data base chosen unit individually locked lock management algorithm used ensure integrity simulation model paper explores desired size granule wide variety seemingly realistic condition surprisingly coarse granularity called paper concludes implication  concerning viability called predicate locking
1977,observation data manipulation language embedding general purpose programming language,many data base query language stand-alone coupled general purpose programming language proposed number issue various design addressed different way treated paper issue include specification performance option side effect implicitness handling type time binding case emphasis comparative analysis rather exhaustive survey proposal several general observation language design data base access also mades
1976,design implementation ingres,currently operational  version ingres database management system described multiuser system give relational view data support two high level nonprocedural data sublanguages run collection user process top unix operating system digital equipment corporation pdp / / / computer emphasis design decision tradeoff related  structuring system process  embedding one command language general purpose programming language  algorithm implemented process interaction  access method implemented  concurrency recovery control currently provided  data structure used system catalog role database administratoralso discussed  support integrity constraint   yet supported feature concerning view protection  future plan concerning system
1976,sigbdp paper session,n/a
1976,ingres protection system,paper present design protection system implemented ingres relational data base management system brief description ingres system operational environment first presented provide setting protection scheme mechanism protecting physical data file enforcing sophisticated access control rule shared relation presented lastly important design decision concerning protection discussed 
1976,sigmod paper session,n/a
1976,proposal network ingres,n/a
1976,data base management system ingres,n/a
1976,comparison use link secondary index relational data base system,possibility supporting relational data sublanguages top data base system underlying network data structure widely suggested paper present formal model mix interaction one non procedural relational language collection assumption concerning data base performance criterion compare following performance oriented data structure secondary index network structure similar pointer array implementation dbtg set structure   togetherit shown option  never preferred @@@@  range model parameter hence sole use set link performance oriented access path questionable
1976,embedding relational data sublanguage general purpose programming language,paper describes equel programming language embeds relational data sublanguage quel general purpose programming language c quel equel operational part ingres relational data base management system berkeley also briefly described two operational subsystem written combined language lastly language oriented shortcoming observed quel equel discussed 
1975,ingres relational data base system,ingres  relational data base graphic system implemented pdp-/ based hardware configuration berkeley ingres run normal user job top unix operating system developed bell telephone laboratory significant modification unix ingres requires substantial increase maximum file size allowed change implemented unix designer implementation ingres primarily programmed c high level language unix written parsing done assistance yacc compiler-compiler available unix
1975,network hierarchy relation data base management system,n/a
1975,storage structure access method relational data base management system ingres,n/a
1975,cupid - friendly query language,n/a
1975,implementation integrity constraint view query modification,user interface relatonal data base management system may decoupled storage representation data novel powerful efficient integrity control scheme possible paper indicates mechanism implemented one relational system prevent integrity violation result improper update process basically interaction data immediately modified query language level one guaranteed integrity violation also similar modification technique indicated support use view ie relation physically present data base defined term one 
1975,approach implementing geo-data system,often undesirable impossible provide redundant index domain file existing secondary storage device problem considered paper selection limited number index best facilitate interaction file probabilistic model interaction activity encompassing query update presented parametric description storage medium assumed significant  independent many file storage characteristic found concerning best choice index two case first choice domain include partial inversion desired find best possible subset domain provide index second case concern choice combined index situation best way grouping domain sought order provide one index group
1974,choice partial inversion combined index,often undesirable impossible provide redundant index domain file existing secondary storage device problem considered paper selection limited number index best facilitate interaction file probabilistic model interaction activity encompassing query update presented parametric description storage medium assumed significant  independent many file storage characteristic found concerning best choice index two case first choice domain include partial inversion desired find best possible subset domain provide index second case concern choice combined index situation best way grouping domain sought order provide one index group
1974,access control relational data base management system query modification,work describes access control system implemented ingres scheme applied relational data base management system several advantage suggested scheme include implementation ease b small execution time overhead c powerful flexible control conceptual simplicity basic idea utilized user interaction data base modified alternate form guaranteed access violation modification take place high level interaction language hence processing resulting interaction accomplished regard protection particular procedure call access path control purpose  avoided
1974,functional view data independence,many researcher used term data independence without indicating precise meaning one common definition isthe isolation program consideration data process another isthe ability application program execute correctly regardless actual storage dataalthough suggest general concept precise framework clearly needed current paper provides framework explores ramification 
1972,simplification forresters model urban area,greatly simplified version forresters model urban area described whereas forresters original model contained  state  equation revised one contains  state  equation revised model converges close approximation forresters equilibrium similar temporal behavior produce comparable  forresters  urban program applied
1972,retrieval efficiency using combined index,problem considered involves choosing best set index indexing file secondary storage device space may limited general class query specific index organization approximation expected retrieval time choice index developed subject simplifying assumption best selection index obtained several case number possible list constrained example indicate retrieval time quite sensitive choice made
