2010,wireless sensor network soil science,wireless sensor network revolutionise soil ecology providing measurement temporal spatial granularity previously impossible paper present first step towards fulfilling goal developing deploying two experimental soil monitoring network urban forest baltimore md node network periodically measure soil moisture temperature store measurement local memory raw measurement incrementally retrieved sensor gateway persistently stored database database also store calibrated version collected data measurement database available third-party application various web service interface high level deployment successful exposing high-level variation soil factor however encountered number challenging technical problem need low-level programming multiple level calibration across space time sensor fault problem must addressed sensor network fulfil potential high-quality instrument deployed scientist without major effort cost
2008,catalog archive server database management system,multiterabyte sloan digital sky survey  catalog data stored commercial relational database management system sql query access built-in query optimizer sd catalog archive server add advanced data mining feature dbms provide fast online access data
2008,sqlloader data-loading pipeline,using database management system  essential ensure data integrity reliability large multidimensional data set however loading multiterabyte data dbms time-consuming error-prone task author tried automate developing sqlloader pipeline-a distributed workflow system data loading
2008,distributed computing economics,computing economics changing today rough price parity  one database access   byte network traffic   instruction   byte disk storage  megabyte disk bandwidth implication one structure internet-scale distributed computing one put computing close data possible order avoid expensive network traffic
2008,flash disk opportunity server application,future flash-based disk could provide breakthrough iop power reliability volumetric capacity compared conventional disk
2007,data management worldwide sensor web,harvesting benefit sensor-rich world present many data management challenge recent advance research industry aim address challenge rapidly increasing number large-scale sensor network deployment vision worldwide sensor web close becoming reality
2006,petascale computational system,balanced cyberinfrastructure necessary meet growing data-intensitive scientific need believe available resource allocated benefit broadest cross-section scientific community given power-law distribution problem size mean half funding agency resource spent tier- center petascale level half dedicated tier- tier- center cost-sharing basis funding agency support balanced system cpu farm well petascale io networking also allocate resource balanced tier- tier- cyberinfrastructure
2006,designing building terraservice,simple rule guide design web service terraservice geospatial service added microsofts popular terraserver database sticking standards-based tool author able implement web service major structural change database conforms -to- kbytes one second guideline terraservice offer highly acceptable user experience success two u department agriculture application bridge web service database demonstrates astuteness author design principle
2006,consensus transaction commit,distributed transaction commit problem requires reaching agreement whether transaction committed aborted classic two-phase commit protocol block coordinator fails fault-tolerant consensus algorithm also reach agreement block whenever majority process working paxos commit algorithm run paxos consensus algorithm commit/abort decision participant obtain transaction commit protocol us f +  coordinator make progress least f +  working properly paxos commit stable-storage write delay implemented message delay fault-free case two-phase commit us message classic two-phase commit algorithm obtained special f =  case paxos commit algorithm
2006,triumph sin challenge database benchmarking,n/a
2006,memory - memory model scientific algorithm graphic processor,present memory model analyze improve performance scientific algorithm graphic processing unit  memory model based texturing hardware us block-based array representation perform underlying computation incorporate many characteristic gpu architecture including smaller cache size block representation use c model analyze cache miss moreover present technique improve performance nested loop gpus order demonstrate effectiveness model highlight performance three memory-intensive scientific application - sorting fast fourier transform dense matrix-multiplication practice cache-efficient algorithm application able achieve memory throughput - gb/s nvidia  gtx gpu also compare  prior gpu-based cpu-based implementation high-end processor practice able achieve - x performance improvement
2006,data analysis tool sensor-based science,science increasingly driven data collected automatically array inexpensive sensor collected data volume require different approach scientist current excel spreadsheet storage analysis model spreadsheet work well small data set scientist want high level summary data various statistical analysis without sacrificing ability drill every bit raw data demonstration describes prototype data analysis system suitable browsing visualization - like spreadsheet - scalable much larger data set
2006,gputerasort high performance graphic co-processor sorting large database management,present novel external sorting algorithm using graphic processor  large database composed billion record wide key algorithm us data parallelism within gpu along task parallelism scheduling memory-intensive compute-intensive thread gpu new sorting architecture provides multiple memory interface pc -- fast dedicated memory interface gpu along main memory interface cpu computation result achieve higher memory bandwidth compared cpu-based algorithm running commodity pc approach take account limited communication bandwidth cpu gpu reduces data communication two processor algorithm also improves performance disk transfer achieves close peak i/o performance tested performance algorithm sortbenchmark applied large database composed hundred gigabyte data   ghz pentium iv pc $ nvidia  gt gpu indicate significant performance improvement optimized cpu-based algorithm high-end pc  ghz dual xeon processor implementation able outperform current high-end pennysort benchmark  higher performance price ratio overall  indicate using gpu co-processor significantly improve performance sorting algorithm large database
2005,lowell database research self-assessment,database need changing driven internet increasing amount scientific sensor data article author propose research several important new direction database management system
2005,measure transaction processing 20 year later,n/a
2005,call arm,long anticipated arrival radically restructured database architecture finally hand
2005,scientific data management coming decade,scientific instrument computer simulation creating vast data store require new scientific method analyze organize data data volume approximately doubling year since new instrument extraordinary precision data quality also rapidly improving analyzing data find subtle effect missed previous study requires algorithm simultaneously deal huge datasets find subtle effect --- finding needle haystack finding small haystack undetected previous measurement
2005,database system meet grid,illustrate benefit combining database system grid technology data-intensive application using cluster sql server reimplemented existing grid application find galaxy cluster large astronomical database sql implementation run order magnitude faster earlier tcl-c-file-based implementation discus grid application take advantage database system 
2004,rubber meet sky bridging gap database science,n/a
2004,revolution database system architecture,database system architecture undergoing revolutionary change algorithm data unified integrating programming language database system give extensible object-relational system non-procedural relational operator manipulate object set coupled dbms web service huge implication structure application dbms object container queue first object added queue basis transaction processing workflow applica-tions future workflow system likely built core data cube online analytic processing baked dbms beyond dbms framework data mining machine learning algorithm decision tree bayes net clustering time series analysis built new algorithm added text temporal spatial data access method along probabilistic reasoning added database system allowing approximate probabilistic answer essential many application many believe xml xquery main data structure access pattern database system must accommodate perspectivethese change mandate much dynamic query optimization strategy intelligence moving periphery network disk sensor competent database machine relational algebra convenient way program system database system expected self-managing self-healing always-up
2004,next database revolution,database system architecture undergoing revolutionary change importantly algorithm data unified integrating programming language database system give extensible object-relational system non-procedural relational operator manipulate object set coupled dbms web service huge implication structure application dbms object container queue first object added queue basis transaction processing workflow application future workflow system likely built core data cube online analytic processing baked dbms beyond dbms framework data mining machine learning algorithm decision tree bayes net clustering time series analysis built new algorithm added rebirth column store sparse table optimize bandwidth text temporal spatial data access method along probabilistic reasoning added database system allowing approximate probabilistic answer essential many application many believe xml xquery main data structure access pattern database system must accommodate perspective external data increasingly arrives stream compared historical data stream-processing operator added dbms publish-subscribe system invert data-query ratio incoming data compared million query rather query searching million record meanwhile disk memory capacity growing much faster bandwidth latency database system increasingly use huge main memory sequential disk access change mandate much dynamic query optimization strategy - one adapts current condition selectivity rather static plan intelligence moving periphery network disk sensor competent database machine relational algebra convenient way program system database system expected self-managing self-healing always-up researcher developer work cut u delivering feature
2004,rubber meet sky semantic gap data producer data consumer,summary form given historically scientist gatherer analyzed data technology created functional specialization scientist gather generate data others analyze technology allows u easily capture vast amount empirical data generate vast amount simulated data technology also allows u store byte almost indefinitely tool organize scientific data easy access query tool curate data tool federate science archive domain scientist notably ncbi virtual observatory making heroic effort address problem generic problem cut across scientific discipline requires coordinated effort computer science community build generic tool help science current database product start much needed
2003,migrating multiterabyte archive object relational database,commercial object-oriented database engine custom tool data-mining multiterabyte sloan digital sky survey archive meet performance objective describe problem technical issue process migrating large data set project relational database technology
2003,next dozen information-technology research goal,n/a
2003,on-line science world-wide telescope prototype new computational science,n/a
2003,lowell report,n/a
2002,whats next high-performance computing,trace evolution crays cluster supercomputing center go
2002,world-wide telescope,mining vast database astronomical data new online way see global structure universe promise wonderful virtual telescope archetype evolution computational science
2002,sd skyserver public access sloan digital sky server data,skyserver provides internet access public sloan digital sky survey  data astronomer science education paper describes skyserver goal architecture also describes experience operating skyserver internet sd data public well-documented make good test platform research database algorithm performance
2002,data mining sd skyserver database,earlier paper  described sloan digital sky survey  data management need defining twenty database query twelve data visualization task good data management system support built database interface support query load also website ad-hoc access paper report database design describes data loading pipeline report query implementation performance query typically translated single sql statement query run le  second allowing scientist interactively explore database paper in-depth tour query reader first studied companion overview paper szalay et al sd skyserver public access sloan digital sky server data acm sigmond 
2001,digital immortality,n/a
2000,rule thumb data engineering,paper reexamines rule thumb design data storage system briefly look storage processing networking cost ratio trend particular focus performance price/performance amdahls ratio law system design need slight revision  years-the major change increased use ram analysis also indicates storage used cache database web data save disk bandwidth network bandwidth people time surprisingly -minute rule disk caching becomes cache-everything rule web caching
2000,terraserver spatial data warehouse,microsoft® terraserver store aerial satellite topographic image earth sql database available via internet world largest online atlas combining eight terabyte image data united state geological survey  spin- internet browser provide intuitive spatial text interface data user need special hardware software knowledge locate browse imagery paper describes terabyte “internet unfriendly” geo-spatial image scrubbed edited hundred million “internet friendly” image tile loaded sql data warehouse meta-data imagery stored sql databaseterraserver demonstrates general-purpose relational database technology manage large scale image repository show web browser good geo-spatial image presentation system
2000,designing mining multi-terabyte astronomy archive sloan digital sky survey,next-generation astronomy digital archive cover sky fine resolution many wavelength x-ray ultraviolet optical infrared archive stored diverse geographical location one first project sloan digital sky survey  creating -wavelength catalog  square degree sky   million object multi-terabyte database mostly numerical attribute + dimensional space point space highly correlated distributionsthe archive enable astronomer explore data interactively data access aided multidimensional spatial attribute index data partitioned many way small tag object consisting popular attribute accelerate frequent search splitting data among multiple server allow parallel scalable i/o parallel data analysis hashing technique allow efficient clustering pair-wise comparison algorithm parallelize nicely randomly sampled subset allow de-bugging otherwise large query desktop central server operate data pump support sweep search touching data anticipated query require special operator related angular distance complex similarity test object property like shape color velocity vector temporal behavior issue pose interesting data management challenge
1999,system r architectural overview,described architecture system r including relational data system research storage system rds support flexible spectrum binding time ranging precompilmion “canned transactions” on-line execution ad hoc query advantage approach may summarized follows  repetitive transaction work parsing name binding access path selection done precompilation time need repeated  ad hoc query compiled line small machine-language routine execute efficiently interpreter  user given single language sql use ad hoc query well writing pl/i cobol transaction program  sql parser access path selection routine machine language code generator used common query processing precompilation transaction program  index used transaction program dropped new access path automatically selected transaction without user intervention
1999,fcast multicast file distribution tune download drop,reliable data multicast difficult scale fcast file multicasting combine multicast forward error correction  solve problem like classic multicast fcast scale large audience like fec scheme us bandwidth efficiently benefit combinati known previously fcast contributes new caching method improve disk throughput new optimization small file transfer 
1998,asilomar report database research,database research community rightly proud success basic research remarkable record technology transfer field need radically broaden research focus attack issue capturing storing analyzing presenting vast array online data database research community embrace broader research agenda  broadening definition database management embrace content web online data store rethinking fundamental assumption light technology shift accelerate transition recommend changing way research  evaluated presented particular advocate encouraging speculative long-range work moving conference poster format publishing research literature web
1998,design architecture microsoft cluster service - practical approach high-availability scalability,microsoft cluster service  extends window nt operating system support high-availability service goal offer execution environment off-the-shelf server application continue operate even presence node failure later version msc provide scalability via node application management system allows application scale hundred node paper provide detailed description msc architecture design decision driven implementation service paper also describes major application use msc feature describes feature added make easier implement manage fault-tolerant application msc
1997,data cube relational aggregation operator generalizing group-by cross-tab sub total,data analysis application typically aggregate data across manydimensions looking anomaly unusual pattern sql aggregate function group operator produce zero-dimensional orone-dimensional aggregate application need n-dimensional generalization operator paper defines operator called data cube simply cube cube operator generalizes histogramcross-tabulation roll-updrill-down sub-total construct found report writersthe novelty cube relation consequently cube operator imbedded complex non-procedural dataanalysis program cube operator treat n aggregation attribute dimension n-space aggregate particular set attribute value point space theset point form n-dimensional cube super-aggregates arecomputed aggregating n-cube lower dimensional spacesthis paper  explains cube roll-up operator  showshow fit sql  explains user define new aggregate function cube  discus efficient technique tocompute cube many feature added sqlstandard
1997,five-minute rule ten year later computer storage rule thumb,simple economic performance argument suggest appropriate lifetime main memory page suggest optimal page size fundamental tradeoff price bandwidth ram disk analysis indicates today technology five minute good lifetime randomly accessed page one minute good lifetime two-pass sequentially accessed page  kb good size index page rules-of-thumb change predictable way technology ratio change also motivate importance new kaps map scan $/kaps $/maps $/tbscan metric
1996,evolution data management,computer store form information record document image sound recording video scientific data many new data format society made great stride capturing storing managing analyzing visualizing data task generically called data management article sketch evolution data management system six distinct phase data management initially data manually processed next step used punched-card equipment electromechanical machine sort tabulate million record third phase stored data magnetic tape used stored-program computer perform batch processing sequential file fourth phase introduced concept database schema on-line navigational access data fifth step automated access relational database added distributed client server processing early stage sixth-generation system store richer data type notably document image voice video data sixth-generation system storage engine emerging internet intranet early data management system automated traditional information processing today allow fast reliable secure access globally distributed data tomorrow system access summarize richer form data argued multimedia database cornerstone cyberspace
1996,data cube relational aggregation operator generalizing group-by cross-tab sub-total,data analysis application typically aggregate data across many dimension looking unusual pattern sql aggregate function group operator produce zero-dimensional one-dimensional answer application need n-dimensional generalization operator paper defines operator called data cube simply cube cube operator generalizes histogram cross-tabulation roll-up drill-down sub-total construct found report writer cube treat n aggregation attribute dimension n-space aggregate particular set attribute value point space set point form n-dimensionai cube super-aggregates computed aggregating n-cube lower dimensional space aggregation point represented infinite value point  represents global sum item value actually represents set value contributing aggregation
1996,danger replication solution,update anywhere-anytime-anyway transactional replication unstable behavior workload scale ten-fold increase node traffic give thousand fold increase deadlock reconciliation master copy replication  scheme reduce problem simple analytic model demonstrates  new two-tier replication algorithm proposed allows mobile  application propose tentative update transaction later applied master copy commutative update transaction avoid instability replication scheme
1995,alphasort cache-sensitive parallel external sort,new sort algorithm called alphasort demonstrates commodity processor disk handle commercial batch workload using commodity processor memory array scsi disk alphasort run industrystandard sort benchmark seven second beat best published record -cpu -disk hypercube  another benchmark alphasort sorted gigabyte one minute alphasort cache-sensitive memoryintensive sort algorithm argue modern architecture require algorithm designer re-examine use memory hierarchy alphasort us clustered data structure get good cache locality file striping get high disk bandwidth quicksort generate run replacement-selection merge run us shared memory multiprocessor break sort subsort chore startup time becoming significant part total time propose two new benchmark  minutesort much sort one minute  pennysort much sort one penny
1995,super server commodity computer cluster pose software challenge,technology pushing fastest processor onto single mass-produced chip standard defining new level integration pizza box - one board computer memory disk base ware middleware development fundamentally change way build computer future design must leverage commodity product cluster computer natural way build future mainframe simple analysis suggests machine thousand processor giving tera-op processing rate terabyte ram storage many terabyte disc storage terabits-per-second communication bandwidth presage cluster iron monger software house stand terror! customer stand tremendous! computer ideally suited super-servers future network software extract parallelism application key making cluster useful client-server computing natural parallelism many client submit many independent request processed parallel database visualization scientific computing application also made great stride extracting exploiting parallelism within single application promising first step bode well cluster architecture challenge remains extend technique general purpose system
1995,critique ansi sql isolation level,ansi sql-  defines isolation level term phenomenon dirty read non-repeatable read phantom paper show phenomenon ansi sql definition fail properly characterize several popular isolation level including standard locking implementation level covered ambiguity statement phenomenon investigated formal statement arrived addition new phenomenon better characterize isolation type introduced finally important multiversion isolation type called snapshot isolation defined
1995,parallel database system 101,n/a
1995,database workflow management panel,n/a
1994,loading database using dataflow parallelism,paper describes parallel database load prototype digitals rdb database product prototype take dataflow approach database parallelism includes explorer discovers record cluster configuration database client cui interface gather load job description user rdb catalog optimizer pick best parallel execution plan record web data structure web describes data operator dataflow river among binding operator process process processor file disc tape paper describes optimizers cost-based hierarchical optimization strategy detail prototype executes web plan spawning web manager process node cluster manager create local executor process orchestrate startup phasing checkpoint shutdown execution process perform one operator data flow among operator via memory-to-memory stream within node via web-manager multiplexed tcp/ip stream among node design transaction checkpoint/restart mechanism also described preliminary measurement indicate design give excellent scaleups
1994,desktop batch processing,today online transaction processing  application downsize mainframe microprocessor commodity database system operating system hardware came age /spl minus/they surpassed online transaction processing performance proprietary solution lingering doubt downsizing batch transaction processing application doubt center ability microprocessor hardware handle high i/o bandwidth required batch processing doubt microprocessor system offer software service utility key batch processing application paper review impressive progress made commodity software hardware processing oltp workload discussion quantitative transaction processing performance council defined set benchmark characterize oltp quantify price performance discussion turn batch transaction processing le consensus characteristic batch transaction processing consequently much discussion focus requirement discussion end performance measurement utility running dec alpha axp microprocessor commodity disk  indicate microprocessor today capacity process batch workload mainframe speed predict next year batch-processing software exploiting parallel processing emerge combined commodity hardware provide superior performance price/performance ratio< >
1994,alphasort risc machine sort,new sort algorithm called alphasort demonstrates commodity processor disk handle commercial batch workload using alpha axp processor commodity memory array scsi disk alphasort run industry-standard sort benchmark seven second beat best published record -cpu -disk hypercube  another benchmark alphasort sorted gigabyte minutealphasort cache-sensitive memory-intensive sort algorithm us file striping get high disk bandwidth us quicksort generate run us replacement-selection merge run us shared memory multiprocessor break sort subsort choresbecause startup time becoming significant part total time propose two new benchmark  minutesort much sort minute  dollarsort much sort dollar
1994,quickly generating billion-record synthetic database,evaluating database system performance often requires generating synthetic databases—ones certain statistical property filled dummy information evaluating different database design often necessary generate several database evaluate design database size grow terabyte generation often take longer evaluation paper present several database generation technique particular discus  parallelism get generation speedup scaleup  congruential generator get dense unique uniform distribution  special-case discrete logarithm generate index concurrent base table generation  modification  get exponential normal self-similar distributionsthe discussion term generating billion-record sql database using c program running shared-nothing computer system consisting hundred processor thousand disc idea apply smaller database large database present difficult problem
1993,tp-lite dominate tp market,n/a
1992,parallel database system future high performance database system,success system refutes  paper predicting demise database machine  ten year ago future highly parallel database machine seemed gloomy even staunchest advocate database machine research focused specialized often trendy hardware ccd memory bubble memory head-per-track disk optical disk none technology fulfilled promise sense conventional cpu  electronic ram mcving-head magnetic disk would dominate scene many year come time disk throughput predicted double processor speed predicted increase much larger factor consequently  critic predicted multiprocessor system would scxm i/o limited unless solution i/o bottleneck found whiie prediction fairly accurate future hardware critic certainly wrong overall future parallel database system last decade eradata tandem host startup company successfully developed marketed highly parallel machine
1992,increasing effectiveness o research,n/a
1992,database transaction processing benchmark,n/a
1991,high-availability computer system,technique used build highly available computer system sketched historical  provided terminology defined empirical experience computer failure briefly discussed device improvement greatly increased reliability digital electronics identified fault-tolerant design concept approach fault-tolerant hardware outlined role repair maintenance design-fault tolerance discussed software repair considered use pair computer system separate location guard unscheduled outage due outside source  addressed< >
1990,third-generation database system manifesto - committee advanced dbms function,n/a
1990,parallel database system future database processing passing fad,concept parallel database machine consisting exotic hardware replaced fairly conventional shared-nothing hardware base along highly parallel dataflow software architecture design provides speedup scaleup processing relational database query paper review technique used system survey current commercial research system
1990,benchmark nonstop sql release 2 demonstrating near-linear speedup scaleup large database,n/a
1990,fastsort distributed single-input single-output external sort,external single-input single-output sort use multiple processor large tournament replacement-selection memory private disk sort input stream linear elapsed time course increased number processor memory disk required input file size grows paper analyzes algorithm report performance implementation
1990,committee advanced dbms function third generation data base system manifesto,n/a
1990,parity striping disk array low-cost reliable storage acceptable throughput,analysis mirrored disc raid show mirror considerably better throughput measured requests/second random request arbitrary size  mirror comparable better response time request reasonable size  mirror  storage penalty storing data twice parity striping data layout stripe parity across disc stripe data parity striping throughput almost good mirror cost/gb comparable raid design -combing advantage high-traffic disc resident data parity striping additional fault containment software benefit well parity striping sacrifice high data transfer rate raid design high throughput argued response time throughput preferable performance metric
1990,adaptive hash join algorithm multiuser environment,main memory becomes cheaper resource hash join alternative traditional method performing equi-joins nested loop merge join paper introduces modified adaptive hash join method designed work dynamic change amount available memory general idea algorithm regulate resource usage hash join way allows run concurrently application algorithm provides good performance broad range problem size allows join large table small main memory us advanced i/o controller tracksize i/o transfer implemented prototype nonstop sql dbms running tandem machine
1989,future direction dbms research - laguna beach participant,february -  international computer science institute sponsored two day workshop  senior member data base research community discussed future research topic dbms area paper summarizes discussion took place 
1989,database performance metric,n/a
1988,cost message,distributed system modeled process communicating via message model abstract three degree distribution shared memory local network wide area network although three form distribution qualitatively huge quantitative difference message transport cost message transport reliability paper quantifies difference past current future technology 
1988,disk shadowing,disk shadowing technique maintaining set two identical disk image separate disk device primary purpose enhance reliability availability secondary storage providing multiple path redundant data however shadowing also boost uo performance paper contend intelligent device scheduling shadowed disk increase i/o rate allowing parallel read substantially reducing average seek time random read particular develop analytic model show seek time random read shadow set monotonic decreasing function number disk set
1987,view database system performance measure,database system allow quick creation performance problem goal database system allow computer-illiterate write complex complete application job system translate high-level description data procedure efficient algorithm real performance metric system successfully meet goalspractitioners use much narrower definition system performance assume standard workload measure performance peak throughput dollar cost per transactionalthough many vendor “private” performance measure bitton dewitt turbyfill first publish measure database system performance  measure called wisconsin benchmark consists database design set  retrieval update statement script multi-user test give two performance metric elapsed time statement throughput system running sixteen simultaneous script response time requirement cost measure included definition wisconsin benchmark widely used database benchmarklargely response wisconsin benchmark informal group including bitton dewitt defined benchmark representative transaction processing application  workload isscan - mini-batch operation sequentially copy  recordssort - batch operation sort one million recordsdebitcredit - short transaction terminal input output via x presentation service mix five database accessesthe debitcredit transaction rule scaling terminal network database size transaction rate increase also rule distributing transaction system decentralizedthe performance metric benchmark areelapsed time scan sortpeak throughput debitcredit transaction  second response time  transaction give tps  ratingprice per transaction price -year cost hardware software maintenance sometimes called vendors-view pricethis benchmark adopted several vendor compare performance price performance release release also compare performance competitive product mips whetstone megaflop served similar role scientific communitya system tps rating indicates processor speed also io architecture operating system data communication database software performance unfortunately capture ease-of-usework continues formalizing benchmark present written english ultimately defined file generator set program written standard database language cobol-sqlwhen vendor first measure system benchmark  usually terrible benchmark designed expose generic performance bug frequently used transaction processing atom example wisconsin scan benchmark heavily penalize system slow read next record filea system poor performance benchmark analyzed follows vendor “atomic” model system represents transaction collection atom atom primitive system example scan benchmark represented vendor scan begin transaction perform  time read sequential insert sequential commit transactionthe atomic weight begin read sequential insert sequential commit measured release atomic weight usually consists cpu instruction message byte disc io “typical” call operation weight converted service time knowing speed utilization device  used application molecular weight service time scan computed sum atomic weightsdefining measuring system atom valuable produce simple conceptual model system used atomic measurement also expose performance bug example based scan benchmark system perform read sequential  instruction  disc io system us many instruction many io performance problem similarly debitcredit transaction typically consumes ooki  five disc io per transaction one system known use ki  io per transaction vendor could use atomic measurement find cause poor performance problem localized atom solution problem readily suggest atomic measurement useful performance assurance performance improvementatomic measurement also major role system sizing capacity planning customer describe application term atom spreadsheet application give estimate cpu disc line cost application substantially effort  system response time predicted even effort prototype system generated benchmarked atomic transaction description snapshot  envision  example system combine atomic modeling queue modeling ultimately benchmarking real system generated atomic description application
1987,operating system support data management system,n/a
1987,5 minute rule trading memory disk access 10 byte rule trading memory cpu time,item accessed frequently enough main memory resident current technology “frequently enough” mean every five minutesalong similar vein one frequently trade memory space cpu time example bit packed byte expense extra instruction extract bit make economic sense spend ten byte main memory save one instruction per secondthese  depend current price ratio processor memory disc access ratio changing hence constant rule changing
1986,approach decentralized computer system,technology distributed computing available however argued decentralized system always require careful design planning management centralized counterpart rationale decentralization given technical approach decentralized system sketched approach contrast popular concept distributed integrated database transparently provides remote io single system image rather proposes function distributed `servers abstract data high-level operation object communicate `requestors via standard message protocol requestor-server approach advantage modularity performance
1986,comparison byzantine agreement problem transaction commit problem,transaction commit algorithm byzantine agreement algorithm solve problem multiple process reaching agreement presence process message failure paper summarizes computation fault model two kind agreement discus difference particular explains byzantine agreement rarely used practice involves significantly hardware message yet give predictable behavior fault
1986,computer stop done,analysis failure statistic commercially available fault -tolerant system show administration software ar e major contributor failure various approach software fault-tolerance discussed -- notably process-pairs transaction reliable storage pointed fault production software often soft  ransaction mechanism combined persistent processpairs provides fault-tolerant execution -- key software fault -tolerance
1985,transaction acceleration,n/a
1985,computer stop,analysis failure statistic commercially available fault -tolerant system show administration software ar e major contributor failure various approach software fault-tolerance discussed -- notably process-pairs transaction reliable storage pointed fault production software often soft  ransaction mechanism combined persistent processpairs provides fault-tolerant execution -- key software fault -tolerance
1985,one thousand transaction per second,several company intend provide general-purpose transaction processing system capable one thousand transaction per second paper survey need system contrast approach taken three different group 
1985,fault tolerance tandem system,n/a
1983,practical problem data management - position paper,n/a
1982,transaction consistency distributed database system,concept transaction data consistency defined distributed system case partitioned data fragment file stored multiple node replicated data file replicated several node discussed argued distribution replication data transparent program use data programming interface provide location transparency replica transparency concurrency transparency failure transparency technique providing transparency abstracted discussedby extending notion system schedule system clock handle multiple node shown distributed system modeled single sequential execution sequence model used discus simple technique implementing various form transparency
1981,history evaluation system r,system r experimental database system constructed demonstrate usability advantage relational data model realized system complete function high performance required everyday production use paper describes three principal phase system r project discus lesson learned system r design relational system database system general
1981,recovery manager system r database manager,recovery subsystem experimental data management system described evaluated transactmn concept allows application program commit abort partially undo effect do-undo-redo protocol allows new recoverable type operation added recovery system apphcation program record data transaction log facilitate application-specific recovery transaction undo redo based record kept transaction log checkpoint mechanism based differential fry  recovery log recorded disk rather tape
1981,straw man analysis probability waiting deadlock database system,n/a
1981,approach end-users application design,soon every desk computer software mundane thing payroll mail text processing exists by-product produce vast quantity one-line information many user want manipulate data often unanticipated way unexpected us cannot justify substantial programming cost paper argues relational data model operator combined screen-oriented form design display system answer many need user system data represented term record field user defines screen  want see specifies mapping field screen field database record term predicate relational operator
1981,transaction concept virtue limitation invited paper,transaction transformation state property atomicity  durability  consistency  transaction concept key structuring data management application concept may applicability programming system general paper restates transaction concept attempt put several implementation approach perspective describes area require study  integration transaction concept notion abstract data type  technique allow transaction composed sub- transaction  handling transaction last extremely long time 
1980,transaction model,paper attempt tersely restate several theoretical  transaction recovery concurrency control formal model entity action transaction entity failure concurrency distributed system required present  included theorem transaction undo redo degree consistency predicate lock granularity lock deadlock two-phase commit
1979,system r relational data base management system,relational approach make experimental data base management system unusually easy install use decision made system r design order enhance usability also offer major bonus area
1979,convoy phenomenon,n/a
1978,note data base operating system,paper compendium data base management operating system folklore early paper still draft form intended set course note class data base operating system brief overview data management system focus particular issue unique transaction management component especially locking recovery
1976,notion consistency predicate lock database system,database system user access shared data assumption data satisfies certain consistency constraint paper defines concept transaction consistency schedule show consistency requires transaction cannot request new lock releasing lock argued transaction need lock logical rather physical subset database subset may specified predicate implementation predicate lock satisfies consistency condition suggested
1976,system r relational approach database management,system r database management system provides high level relational data interface system provides high level data independence isolating end user much possible underlying storage structure system permit definition variety relational view common underlying data data control feature provided including authorization integrity assertion triggered transaction logging recovery subsystem facility maintaining data consistency shared-update environmentthis paper contains description overall architecture design system present time system implemented design evaluated emphasize system r vehicle research database architecture planned product
1976,granularity lock degree consistency shared data base,problem choosing appropriate hranularit~  lockable object introduced tradeoff concurrency overhead discus locking protocol allows simultaneous locking various granularity different transaction presented based introduction additional lock mode besides conventional share mode exclusive mode proof given equivalence protocol conventional one 
1975,view authorization locking relational data base system,interest brevity assume reader familiar notion relational data base particular assume familiarity work codd boyce chamberlin example paper drawn data base describes department store consists three relationsempsalesloc
1975,granularity lock large shared data base,paper proposes locking protocol associate lock set resource protocol allows simultaneous locking various granularity different transaction based introduction additional lock mode besides conventional share mode exclusive mode protocol generalized simple hierarchy lock directed acyclic graph lock dynamic graph lock issue scheduling granting conflicting request resource discussed lastly idea compared lock mechanism provided existing data management system
1973,canonical precedence scheme,general theory canonical precedence analysis defined studied familiar type precedence analysis operator precedence simple precedence occur special case theory among theoretical  obtained characterization structure precedence relation relation canonical precedence scheme operator set
1972,covering reduction problem context-free grammar,formal definition one grammar covering another grammar presented argued definition property g cover g ability parse g suffices parsing g shown every grammar may covered grammar canonical two form every a-free grammar covered operator normal form grammar exist grammar cannot covered grammar greibach form grammar may covered invertible grammar a-free chain reduced lr  grammar covered precedence detectable lr  reducible grammar
1969,single pas precedence analysis extended abstract,n/a
1968,infinite linear sequential machine,linear sequential machine  considered arbitrary field various finiteness condition given sequential machine lsms property characterized shown set input/output pair characterizes lsm effect realization varying ground field studied number algorithm given solution specific problem decision procedure given equivalence problem lsms problem determining one state accessible another discussed number  presented
1967,two-way pushdown automaton,paper new type automation called two-way pushdown automaton defined studied model generalization pushdown automaton two-way motion allowed input tape assumed endmarkers model investigated nondeterministic deterministic case number basic  obtained include relationship family closure nonclosure  decidability property certain special case studied case input alphabet one letter device endmarkers
1966,theory sequential relation,n/a
